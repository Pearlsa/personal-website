[{"body":"","date":"","id":1,"title":"GitHub","type":"profile","url":"http://github.com/charlieegan3"},{"body":"","date":"","id":2,"title":"Twitter","type":"profile","url":"http://twitter.com/charlieegan3"},{"body":"","date":"","id":3,"title":"Instagram","type":"profile","url":"http://instagram.com/charlieegan3"},{"body":"","date":"","id":4,"title":"Unsplash","type":"profile","url":"https://unsplash.com/@charlieegan3"},{"body":"","date":"","id":5,"title":"LinkedIn","type":"profile","url":"https://news.ycombinator.com/item?id=9045677"},{"body":"","date":"","id":6,"title":"Strava","type":"profile","url":"https://www.strava.com/athletes/1238371"},{"body":"","date":"","id":7,"title":"Parkrun","type":"profile","url":"http://www.parkrun.org.uk/results/athleteresultshistory/?athleteNumber=358706"},{"body":"","date":"","id":8,"title":"LastFM","type":"profile","url":"http://www.last.fm/user/charlieegan3"},{"body":"","date":"","id":9,"title":"Letterboxd","type":"profile","url":"https://letterboxd.com/charlieegan3"},{"body":"","date":"","id":10,"title":"Keybase","type":"profile","url":"https://keybase.io/charlieegan3"},{"body":"","date":"","id":11,"title":"Monzo","type":"profile","url":"https://monzo.me/charlieegan"},{"body":"","date":"","id":12,"title":"Stack Overflow","type":"profile","url":"http://stackoverflow.com/users/1510063/charlie-egan"},{"body":"","date":"","id":13,"title":"Hacker News","type":"profile","url":"https://news.ycombinator.com/user?id=charlieegan3"},{"body":"","date":"","id":14,"title":"MacRumors","type":"profile","url":"http://forums.macrumors.com/members/charlieegan3.669532/"},{"body":"","date":"","id":15,"title":"Product Hunt","type":"profile","url":"http://www.producthunt.com/@charlieegan3"},{"body":"","date":"","id":16,"title":"Bungie","type":"profile","url":"https://halo.bungie.net/stats/reach/default.aspx?player=tartan%20turtle"},{"body":"","date":"","id":17,"title":"Steam","type":"profile","url":"http://steamcommunity.com/id/charlieegan3"},{"body":"","date":"","id":18,"title":"Starcraft","type":"profile","url":"http://eu.battle.net/sc2/en/profile/5512289/1/TartanTurtle/"},{"body":"","date":"","id":19,"title":"Google Scholar","type":"profile","url":"https://scholar.google.co.uk/citations?user=pqb-ZNAAAAAJ\u0026amp;hl=en"},{"body":"","date":"","id":20,"title":"Kattis","type":"profile","url":"https://open.kattis.com/users/charlie"},{"body":"Codified building of dev machine image for Hetzner Cloud VM. Used from iPad via Blink shell for development on the go.","date":"","id":21,"title":"dev-machine","type":"side-project","url":"https://github.com/charlieegan3/dev-machine"},{"body":"Spaced repetition tool MVP","date":"","id":22,"title":"flashcards","type":"experiment","url":"https://github.com/charlieegan3/flashcards"},{"body":"Using OPA to keep Christmas safe and fun","date":"","id":23,"title":"Policing Christmas Trees","type":"talk","url":"/posts/2019-12-05-rego-fun/"},{"body":"Personal side project platform configuration","date":"","id":24,"title":"charlieegan3/infrastructure","type":"side-project","url":"https://github.com/charlieegan3/infrastructure"},{"body":"Remove old Kubernetes objects when applying new ones","date":"","id":25,"title":"kubectl vapply","type":"experiment","url":"https://github.com/charlieegan3/kubectl-vapply"},{"body":"A dashboard for my music play history.","date":"","id":26,"title":"charlieegan3: Music","type":"side-project","url":"https://music.charlieegan3.com"},{"body":"A portfolio site built from my Instagram posts with a map and calendar.","date":"","id":27,"title":"charlieegan3: Photos","type":"side-project","url":"https://photos.charlieegan3.com"},{"body":"Create long-exposure style stacked images from videos","date":"","id":28,"title":"stackr","type":"experiment","url":"https://github.com/charlieegan3/stackr"},{"body":"Twitter bot that uses AWS Polly to narrate @dril tweets","date":"","id":29,"title":"Geraint","type":"experiment","url":"https://github.com/charlieegan3/geraint"},{"body":"Uses cloud image recognition APIs to tag Instagram photos","date":"","id":30,"title":"insta-tagger","type":"experiment","url":"https://github.com/charlieegan3/insta-tagger"},{"body":"Find a critic that feels the same way you do about film","date":"","id":31,"title":"MyCriticMatch","type":"side-project","url":"https://github.com/charlieegan3/mycriticmatch"},{"body":"RSS feed interleaving as-a-'service'","date":"","id":32,"title":"RSSMerge","type":"side-project","url":"https://github.com/charlieegan3/rssmerge"},{"body":"Broken link scanner-as-a-free-service. Built with Terraform","date":"","id":33,"title":"Borked","type":"side-project","url":"https://github.com/charlieegan3/borked"},{"body":"Super short talk about some things I found out using BigQuery","date":"","id":34,"title":"Heroku Treasure Hunt - Unboxed FTL","type":"talk","url":"/blog/2017/01/12/heroku-treasure"},{"body":"Build self-printable playing cards with image generation. Built with Rails \u0026amp; Primitive.","date":"","id":35,"title":"Shuffled","type":"experiment","url":"https://github.com/charlieegan3/shuffled"},{"body":"Generate email updates and RSS feeds for Twitter accounts. Built on Rails \u0026amp; Heroku.","date":"","id":36,"title":"Twitter Roundup","type":"side-project","url":"https://github.com/charlieegan3/twitter-roundup"},{"body":"Collection of tools and for analyzing online discussion. Built with Rails \u0026amp; Rust.","date":"","id":37,"title":"StandPoint","type":"side-project","url":"https://github.com/charlieegan3/standpoint"},{"body":"Rust library for matching subgraphs.","date":"","id":38,"title":"Graph Match","type":"experiment","url":"https://github.com/charlieegan3/graph_match"},{"body":"First meetup talk where I talk about a means of making repeated bundle installations in container builds less painful","date":"","id":39,"title":"Development Rebundling in Dockerland - LRUG","type":"talk","url":"https://skillsmatter.com/skillscasts/9120-development-re-bundling-in-dockerland"},{"body":"App aggregating common dotfile settings. Built on Heroku using Go.","date":"","id":40,"title":"Dotfiled","type":"side-project","url":"https://github.com/charlieegan3/dotfiled"},{"body":"Paper accepted at the 3rd Workshop on Argument Mining (ACl 2016)","date":"","id":41,"title":"Summarising the points made in online political debates","type":"paper","url":"https://scholar.google.co.uk/citations?view_op=view_citation\u0026hl=en\u0026user=pqb-ZNAAAAAJ\u0026citation_for_view=pqb-ZNAAAAAJ:u5HHmVD_uO8C"},{"body":"Write up of search engine performance on corrupted queries","date":"","id":42,"title":"Search engine correction accuracy study","type":"paper","url":"/projects/query_corruption_2016.pdf"},{"body":"Honours project thesis. Foundational work for the ACL paper","date":"","id":43,"title":"An argumentation-based approach to summarizing discussions","type":"thesis","url":"/projects/thesis.pdf"},{"body":"As part of a my security course I wrote up a risk assessment of hosting on the Heroku Platform","date":"","id":44,"title":"Security discussion of the Heroku PaaS","type":"paper","url":"/projects/heroku_ra_2016.pdf"},{"body":"'Linearizing' tracker for tech news. Built on Rails","date":"","id":45,"title":"serializer","type":"side-project","url":"https://serializer.io"},{"body":"\nFind things on my website.\n","date":"","id":46,"title":"search","type":"page","url":"/search/"},{"body":"\nHere's a list of some things I've worked on.\n","date":"","id":47,"title":"projects","type":"page","url":"/projects/"},{"body":"\nYou can find me on the internet in these places\n","date":"","id":48,"title":"profiles","type":"page","url":"/profiles/"},{"body":"\n\nHello, my name is Charlie - thanks for stopping by! Nice to meet you üëã. Here's a little bit about me.\n\nBefore COVID-19 I used to go to:\n\n- üèûÔ∏è [Parkrun](http://www.parkrun.org.uk/results/athleteresultshistory/?athleteNumber=358706)\n- \u0026 help organise this [book club](http://london.computation.club/) üìñ\n- üé≤ board game nights\n- üßò‚Äç‚ôÇÔ∏è yoga (and spin classes with [scoreboards](https://www.strava.com/activities/2095479215))\n\nI'm interested in:\n\n- üì∏ [photography](https://photos.charlieegan3.com/)\n- üìà quantified self\n- üåê [Kubernetes](https://kubernetes.io/), [OPA](https://www.openpolicyagent.org/)\n- üí≥ personal finance\n- üöÇ public transport systems and their usability\n\nOther things I do:\n\n- üë®‚Äçüç≥ cook vegetarian food\n- üé∂ [track](https://music.charlieegan3.com/) all the music I listen to\n- üßª keep a [blog](https://www.notion.so/posts) \u0026 write a journal\n- üéΩ [run](https://www.strava.com/athletes/1238371), bike, swim - but not all three at once just yet!\n\nI work at [Jetstack](https://jetstack.io/) üíº.\n","date":"","id":49,"title":"about\n","type":"page","url":"/about/"},{"body":"I‚Äôve always liked the idea of keeping a diary, incrementally creating something worthwhile to look back on. This is hopefully the start of something like that.","date":"Nov 30 2012","id":50,"title":"Prologue","type":"blog post","url":"/posts/2012-11-30-prologue/"},{"body":"I can‚Äôt understand why we all go back to video games. Every time we play it‚Äôs ‚Äòthe last time‚Äô yet somehow we keep coming back for another dose of¬†disgruntlement and discontent. The games we choose to seem to be inherently¬†frustrating.\n\nI play strategy games, RPGs and arcade games but the ones I continue to play regularly are the most frustrating ones around: First Person Shooters. Every time I finish a session on Halo I‚Äôm in a bad mood, yet whenever there is some free time again it‚Äôs straight back in there for another batch of bile bitterness.\n\nThis is the problem: When players start on different start lines and run different races to the same goal and reward there will always be discontent. In my game this tends to be:¬†\n\n* Overpowered Ordinance: tools available to the player that are made to good.\n* Poor Play-Space Planning: when there are key areas that are easier for one team to get to, or there are positions with ample cover and astonishing sight-lines\n* Client Connection: even a small discrepancy in player latency rates can lead to exceptionally vexing gameplay for all, host and hosted. This is the hardest problem to solve and because of this it has often been overlooked as games evolve.\n\nThere needs to be a new focus in video games: coequal contestants.\n\nPlayers need to start with the same prematch options. They need to play on symmetrical maps and modes. And most importantly with others of equivalent connection quality.\n\nThis might sound like a thoroughly boring game, and it may be, but I think if you could get this to work it would have a new attractive value to exceed the current splurge of limitless customisation and heavy graphics.\n\nPeople are attracted to current generation games bristling with features only to lose interest when they see it‚Äôs the same incompetent mechanics ingrained beneath the gloss.\n\nA coequal contestant¬†game would aim to see a true representation of skill, if you are skilled you are reward by the accompanying score rather than a tacky ordnance drop or perk. If you are unskilled there is no overpowered tool available to boost you - as with everything else in life - skill will come with practice.\n\nPlayers skill would be represented as per playlist ratio rather than a homogenised score devoid of any meaning. If you‚Äôre good at Capture the Flag then this is immediately apparent to the team members or opposition. For example:¬†(Player captures + 0.8*(Team-Player) captures)/Opposition captures. This is then accumulated across games and should accessible as the players name and voice status. This makes it easy to compare and understand¬†player skill.\n\nOther ideas might be:\n\n* Different Arenas: for example: Pro (less health, less aim assist, greater speed, lower respawns‚Ä¶), Snipers (restricted to that weapon class‚Ä¶). One thing is key: each player is treated the same as every other player in that match. This would allow for more variation but would be implemented fairly.\n* Player¬†disconnection¬†and quitting is severely punished and when the player quits the game ends. Punishment maybe: Only¬†accumulate¬†-ve stats for next 5 games, Ban on any¬†progression¬† Another way to do it might be to publicly mark (for a limited time) quitters so people can leave before playing with them. You might be allowed 1 quit per fortnight before punishments. You would also want to punish AFK/inactive players to prevent any boosting.\n* In game player feedback. Players are rated for good or bad communication, team play etc. by other players. Gives another useful¬†comparison¬†criteria.\n\nThat hardly scratches the surface of the full potential of¬†coequal contestant¬†but it‚Äôs something to think about all the same.","date":"Dec 1 2012","id":51,"title":"Coequal Contestants","type":"blog post","url":"/posts/2012-12-01-coequal-contestants/"},{"body":"There are some great apps out there that really don't get enough\nrecognition -¬†[CodeRunner](http://krillapps.com/coderunner/)¬†is my personal\nfavourite of that description.\n\nWe've all used the command line to run code, we've also used a full blown IDE -\nbut CodeRunner is something special. CodeRunner lets you write and run in many,\ntotally different languages and all from the same sleek window.\n\nHit play to run - instantly, choose the syntax from a drop down - made just so\nperfectly simple. And from simplicity comes speed and greater productivity.\n\nAs an example. For one of my courses we were given Ruby assignment.\nI think because the tutors wanted everyone to be on the same system they told\nus that we needed to do it through a virtual machine. Just crazy - you can run\nRuby on Windows, Mac and Linux - and of course everyone goes ahead and sets up\nthe virtual machine leading to many more problems‚Ä¶ For me and CodeRunner though\nit was as simple as just typing code away into one app - naively! For me there\nwas no buggy virtualisation, no over baked text editors - just one clean window\nfor every task.\n\nWhat‚Äôs great though is the same solution worked for my python assessment too,\njust so simple to switch language.","date":"Dec 2 2012","id":52,"title":"Coderunner","type":"blog post","url":"/posts/2012-12-02-simple-as-syntax/"},{"body":"We had a computer when I was very young - I can hardly remember - it broke and\nwas never replaced. When you‚Äôre that age you don‚Äôt think anything of it - just\nget on with life, forget about it the next day.\n\nYears later, shortly after the demise of the Lego phase computers popped up\nagain, we started using for writing ‚Äòstories‚Äô at school. Not having access to\none at home, the eight year old charlie spent as much time as possible on them.\nWhile I can‚Äôt remember exactly, I‚Äôm sure from at day forth the parent pestering\nbegan. And I wasn‚Äôt giving up till we had one of these computer things too.\n\nThey fought a long battle and it was 2005 by the time we got our ‚Äòreplacement‚Äô\ncomputer. It couldn‚Äôt have come at a better time, years earlier and computers\nwould have been boring, any later and I would have found some other interest to\npour my childhood into. This is the story of the iBook G4.\n\nThere was an art project going on at school, there are always art projects at\nschool but this one was different: It came with an artist. His name was John\nMcNaught and he moved around different schools doing projects. He is a key\nfigure in the story for one reason: He had a Powerbook G3, and it had Adobe\nPhotoshop. Photoshop is amazing, such a powerful tool, such incredible fun. So\nthere I was, the ten year old charlie, addicted to Photoshop and in serious\nneed of a computer.\n\nThere was peace on the battleground by Christmas of 2005 - all thanks to Mr\nMcNaught (and perhaps some parental finances).","date":"Dec 3 2012","id":53,"title":"My first computer","type":"blog post","url":"/posts/2012-12-03-countdown-to-computing/"},{"body":"The ‚Äòapp‚Äô software methodology has been a success in the mobile world - and\nit‚Äôs trivially, tiny transactions have certainly made it very profitable. I\ncan‚Äôt fault it, it has simplified user interaction for better and for always.\nNonetheless it has had some interesting side effects.\n\nIt has revised the way consumers buy software, and how developers make it, but\nmost interestingly it has severely altered our expectations of it.\n\nWe now expect software to be different in many ways, but perhaps most\ninterestingly, it must: A) perform one function (well) and B) look beautiful.\n\nApplications came before ‚Äòapps‚Äô and were profoundly different - they had many\nfunctions. Lets use Outlook/Entourage as an example. It can read ones mail,\nschedule ones tasks, store ones contacts - and more, of course. The more\n‚Äòmodern‚Äô method however contains a plethora or ‚Äòapps‚Äô all distinct in function.\nYou have an app for every assignment, and only use and buy, the ones you need.\n\nBoth clearly have their perquisites and problems. Saving by only shopping for\nthe ones you need while loosing out component concord is the obvious comparison\nto make. Personally I think there is more going for the newer compartmentalised\napproach. It gives consumers a choice and small developers a chance. It also\nlends itself nicely to incremental maintenance and is more robust to changing\nconsumer demands. However I don‚Äôt think it‚Äôs a case of ‚Äòone size fits all‚Äô.\nTake an application like Photoshop or Xcode for example, when working on\nsignificantly more complex projects, it soon becomes apparent that it‚Äôs not\nefficient. Having said that, there are complex tasks done with many smaller\nprograms, like web application development, so it‚Äôs not as if the new method is\ndumbed down - just different.\n\nApps have also brought about big changes application aesthetics and user\ninteraction. There is a large incentive to have a clean and stylish interface\nas it‚Äôs often used as a selling point, even a feature. While this is often a\ngood thing and can lead to greater ease of use it has some obvious set backs.¬†\n\nOlder style apps are far from pretty and even less graceful but none the less\nthey get the job done. However, while they often have a steep learning curve,\nthey all follow the same basic procedures making it easy to pick up other\napplications as required. Apps now of course often have radically different\ninterfaces, and while each app might be easier to pick up individually, they\nshare far less in common.\n\nNew apps fully utilise available graphics power and the result can certainly be\nquite pleasing, and while I know others don‚Äôt, I quite like skeuomorphic apps.\nThe problem with this though is backward compatibility, older machines can‚Äôt\nhandle expensive bitmapped interfaces, and sadly, get dropped from the\ncompatibility list.\n\nAs a (self perceived) developer in-the-making I find this ‚Äòevolution‚Äô in\napplication style intriguing and would like to keep user interaction design as\na future career possibility. Hopefully, in a short a time as possible, me and\nconsumer software development will be in the same place. I just wonder where,\nand when that will be.","date":"Dec 4 2012","id":54,"title":"Skeuomorphic Apps and Backwards Compatibility","type":"blog post","url":"/posts/2012-12-04-skeuomorphic-specialisation/"},{"body":"Computer dictation is something I‚Äôve spent a much time trying out.\nSadly, it‚Äôs been a let down but not due to poor accuracy.\n\nThings are fine when it‚Äôs only you for miles around, alone and productive.\nHowever when there‚Äôs anyone else nearby it‚Äôs just not practical. Nearby doesn‚Äôt\njust mean next to you on the train, it can mean in the same building! Yes I\nknow it sounds crazy but I‚Äôll explain.\n\nFirst off you have to speak very loudly - much louder than a typical\nconversation. It‚Äôs just really disrupting to others when you have to broadcast\nevery command. You‚Äôre often repeating lines, correcting phrases, and all in\nyour robotic ‚ÄúCap-discordant‚Ä¶Cap-dictation‚Äù voice. Also, even while you aren‚Äôt\na secret agent, It‚Äôs not ideal for everyone to hear your every Google search.\n\nNoise. While the act of dictation makes lots of it, at the same time, it‚Äôs\ncompletely intolerant of it. Even someone walking about nearby, or closing\ndoors, seemed to be enough to mis-create words.¬† While it‚Äôs good in totally\nquiet situations for my accent, I‚Äôm sure it‚Äôs better at dealing with an\nAmerican accent. Fed an American voice in noisy surroundings it may well cope\nbetter.\n\nTechnically it‚Äôs brilliant but, due to these social noise related issues, the\nfeature is rendered close to useless and I‚Äôd say no technological advancement\nwill change this any time soon.","date":"Dec 7 2012","id":55,"title":"The problem with speaking to my phone","type":"blog post","url":"/posts/2012-12-07-discordant-dictation/"},{"body":"Apps Store have¬†[changed](http://waazzupppp.wordpress.com/2012/11/01/editorial-has-the-app-store-killed-the-software-market/)¬†the face of the software market. They serve up content cheaply and consistently. While the content they provide may be better than ever, the stores themselves just aren‚Äôt up to scratch.\n\nIn many ways they do everything you would expect. They offer up content, organise your payments and oversee feedback. They even do some vaguely intelligent things like suggest apps or nice things like hold the odd sale. What‚Äôs bad is not what they do, but instead, what they don‚Äôt.\n\nYou‚Äôre thinking well what else should they be doing? The answer: lots. It‚Äôs easier to picture when you think about online stores that sell real things - like amazon.com. Think about how you use amazon, say you‚Äôre shopping for a kettle, do you just go to the household section and buy the item (kettle or not) at the top of the list? Did‚Äôt think so. You‚Äôll sort out items till you only see kettles and then rank them on various factors.\n\nOn app stores there are many fewer routes to each product: the home screen, top lists, broad categories and search - which in itself is not good at generic terms at all. While Amazon and eBay allow a¬†[multitude](http://www.ebay.com/sch/ebayadvsearch/?rt=nc)¬†of search and sort apps stores lack any kind of a results filter.\n\nLets say I‚Äôm looking for a productivity app. I search at the top for ‚Äúproductivity‚Äù and I‚Äôm shown results for that term - good. My options then are to: browse the results, choose an app and choose paid or free. Pretty pathetic if you ask me. What if I wanted an app that was less than ¬£2 and had 4 or more stars from over 50 reviews - an impossible yet not unreasonable request.\n\nThere is a reasonable amount of variation in app stores - Steam for example does have better than average browsing options, perhaps due to its age, but it‚Äôs really let down by a buggy client. However, the mean does tend to be: an over simplistic, under powered and watered down browsing experience.\n\nThere‚Äôs as much, if not more, money in app stores as in other online retail - why isn‚Äôt there the same emphasis on a quality browsing experience?","date":"Dec 8 2012","id":56,"title":"The various shortcomings of 'App Stores'","type":"blog post","url":"/posts/2012-12-08-scrapp-stores/"},{"body":"The subtle iPod Shuffle often gets overlooked, and rarely spends time in the spotlight. This is because it has, as a class of device, truly fulfilled it‚Äôs purpose as the¬†simplest music player.\n\nI would say I‚Äôm a keen runner and spend about three hours a week running. For every minute I run, my shuffle runs with me. It‚Äôs mass and modesty make it the ideal running companion.\n\nI have five running playlists:\n\n* 5k:¬†A playlist with the length of my target 5k time\n* -5k:¬†A playlist for any short run that I don‚Äôt know the distance for\n* +5k:¬†A playlist for slightly longer runs with different, slower songs\n* Long Run:¬†A really long playlist for much longer runs and events\n* Experimental:¬†New songs that I think might work for running\n\nContrary to the popular belief these are actually easy to switch between on the Shuffle. Just hold in the voice over button to hear the playlists read out. As far as running music goes it‚Äôs the best option out there, but is there room for Shuffle+?\n\nI think there is. It would be marginally more featured and aimed at runners. This is something I‚Äôve searched for on many occasions and I know there remains a vacant niche.\n\nThis new device would have two main new features:\n\n* It would have one extra button. This button would start GPS tracking (and give an audio notification of that). It would continues to log the position until pressed again. Once the device was reconnected to a computer there would be a map and movement statistics available, much like what is already available on much larger devices. There would be scope to see where each song was played too, this might help you to better choose a running playlist. Another useful addition to the current voice over would be a call out of the current distance - or for it to automatically call it out at a predefined mileage. These should all be customisable via the computer.\n* The second feature is a simple stop clock. Press a button combination to start the clock, press it again to stop. Simple as that. The current time could be combined with the distance read out.\n\nThe technology exists, the market exists - howbeit the device does not, and it should.","date":"Dec 10 2012","id":57,"title":"The underrated iPod Shuffle","type":"blog post","url":"/posts/2012-12-10-tuneful-tracking/"},{"body":"At school, and now university, it seems trendy to have lower level assessments\nsat via¬†[computer](http://en.wikipedia.org/wiki/E-assessment)¬†- students like\nit for the instant results and teachers¬†**love**¬†the absence of any real\nmarking.  While computer based testing can be handy, and has definite\npotential, it currently has some very apparent short-comings.\n\nThere is really one type of question available: multiple choice. Any other\nquestion types can become very ambiguous and extremely challenging to mark\n(automatically). Take the question:¬†_‚ÄúWhat is the most demanding user interface\non system resources?‚Äù_¬†as an example. Variations on the correct answer are\nmultiple:¬†_Graphical, Graphical UI, Graphical Interface, Graphical User\nInterface_¬†and so on. Now imagine that with the common spelling mistakes or\ncase changes and you‚Äôve got a marking nightmare on your hands. Multiple choice\nit is then‚Ä¶\n\nThis is a problem though, there exists fundamental restrictions on what you can\nask with multiple choice. You might think that¬†_‚Äòcomplete the sentences‚Äô_would\nwork but candidates still pick up more marks on their own answer than\ncompleting a given one. This would suggest that people need to explain things\nin their own words to fully represent their understanding. This makes it\nchallenging to ask anything other than maths related questions, and even then\nthere are problems.\n\nWith maths though the issues aren‚Äôt as apparent and lurk behind the questions\nthemselves. You can‚Äôt have answers that are obviously wrong, why even bother\nputting them in? You also want to try and stop people from just guessing. This\nmeans that you have lots of plausible answers with penalty for getting the\nwrong one. Lets assume we have a candidate who doesn‚Äôt guess and forgets one\nstep in a five step question - they‚Äôve now had marks subtracted, fallen prey to\nthe guesser traps. Even when they did 80% of the question correct.","date":"Dec 12 2012","id":58,"title":"Where online MCQ tests fall short","type":"blog post","url":"/posts/2012-12-12-imitation-invigilators/"},{"body":"One of the games I like to play is called¬†[Frozen Synapse](http://www.frozensynapse.com/). It‚Äôs a turn based strategy game that lends it‚Äôs self nicely to competitive play. You‚Äôre reading about it here because: It‚Äôs not as well know as it should be, it‚Äôs British and above all it‚Äôs a great game.\n\nHow it works is as follows. You‚Äôre given tactical situations to resolve and in your favour. You have total control of over units who will walk backwards into fire or eliminate the enemy team depending on your command. You choose how they walk, wait and wipe-out the enemy team. This amount of precision is great and is essential in planning elegant plays.\n\nYou have to move make your units act in such a way that you think will work worst for your opponent - this is what makes it fun and challenging. You want to make sure you‚Äôre aiming at that door before they come out, make sure the guy you use as a distraction doesn‚Äôt die and so on. Of course the enemy plan is for the exact opposite - this makes it all a game of prediction where it pays to learn your opponents style.\n\nIt‚Äôs a great idea, and the interface and tools available to design plays make it nothing short of incredible.\n\nI‚Äôve played multiplayer but found the AI players also very good. So good that they are near impossible to beat on the higher settings. A nice touch is that there are different styles of AI player, super aggressive/cautious and so on, this combined with literally hundreds of pre-game options makes for something that never grows old.\n\nAnother nice thing is that the game only requires¬†[modest specs](http://www.frozensynapse.com/features.html#sysReqs), meaning you can play it for a long time on battery and on almost any computer.\n\nThis game has picked up great reviews and has certainly made a mark, I just thought that I would do my bit to promote it as a loyal player.","date":"Dec 14 2012","id":59,"title":"Frozen Synapse [Review]","type":"blog post","url":"/posts/2012-12-14-foresight-is-futile/"},{"body":"Recently I‚Äôve had to put in some really long shifts at the computer to get some\ncoursework done. It‚Äôs done now but at its peak I was spending up-to 10 hours a\nday sitting at the computer. This just wasn‚Äôt good - I was getting a sore back\nas well as shoulder pain. You need to keep working though to get stuff done.\nThis was my solution.\n\nI knew that it was very important to take breaks and started by saying: take\nbreaks on the hour for five minutes. This, while seeming very easy just didn‚Äôt\nhappen. I would just forget, or postpone it until I forgot. I tried setting my\nalarm but it was too much of a hassle to setup every time - so the problem\nremained.\n\nThe next morning, another intensive day ahead I thought, right, there must be a\nway to get this working. I did a little research and it turns out there are\nlots of programs to help you remember your breaks. While, somewhat amazingly,\nthere are lots that you need to pay for, there was one good free solution\navailable which was what I needed.\n\nIt‚Äôs simply called:¬†[Time Out](http://www.dejal.com/timeout/).¬†When it‚Äôs time\nfor a break the screen fades and a timer ticks down showing the time left on\nthe break. There are two different break cycles. The first is for regular\nbreaks, I take 5 minutes every 55 minutes and have set them to be un-skipable.\nThe second is for micro breaks to remind you to stretch and move about, these\ncan be delayed and I take 30 seconds every 10 minutes. These settings are all\ntotally customisable though.\n\nI‚Äôve been using it for a few days now and it‚Äôs really helped me keep track of\nthe time. You would think it‚Äôs bad being interrupted but I think for me it was\njust what I needed.\n\nWhile it doesn‚Äôt make it healthy to work away at the computer for hours at a\ntime it certainly makes it hopefully less damaging and more bearable. I hope to\nbe able to use computers with as little pain as possible in years to com,\nhopefully this will help that happen.","date":"Dec 15 2012","id":60,"title":"Remembering to take breaks","type":"blog post","url":"/posts/2012-12-15-taking-time-out/"},{"body":"The problem of how to enforce¬†licensing¬†is a big one and one that certainly\ndeserves our attention. Many are trying new methods to¬†license¬†their products.\nThis is a short story about my experience with a fantastic program with an\ninteresting¬†approach¬†to the¬†problem.\n\nWith the retina screen there are\nonly¬†[certain¬†apps](http://forums.macrumors.com/showpost.php?p=15359072\u0026postcount=263)\nthat render text clearly. I was making¬†heavy use of\n[TextWrangler](http://www.barebones.com/products/textwrangler/)¬†but wanted\nsomething that looked cleaner. I spent some time looking around and settled\nwith [Sublime Text 2](http://www.sublimetext.com/), it‚Äôs a clean editor\nthat¬†rendered¬†text well on the retina display.\n\nI use the unregistered version, I didn‚Äôt want to buy it before testing it out\n(a major flaw with the mac app store I hasten to add). It‚Äôs great, perfect even\nfor my [rails](http://rubyonrails.org/) projects. There is a sidebar that has\nallows for project folders and each open file has tabs along the top. There\nare many different colorings and some great little editing tools. However the\nbest thing about the software was the a customer support response.\n\nI should first note my experiences with other developers. I\ncontacted¬†[Guided Ways](https://www.2doapp.com/)¬†and various other\ndevelopers only to be brushed off about a student¬†license. In some ways this is\nfine, smaller developers often only have one¬†license type. With Sublime Text\nthough they were quite happy let me continue using the trial for the time\nbeing.\n\n\u003e Thanks for your interest in Sublime Text. Unfortunately, there are no student\n\u003e discounts available, but since the current version does not have a time\n\u003e limit, please feel free to continue using it until you are able to make a\n\u003e purchase.\n\nWhile they still didn‚Äôt have a student¬†license¬†this response and the freedom of\nan¬†unlimited¬†trail made me think much more of the program and company. It has\nalmost¬†guaranteed¬†that I will purchase the program in the near¬†future.  I don‚Äôt\nthink this is a fix for every developer with every app but it‚Äôs certainly a\nmethod that worked here. I wonder if there will ever be a complete answer to\nthe digital¬†licensing¬†issue.","date":"Dec 18 2012","id":61,"title":"A positive experience with Sublime Text's support","type":"blog post","url":"/posts/2012-12-18-sublime-service/"},{"body":"I‚Äôm a critic of app stores, I think they‚Äôve got a long way to go before they\nbecome a good thing. They are in danger of becoming all that the consumer\nexpects from a software distribution system, I worry there will be no incentive\nfor content providers to build on the experience.\n\nI‚Äôve already covered the [point](/posts/2012-12-08-scrapp-stores) that they\nare generally terrible to browse, here are what I see as two other crucial\nflaws in the App Store model.\n\nThe most¬†prominent¬†is the lack of software trials. Fully functional apps that\ncease to function at the end of the demo period. The key part is the\n100%¬†functionality, this is needed to truly make an informed purchasing\ndecision. There are plenty of lite versions out there but they don‚Äôt offer\nfull¬†functionality. They also make the stores terribly¬†cluttered¬†places by\ncreating multiple instances of each product. Would it really be so hard to\nintroduce a ‚ÄòDownload as Trial‚Äô option to the item page?\n\nThe second issue is the¬†absence¬†of¬†license¬†types, you either own the software\nor you don‚Äôt. There are no trial users, there are no bulk users, there are no\nstudent users - just users. I‚Äôm not suggesting that there should be a student\nversion of Angry Birds but I don‚Äôt think app stores should be an excuse for the\nlack of¬†license¬†types.\n\nThese suggestions don‚Äôt come for free, they have their own problems. For\ninstance how to verify students, stop offering the discounts when they stop\nbeing students and how to stop them getting the app for others who\ndon't¬†qualify. On trial front it does open you up to a greater¬†piracy¬†threat,\nany access to source prior to payment is an opportunity for cracking -\nsomething we really want to avoid.\n\nThat said, I think these are small problems and in light of what has already\nbeen overcome and there‚Äôs a good chance that these will in fact become features\n- it‚Äôs just a case of waiting.\n","date":"Dec 20 2012","id":62,"title":"Payment Without Trial","type":"blog post","url":"/posts/2012-12-20-payment-without-trial/"},{"body":"I‚Äôm mainly writing this for some friends over at Enemies Everywhere (update: site is no more), It‚Äôs a gaming blog and I offered to do a guest post. This is it:\n\nI‚Äôve covered multiplayer gaming [before](/posts/2012-12-01-coequal-contestants) - sadly, it's¬†something¬†that's held back by so many gripes it‚Äôs often hard to know where to start. I made the point that there are some aspects that need more attention. Today I‚Äôll cover some things that distract from the real work towards the perfect shooter.\n\nCustomisation is the first snag, it sits forever at the figurehead of marketing¬†campaigns¬†because¬†it‚Äôs easy to sell. It‚Äôs easy to show off the variety of the new weapons, perks and power-ups you‚Äôve coded ¬† in, really easy. Thanks to this they have been moved to pole position and are often all that is talked about the new game. You can‚Äôt blame developers for this and there have been some really original and noteworthy¬†specimens¬†from this cycle. However I think it is _taking up too much time_ and is only¬†obscuring¬†from problems closer to the core of the system being sold.\n\nGraphics is next on the hit list. Don‚Äôt get me wrong, the work on high end graphics is making the world a better place not only for your eyes but for¬†[science](http://www.nvidia.com/object/what-is-gpu-computing.html)¬†too. It‚Äôs crucial that we keep improving our¬†graphical¬†technologies, both hardware and software, but I feel they bear little relevance when it comes to¬†multiplayer¬†shooters. Sure we don‚Äôt want to run around an untextured world but we‚Äôre aiming for 0 lag with a high frame rate and complex meshes don‚Äôt help and seriously hinder dated console performance. High end graphics should remain the forefront of PC gaming, fantastic immersive¬†campaigns and sandboxes are what people [want](http://en.wikipedia.org/wiki/The_Witcher_2:_Assassins_of_Kings).¬†However to expect the same visuals in the other-worldly multiplayer space would be¬†naive.\n\nThe wish for a 'multi-talented game'... All games have a speciality - some have great stories, some have huge replay value, but none have consistent acclaim in all play spaces. I say games stick to what they‚Äôre good at. Appealing to the wider demographic with¬†secondary¬†many modes is a cheap trick. Just because a game has an extra mode doesn‚Äôt make it worth playing. In my opinion Call of Duty‚Äôs campaign¬†and Mass Effect 3‚Äôs multiplayer are prime examples of this. These again are distractions, and subtract from rather than building great games.\n","date":"Dec 21 2012","id":63,"title":"Equivocating Excellence","type":"blog post","url":"/posts/2012-12-21-equivocating-excellence/"},{"body":"When you‚Äôre learning to program the variety can be very distracting, it‚Äôs hard\nto choose what to program with when there isn‚Äôt any real project. It‚Äôs just you\nand some spare time.\n\nI first learnt to program in VB.Net - in some ways it‚Äôs a good place to start.\nYou can easily create¬†programs¬†that look ‚Äòreal‚Äô, you can run them on most\nWindows boxes and they are basically good fun.\n\nStill, you can‚Äôt run them **everywhere** and the idea that it‚Äôs hard to show\neveryone¬†your¬†creation has always bothered me somewhat. I had never really\nthought about web applications until recently when I covered them on my course\nand they appear to be the answer to my sharing problem.\n\nIf they taught this kind of programming/development¬†at school and equipped\npupils with a free¬†web host then I think there would be a great deal more\ninterest. Parents would be able to see what there kids were creating and I\nthink this would lead to a greater interest in computing as a subject.\n\nSo on that note I think web, and web¬†applications,¬†is where I‚Äôm headed - I\nthink it‚Äôs the answer to my long standing cross platform¬†development¬†problem.\n\nI‚Äôve bought a really nice app called [Espresso](http://macrabbit.com/espresso/)\nfrom [MacRabbit](http://macrabbit.com/)¬†on special offer that does lots of the\nboring stuff for me.\n\nI‚Äôll be spending my spare time over the next few months learning more about web\nrelated things.","date":"Dec 22 2012","id":64,"title":"Where? But to the web!","type":"blog post","url":"/posts/2012-12-22-where-but-to-the-web/"},{"body":"I‚Äôve noticed a growing trend in the use of ‚Äòshare‚Äô functions across¬†platforms.\nIt‚Äôs an interesting concept with great variation in it‚Äôs implementation. I‚Äôll\ndiscuss the differences in sharing on Android, OSX and Windows 8.\n\nAndroid is the only place that I‚Äôve really made _any_ use of a share feature.\nIt‚Äôs used primarily for passing content between¬†applications¬†on the device\nitself rather than to an external service. I mainly tend to use it as an 'open\nwith‚Äô feature - however it‚Äôs a little more than that. It‚Äôs context sensitive\nand handles¬†different¬†file types very well. It‚Äôs closer to the combination of\ncopy + paste and open with really - it‚Äôs good and is a truly useful feature.\nThe only issue I would raise is the lack of the option to hide sharing targets.\nIt would be nice to be able to hide them as you can hide the\napplications¬†themselves, however it seems this is [not\npossible](http://forums.androidcentral.com/general-help-how/32716-how-do-i-edit-my-share-options.html).\n\nWhile [Airdrop](http://en.wikipedia.org/wiki/AirDrop) itself was actually a new\nfeature for 10.7 Lion, 'sharing‚Äô wasn't¬†brought centre stage ¬†until¬†last year\nwith Mountain Lion. For me these new options have been crippled by poor\nbehaviour under proxy servers and the lack of any API for 3rd party developers.\nIt would be nice to be able to share pictures to Google+ in the same way I do\non my phone but I guess we‚Äôll just have to wait (or not) for that. I think\nApple‚Äôs 'walled garden‚Äô trend is¬†damaging¬†user¬†experience, it‚Äôs certainly\ndamaged mine. The awkward foibles of the iOS¬†methodology¬†are slowing creeping\nin, lets just hope they don‚Äôt cripple the mac.\n\nOne of the reasons behind my lack of¬†experience with the¬†Windows 8 'share\n[charm](http://winsupersite.com/article/windows8/windows-8-feature-focus-charms-142999)‚Äô\nis due to the lack of ¬†support. It lacks sources and targets for share actions.\nThis, as with many other features of the new OS, is it‚Äôs downfall. I would say,\nhowever, that it has a much greater potential than that of OSX.\n\nI would say the separating factor is (near) global¬†visibility. Share is\n_everywhere_ on Android, this helps¬†familiarise¬†users with the feature and\nmakes them more confident to use it. Windows Charms are good too, perhaps even\nbetter setup than android, however the lack of targets turns\ngreat¬†ingenuity¬†into nothing. OSX tails behind, crippled by lack of 3rd party\nsupport and restricted access to the feature.","date":"Jan 5 2013","id":65,"title":"'Sharing' as Feature","type":"blog post","url":"/posts/2013-01-05-sharing-concept-comparison/"},{"body":"I‚Äôve really enjoyed building computers. I like how, while it‚Äôs reasonably\nsimple, you still get a feel that it‚Äôs somehow your own creation. Once you‚Äôve\nchecked for 10 fingers and toes you though want to push it a little more, ¬†make\nit work. You can buy the¬†latest¬†game, ¬£40, and watch the¬†beautiful graphics\nbring your computer¬†to its knees, only to get¬†refused¬†refund thanks to the\nwonders of modern DRM. Or you could just use a benchmarking tool to check that\nit‚Äôs up to scratch.\n\nThere are some really nice 3D benchmarks out there - my two¬†favourites¬†would\nhave to be [Heaven](http://unigine.com/products/heaven/) and\n[3DMark](http://www.3dmark.com/). They‚Äôre both totally free and give you a much\nbetter idea of how well games will perform for real. While games still have the\nspecs on the back it‚Äôs often hard to compare, using a benchmark give much\ngreater accuracy.\n\nThese tools basically display scenes that challenge the core components and\nmeasure the key factor of smooth game play: frame rate or FPS. Numbers and\nscores aside, there are some really nice graphics to watch while\nyour¬†computer¬†trundles through the tests.\n\nI know that this may seem kind of¬†obvious but I still get plenty of questions\nfrom people asking how well games will run on their new¬†computers.\n\nOn the topic of benchmarks there is another tool that deserves a mention,\n[Geekbench](http://browser.primatelabs.com/user/charlieegan3/geekbench2).\nGeekbench doesn‚Äôt have a fancy 3D world but instead uses a set of tests\nto¬†evaluate¬†CPU and RAM performance. The great thing about Geekbench is a\ncomparable score, you can run the suite on many devices (even phones and\ntablets) and get a meaningful score out of them.","date":"Jan 7 2013","id":66,"title":"Some benchmarking tools","type":"blog post","url":"/posts/2013-01-07-benchmark-boundaries/"},{"body":"As part of an upcoming exam I‚Äôve got to write sections of code in the language\nwe were taught. We‚Äôve had to do this kind of thing before, at school, and I‚Äôve\nbeen fine with it. This as a method to access ones programming ability just\nseems to me to be so fundamentally flawed.\n\nIf you‚Äôve ever written any kind of program you‚Äôll know that it‚Äôs rare to get it\nspot on first time - perhaps never on the first time. We‚Äôre taught this is\nfine, even good. We‚Äôre taught how to test programs and rollback to prior\nversions to fix errors - mistakes are a given and it‚Äôs our job to fix them.\n\nThis may be different if you were to only write in one language at any time but\nfor students, like me, you‚Äôre often learning more than one _new_ language and\nit‚Äôs easy to mix up the different¬†syntaxes. New languages are designed\nsimilarly to older ones to make them easily picked up but because¬†of this it\ncan get rather confusing.\n\nThese kind of¬†problems¬†though are easily sorted at a computer though, a\nreminder is nearly always just a quick Google search away. In exams though you\ndon‚Äôt have a computer, it‚Äôs just you and your paper.\n\nThis brings in many different biases and creates a\nrather¬†inadequate¬†assessment¬†in my opinion.\n\nFirst off, you don‚Äôt have access¬†to the tools that are _always_¬†present when\nyou‚Äôre working in the real world - no auto-complete, no reference - basically\nno help. This means that you have to write perfectly executable¬†code, on paper\nwith no aids. Like taking a language writing exam with no dictionary.\n\nThat‚Äôs not the only problem, the lack of help I can live with but the inability\nto check your solution‚Äôs correctness is just far too false. When you write a\nsolution you‚Äôre always running it, reading the exceptions and making\nadjustments - it‚Äôs all part of the skill. I would even argue that the ability\nto fix errors is of greater value than¬†memorising syntax. It‚Äôs more than\nknowledge, it‚Äôs a way of thinking about a problem where exceptions¬†are like\nclues to the mystery. It also transfers well between¬†environments which only\nmakes it all the more valuable.\n\nTo be able to pass the programming questions with top marks you basically need\na photographic memory¬†because¬†it _must be¬†executable_. This means that even if\nyou have all the ‚Äòsteps‚Äô (even all the lines) included you can still loose\nmarks for the wrong kind of¬†parentheses!¬†In a French test for example you\nshould loose marks for wrong words as it makes it less understandable. Here\nthough, answers with mistakes that would be fixed by auto correction or easily\nspotted after a trial run should still be able to claim full marks.\n\nI personally think that the _only_¬†way to test programming ability is by\njudging a completed project.¬†There is nothing to be gained from blocking\ninternet access or preventing discussion - plagiarism¬†is always obvious too.\nIn¬†recruitment¬†candidates¬†can be asked to write code at the interview - how\noften do you think this is done in a silent exam hall with pen and paper?","date":"Jan 10 2013","id":67,"title":"Why I don't like writing code in exams","type":"blog post","url":"/posts/2013-01-10-coding-assessment/"},{"body":"At the same time as thinking that I‚Äôm slowly growing out of video games I\nremain utterly¬†fascinated¬†by the by the universe of my favourite series: Halo.\n\nI got into Halo quite a long time ago, early primary school in fact - back then\nit was Halo: Combat Evolved. However I've¬†only¬†_closely_¬†followed\nthe¬†franchise¬†since Halo 3.\n\nCurrently Halo is in an interesting place, with a new studio¬†poised¬†to grow the\nuniverse. [They](http://www.343industries.org/news)‚Äôve got the staff, tools and\ncapital to make it happen too. While Halo 4 still¬†feels¬†hot off the press there\nis talk of the future and how Halo should play the market long term.\n\nWhile it‚Äôs not a popular option, I think they need to aim for maximum revenue\nand¬†audience, not¬†because¬†it¬†necessarily¬†creates great games but¬†because¬†it\ngenerate some new support and capital to explore more delicate markets.\n\nThere is [talk](http://www.youtube.com/watch?v=N2E9E3en-FU\u0026feature=youtu.be) of\nother genres and I see this as being really exciting. I personally would be\nvery interested in any RTS developments, not matter how far down the line they\nmay crop up. Halo Wars was a great game and was only let down by it‚Äôs poor\nplatform¬†options (Xbox Only).\n\nI think they need to make a big push to the professional arena, they need to\nlook to keep their FPS games as lean and competitive as attention rises in pro\ngaming. Whilst doing this they can explore and perhaps make a push for\nStarcraft with a¬†competitive¬†Sci-Fi RTS.\n\nWhatever they need to do they need to keep their options open and aim to pull\nin new fans with more than a million dollar advertising campaign.","date":"Jan 12 2013","id":68,"title":"The Genre Games","type":"blog post","url":"/posts/2013-01-12-the-genre-games/"},{"body":"Before taking up my degree I spent a great deal of time trying\nto¬†[researching](http://forums.macrumors.com/showthread.php?t=1350137)¬†the\nvalue of a CS degree. I asked family members in the industry and spent a great\ndeal of time online. While every one of my friends ploughed into university\nfull force, stating that it¬†was¬†_without a doubt_ the best thing to do - it may\nbe for them, but I wasn‚Äôt convinced - I‚Äôm still not convinced.\n\nHowever I came to the point where I decided I was going and in September moved\nto [Aberdeen](http://www.abdn.ac.uk/).\n\nWhile most of the time I‚Äôve been thinking about my studies, sadly the doubt\nremained, even been reinforced. Now that I‚Äôm here I have a little more\nto¬†balance¬†up. My aim is to get into relevant work and the¬†university¬†question\nhas become all about time and money. I‚Äôm spending four years and over ¬£5000\npounds a year to get this degree - but is it worthwhile?\n\nOn the money side I was almost decided that it, was in fact, worth it, it costs\na great deal more in England and the student loan is a real gift. However, I\ncan‚Äôt help but notice a discrepancy. Most of the people who gave me advice were\nolder than me, and I think degree courses might have changed. That has become\nmy new question. I get around 14 hours of contact time per week, asking my\nparents though they seem to have had a great deal more than that. I‚Äôm going to\nask around to get a better idea on this - I‚Äôm suspicious that students today\nget significantly less teaching and much poorer value for money.\n\nThe other question is one harder to research: Is a CS degree really the best\nway to train computing professionals? While I‚Äôm far from¬†qualified¬†to answer\nit, the question remains. This is an opinion that I‚Äôm prepared to alter but,\nfrom what I can tell it‚Äôs not - far from it even.\n\nI do four courses - one, Management is not directly relevant and is more of a\nprep course for a¬†Management¬†degree which is largely psychological. The rest\nare my three CS courses. Personally I think that only one, Web\nApplication¬†Development, teaches any _real skills_ - the others\nare¬†interesting¬†all the same. We use real tools and learn the basics to real\npeoples jobs - it‚Äôs seems¬†obviously¬†the most useful.\n\nIf it‚Äôs the practical courses that are perhaps the most useful then wouldn‚Äôt\nthose skills be better taught on the job? Computer\nscience can still be an option, but I‚Äôm currently in need of some explaining as\nto why it has to be the _only_ option.","date":"Jan 18 2013","id":69,"title":"Start of a Study","type":"blog post","url":"/posts/2013-01-18-start-of-a-study/"},{"body":"Now that I‚Äôm back at university after the Christmas break and starting the\nsecond term I‚Äôm reminded of the incredible number of different systems (and log\nins) I have to use. There is one system per main function yet they are in\ncompletely different places. I give you: ‚ÄòThe University of Aberdeen Portal\nSystem!‚Äô.\n\n* First up is the main student portal, it‚Äôs the one you get from the search:\n  'Aberdeen student portal‚Äô. Despite being the easiest to find and described as\n  the main one it is used the least. It‚Äôs capable of showing you your exam\n  timetable, exam results and advisory meetings - that‚Äôs all. Here you log in\n  with a Student ID, Username and Password.\n* MyAberdeen is what should be the main one. It contains the majority of our\n  learning materials and is where we get _some_ of our coursework grades. It‚Äôs\n  kept reasonably up to date but is not directly accessible from a Google\n  search and so is harder to find. It‚Äôs also the place for end of unit tests\n  and coursework submissions. For submissions it‚Äôs all good but the buggy\n  blackboard software used for tests really lets it down. Here you _only_ need\n  a Username and Password to log in.\n* The second most important place is where your time table is, it‚Äôs called 'My\n  Courses‚Äô. It‚Äôs impossible to find on a Google search and is located at:\n  /mycourses.php - you can bookmark it or type that in each time. It‚Äôs where\n  you sign up for practical and tutorial sessions and view your weekly time\n  table. Some other course work marks are given here. You need a Username and\n  Password to log in.\n* The library system is also separate. Amazingly used for accessing papers and\n  books. A Username and Password and DoB are needed to log in.\n* There is also a separate system from the library system for distributing past\n  papers. You must again use a Username and Password to Log in.\n* Student Email is the final one. I almost forgot it thanks to the wonders of\n  IMAP. It again is totally separate and is accessed with the email address and\n  a¬†different¬†password.\n\nThat‚Äôs all the systems that are needed for studying, there are some extra ones\nassociated with payments and accommodation.\n\n* eAccommodation is used to view your contract and invoices for your\n  accommodation. It‚Äôs where you report faults and sign things related to all\n  things bed. It however is not the same place where you pay for the room, ohh\n  no that would make just too much sense. You need a Student ID, (different)\n  Password and DoB (is a different format) to log in here.\n* ePayments the final system I have to use requires a Student ID, Password and\n  DoB to log in. It‚Äôs relatively simple after that though to make payments.\n\nNow, I‚Äôm wondering if it might be simpler to have _one_ log in combination and\n_one_ portal for all of that - would it be so hard?","date":"Jan 24 2013","id":70,"title":"Authentication at Aberdeen University","type":"blog post","url":"/posts/2013-01-24-lots-to-login/"},{"body":"This is again a suggested feature for mobile phones. It‚Äôs not likely to save\nthe same number of lives as yesterdays but it would be a nice feature all the\nsame.\n\nThe background: phones get notifications. Lots of them, messages, interactions,\nemails and so on. Lots of these, namely emails and interactions you get while\non your computer too.¬†\n\nThe problem: you are ‚Äòover-notified‚Äô of updates whilst at your computer.\n\nThe fix? Implement a mode called: 'Proximity Ignore‚Äô.\n\nThis feature would be user controllable but also have the option of 'sensing‚Äô\nyou were at your computer. This might be done by:\n\n  * Setting a Wifi network that was where you use your computer\n  * Noticing that you weren‚Äôt moving with GPS\n  * Noticing that the phone wasn‚Äôt been used to check the time.\n  * Setting 'Computer Use‚Äô hours in settings\n  * 'Spotting‚Äô your laptop on Bluetooth\n  * ...\n\nThis would mean an end to the annoying 'over-notification‚Äô problem and the\nendless flashing LED‚Äôs that follow.","date":"Jan 26 2013","id":71,"title":"Notification Duplication","type":"blog post","url":"/posts/2013-01-26-proximity-ignore/"},{"body":"This is the first review I‚Äôve done on here for quite a while - here‚Äôs my take on\n[IFTTT](https://ifttt.com/). IFTTT is an online automation service that brings\ntogether many online applications. Before I begin there is some IFTTT jargon\nthat you should know:\n\n* Channel: An application that you have ‚Äòlinked‚Äô.\n* Trigger: An event that a Channel has. _e.g. 'New Mail Received‚Äô._\n* Action: A task that a Channel can complete automatically.\n* Recipe: A combination of a Trigger and an Action.\n\nTo give you a feel two of my current recipes are: Post a link to all my new blog\nposts on Twitter and Add an event to my 'Places'¬†calendar¬†whenever I check in on\n[Foursquare](https://foursquare.com/charlieegan3).  All the recipes I use are\nalso used by others, there‚Äôs a sharing feature, however I‚Äôve made modifications\nto suit. Custom recipes are also really easy to set up.\n\nI mentioned not so long ago about¬†logging¬†my music - this system has some\nflaws. It only accounts for the dates the files were created and doesn‚Äôt keep a\nlog of every single track you play during a day. However, with the help of the\nlast.fm and Google Drive APIs, I have now solved this problem with IFTTT. A\nsimple Recipe is now¬†triggered¬†every time¬†I play a song, it‚Äôs action being to\nlog the song, artist and time in a¬†spreadsheet.\n\nThere are a few minor issues with the service, but none come close to being\nmajor.¬†\n\n* It‚Äôs only as good as the APIs it uses. By this I mean: If an API is weak,\n  lacking in features or none¬†existent\n  ([Google+](https://twitter.com/adamjwray/status/293896078686818304))¬†there\n  is nothing you can really do. There are a few work arounds with 'Send and\n  Email to Add‚Äô features but they are often far from perfect and fail to\n  accomplish anything close to the¬†functionality¬†of a powerful API.\n* Multi Action Recipes. Perhaps best explained with an example: I have three\n  recipes where Blogger is the trigger, all with different actions. Surely it\n  would be more¬†efficient¬†to group these? Say, when this happens, do these\n  three things - that just seems more natural to me.\n* The site‚Äôs interface is nice, however it should have a 'condensed view‚Äô for\n  viewing your recipes. It‚Äôs as if they only ever expected people to have a\n  handful of recipes. It would be nice to have a more tabular view available\n  for those users with more recipes.\n\nIf you‚Äôve read though this and still don‚Äôt really understand what it‚Äôs all\nabout\nhead¬†[here](http://lifehacker.com/5842307/how-to-supercharge-all-your-favorite-webapps-with-ifttt)¬†or\njust head over to [IFTTT](http://ifttt.com/) to‚Ä¶","date":"Jan 28 2013","id":72,"title":"IFTTT: Trigger Happy","type":"blog post","url":"/posts/2013-01-28-trigger-happy/"},{"body":"I‚Äôm quite interested in an app called [Summly](http://summly.com/), what\noriginally interested me was the coverage on\n[BBC](http://www.bbc.co.uk/news/technology-16306742). I guess I was surprised\nby his age. In an [interview](https://www.youtube.com/watch?v=NIPPYhyqs2g) he\ntalks about how he came up with the idea, it seems the idea was, originally,\nmerely a search¬†enhancement¬† I think Summly is great, however the problem\ndescribed in search remains unsolved.\n\nThe problem for him was doing a lot of reading, regularly having to load a new\nresult from search and find the content of interest. I don‚Äôt see why this has\nto be a problem though.\n\nThe add-on would monitor the search term from the Google search page or it‚Äôs\nURL. When a link was clicked this would could then be passed to the browsers\nsearch to find the search terms within the resulting page. There could be\noptions too, some might just want highlighting but there would be scope to add\nin automatic scrolling to the page location too. It depends on the nature of\nthe results page really. For a long PDF scrolling would be useful but in a news\narticle simply highlighting might be enough.\n\nYou might also want to make it so that the results page was opened in a new tab\nto preserve the original search page and it‚Äôs position.\n\nThis is clearly a problem, not a big one by any means, but something we can\ncertainly improve in terms of¬†usability.","date":"Jan 31 2013","id":73,"title":"Passing keywords to search engine results","type":"blog post","url":"/posts/2013-01-31-find-in-result/"},{"body":"This time last year I was in the process of getting in to\n[XNA](http://en.wikipedia.org/wiki/Microsoft_XNA)¬†with C# on PC, it was how I\nspent my spare time back then. Year year I‚Äôve spending a large amount of time\nwith¬†[Ruby on Rails](http://en.wikipedia.org/wiki/Ruby_on_Rails). I‚Äôd say that\nin many ways I‚Äôm enjoying Rails more - partly because I can see how it includes\na broader skill set (HTML, CSS, JavaScript‚Ä¶). I‚Äôm finding it harder to learn\nthough. There is a need to be¬†continually¬†connected to the internet just to\nlook up the syntax and error messages that come with every minute of work. With\nC# I didn‚Äôt have a book¬†but I did have Visual Studio.\n\nVisual Studio has one big bonus over my\n[current](/posts/2012-12-18-sublime-service)\n[setup](http://macrabbit.com/espresso/)¬†- _real_ code completion.\n\nThis made¬†experimentation¬†as a learner a huge¬†amount¬†more successful. It would\nalways be guessing at what you might want to do - working away to beat you to\nit. This not only made it faster when you‚Äôd got the idea but it\nhelped¬†enormously¬†when you didn‚Äôt quite know what you were doing. As long as\nyou got the start right it would often point out the rest (with the help of\nsome educated guessing).\n\nIn terms of the languages themselves there‚Äôs very little in it - they are\nquite¬†similar¬†to look at (although I would say that I find C# more readable).\nIt‚Äôs just the tools available that set them¬†apart. I‚Äôm getting to grips with it\nbut I think that it will be a little while before I can write my own code\nrather than just adjusting and piecing together¬†code from various tutorials.\n","date":"Feb 4 2013","id":74,"title":"Autocompletion as a tool for learning","type":"blog post","url":"/posts/2013-02-04-suggested-learning/"},{"body":"I‚Äôve been working quite a bit with Rails recently and have gotten well\nand¬†truly¬†stuck. I finally gave in after about 5 hours yesterday and posted\n[here](http://stackoverflow.com/questions/14665257/rails-linked-models-undefined-method-for-nilnilclass),\non stackoverflow. I‚Äôm currently waiting it out. Sadly the response hasn‚Äôt been\nthe same as it has been in the past - perhaps just¬†because¬†it‚Äôs the weekend.\n\nAll that I‚Äôm trying to do is build a simple 2-level comment system for my\nproject. There is a comment left and then a _single_ response. It seems\nrepeating the same technique used to create comments on posts recursively has\nnot worked. Anyway this brings me on to my topic.\n\nMuch the same as yesterday‚Äôs topic, today‚Äôs is about learning RoR. I‚Äôm finding\nit much harder to grasp than anything I‚Äôve ever had to learn before. I‚Äôve yet\nto experiment _and get something right_ - all I‚Äôve really been able to do is\nbend guides to suit what I want them do to. What I‚Äôm trying to say is that I‚Äôve\nyet to complete a problem totally independently.\n\nWhat is it about the Rails Framework that I‚Äôm finding so awkward?\n\nFirstly I would say that it is fundamentally more complex than what I‚Äôve had to\ndo before. There are many more related components and more needs to be done in\n  separate areas to make a single feature work.\n\nThe second major one is the exception handling. In Visual Studio exceptions are\ntailored to your code, the code in question is highlighted and suggestions\nmade. In Rails there is none of that there seem to fewer error messages that\nmean more things - certain at the level I‚Äôm at anyway. This means that when you\ndo go looking for help online there are many people with the same¬†exception¬†but\nfew with the same problem underneath.\n\nThese two issues combined make the work a fair bit harder to get done and keep\nyou stuck on single problems for longer. There is just little to go on when it\ncomes to getting to the root of a problem.\n\nI will stick at it for now.","date":"Feb 5 2013","id":75,"title":"Getting stuck \u0026 self-teaching Rails","type":"blog post","url":"/posts/2013-02-05-waiting-game/"},{"body":"This little¬†adjustment¬†has probably saved me hours over the past few months and\nis a must for any programmer.\n\n1. Open_ System Preferences._\n2. Open the¬†_Keyboard_ preference pane.\n3. Choose the _Keyboard Shortcuts_¬†tab at the top\n4. Then find and select _Services_ on the left.\n5. Then find and check the¬†_New Terminal at Folder_¬†item.\n\nNow the services menu when right-clicking on a folder contains¬†_New Terminal at Folder._\n\nI‚Äôve also found the terminal built into [Sublime Text 2](/posts/2012-12-18-sublime-service)\nquite handy recently.\n","date":"Feb 8 2013","id":76,"title":"Terminal-at-Folder","type":"blog post","url":"/posts/2013-02-08-terminal-at-folder/"},{"body":"The topic of online¬†discussion¬†interests me. Talk to who you want, about what\ninterests you and in a way that you like. While I think there are probably\nenough _places_ to take part in the discussion I think we need at least one\nmore _:method_ of structuring that conversation.\n[Branch.com](http://branch.com/) and [Discourse.org](http://discourse.org/)\nboth put forward offerings to achieve this - both are very different and, in\nmy¬†opinion, far from perfect.\n\nBranch is perhaps the most ‚Äòradical‚Äô and looks almost totally different from\nthe traditional, forum style layout. It‚Äôs about coming up with new ideas and\nlinking to interesting stories (+getting\n‚Äô[Highlights](http://bulletin.branch.com/post/40473589463/branch-opens-to-the-world)‚Äô!).\nThere are some [good\ntopics](http://branch.com/b/personal-biases-good-or-bad-for-social-product-design)\ndiscussed¬†and the¬†quality¬†is pretty high. However one crucial factor seems to\nbe missing - the capability to talk with people you don‚Äôt know. Sounds pretty\nbasic, however I would say it‚Äôs a deal breaker for me. You need to be invited\nto the branch and this feels awkward for some reason. It‚Äôs also really hard t!o\n'discover‚Äô topics and branches - like the index is still in implementation!\nThere are 'Most Highlighted‚Äô and recent discussions from people you follow but\nthere is no search and not categorical¬†organisation¬†other than groups which\nare¬†relatively¬†exclusive. Branch is a great idea but to me it falls at the 1st\nhurdle - surely these features are required in such a system?\nThey're¬†certainly¬†on my list.\n\nDiscourse is a newer one for me. It‚Äôs pitched as more of a ‚ÄúvBulletin Killer‚Äù\nand aims to replace existing forums rather than offer an alternative (like\nBranch does). It also\nhas¬†[quality¬†discussion](http://meta.discourse.org/t/how-to-arrive-at-a-conclusion/1178)¬†but\nit is¬†certainly¬†closer to the frivolity of forums. Also most of the discussion\nis about the system itself due to it being an actual product to buy rather than\na product in it‚Äôs existing state. There are many shorter and less informed\nresponses and there is a fair but of spam from people trying out the system (I\nguess interested clients). It‚Äôs very well made and¬†excels¬†in some areas that\nBranch fails in such as [discovery](http://meta.discourse.org/categories). I\nthink it‚Äôs still far from the perfect conversational structure however.\n\nThese are actually the type of system that I‚Äôd like to have a go at building\nmyself one day.","date":"Feb 9 2013","id":77,"title":"Thinking about Branch.com \u0026 Discourse.org","type":"blog post","url":"/posts/2013-02-09-branch-com-and-discourse-org/"},{"body":"I used to take quite a few photos - digital photography and Photoshop were\nreally what got me into computers in the first place. I did have a go at\nsubmitting to [istockphoto](http://www.istockphoto.com/)¬†with a few of my\nbetter ones. They were only ever interested in ‚Äòhappy people shots‚Äô and I\ndidn‚Äôt like taking¬†pictures¬†of people - so that died off. The next thing was\n[deviantart.com](http://deviantart.com/) a few years later but I got bored of\nthat too and closed my account.\n\nThere‚Äôs been a serious amount of down time on the photo front and\nI've¬†decide¬†to change that. With my phone I can use sharing services right from\nthat and geotag photos - add an extra dimension.\n\nI‚Äôve been looking into [flickr](http://www.flickr.com/photos/92984649@N06/) and\n[Instagram](http://instagram.com/charlieegan3) and now have both. Instagram\ngets a bad rap but quite frankly has the better app. I‚Äôve set up a few IFTTT\nrecipes to sync new photos to flickr and on to Twitter after that to get round\nthe Instagram anti-Twitter policy.\n\nI‚Äôm liking it so far and have added one new photo (others taken previously) -\nthinking about trying to do one each day - however I doubt there are that many\nnice places to photograph on campus!","date":"Feb 10 2013","id":78,"title":"Where should I post photos","type":"blog post","url":"/posts/2013-02-10-photos-revisited/"},{"body":"Devices are getting smarter and this helps us most of the time. However, My\nphone and laptop both have a disabled setting in common: _Automatically Adjust\nBrightness_. It _should_ be a useful feature. It‚Äôs not for me - and seems\nto only create a distraction. When you‚Äôre sitting still it‚Äôs fine (perfectly\nstill) - it may even count as useful. However as soon as you move around the\ndisplay¬†practically¬†flashes to¬†compensate for what is _perceives_ as new\nlighting conditions.\n\nIt‚Äôs the rate of change that‚Äôs the problem - at least slow it a little! The\ncontroller for this feature seems happy to make swinging adjustments almost\ncontinuously. I need some more options:\n\n* The option to set¬†boundaries¬†on the variation. For example don‚Äôt set at\n  greater than 80% or less than 20%.\n* Also a setting on the sensitivity - just the option to tone down the rapid\n  reactions a little. Only after a significant change in lighting make the\n  change.\n* Rate of Change. The setting to reduce the¬†maximum¬†'speed‚Äô at which the\n  brightness can change. Set lower this would hopefully give a\n  more¬†gradual¬†adjustment in¬†brightness¬†and in turn be less distracting.\n* The option to **turn it off** is the¬†also¬†essential - lets hope it stays.","date":"Feb 13 2013","id":79,"title":"Automatic brightness adjustment doesn't work yet","type":"blog post","url":"/posts/2013-02-13-robotic-radiance/"},{"body":"I‚Äôll first describe how things worked at [Dingwall\nAcademy](http://www.dingwallacademy.com/), my secondary school. There was a\nproxy server that all web traffic went through - it ran\n[Websense](http://en.wikipedia.org/wiki/Websense). The same filtering software\nran at Guantanamo Bay. It‚Äôs configuration blocked much of the\nuseful internet. This was a constant pain for us students and there¬†was much\ntime spent bribing supply teachers for their logins (the only way we could get\nround the problem!).\n\nThe level of filter must have been set to max for every category. There was no\n_Japan_ in geography and no _pixels_ in computing. Risqu√© terms like that never\nmade it through. It wasn‚Äôt the keywords that were the problem, it was more the\nbroader category blocks such as: forums, youtube and webmail that were the\nproblem.\n\nSome of these categories could have provided some very useful information for\nmany of my subjects - but such a resource may as well have not existed.\n\nThe one that bothered me the most was the ‚Äúmessage boards and forums‚Äù. Under\nthis category were: Stackoverflow and MSDN - two crucial resources for a 6th\nyear computing student. It would have been useful at times to have access\nto youtube as well - all were locked down like they were the pages of\ncriminals.\n\nI think this failure of a system came about from a fundamental lack of\nunderstanding in two areas: the internet at a resource, the nature of skills\ntaught in some subjects (namely computing in my case).\n\nIt was clear to see how blocking tactic that had been employed - it might as\nwell have been a whitelist it was that bad. Someone had thought of everyplace\nthat a pupil _could_ waste time and just blocked it without any consideration\nas to what good could and would come from the resource. YouTube is the perfect\nexample here, true it‚Äôs a top spot for time wasting but the number of\neducational videos or videos of internet is staggering. In the same way it‚Äôs up\nto the teacher to stop pupils chatting it‚Äôs up to them to stop time wasting\nonline. The blanket blocking can take innocent casualties too, like MSDN. Few\nwaste time there and those that do should, perhaps, be encouraged to do so!\n\nThe other problem is that to know what not to block it‚Äôs important to\nunderstand the kinds of materials that are needed for each subject. Forums and\ndiscussion boards are key here. Programming basics rarely come from\nbooks¬†anymore and tools like stackoverflow offer a much better problem\norientated resource for those past the basics. To the untrained eye however it\nlooks like a rather well built nerd chatroom - which in turn get‚Äôs it its\nblocked status. Tragic.\n\nNow I visit stackoverlfow as much as I like. Looking back I should likely have\npushed to have had the filtering adjusted...","date":"Feb 16 2013","id":80,"title":"Learning programming through websense","type":"blog post","url":"/posts/2013-02-16-limits-to-a-level-of-literacy/"},{"body":"I mentioned a few [posts](/posts/2013-02-10-photos-revisited) back that I\nwas going to try and get back into taking some photos again - mainly just a a\nlittle log of where I‚Äôve been. I‚Äôve been using Instagram, it‚Äôs a new one for me\nand this is my experience so far.\n\nI would describe Instagram as less functional Twitter of photos edited to how\nour mind sees them. Sounds rather daft but the effects are quite, prevalent\nshall we say! The effects often make the photos look for far from realistic but\ninstead more arty and I would say as how we remember scenes. Take a nice\nsunset, (a subject the app is full of) how you remember is is different to how\nthe photos look. Often the effects bring the the image closer to ones memory of\nthe scene. I don‚Äôt see this as a bad thing - some others do however‚Ä¶\n\nInstagram gets a bad rap but I think it‚Äôs quite a good service. I like, for the\nsame reason I like Twitter, that it‚Äôs totally public and you know that from the\nword go. No complex privacy settings to bother with. It‚Äôs definitely Twitter\ninspired and the app plays quite similarly. Both very well.\n\nI‚Äôve found that tags really give you a big response on a new photo - not sure\nif that‚Äôs good as it gives the chance for bogus tags which are, without a\ndoubt, a bad thing. Still I don‚Äôt really search for images and so aren‚Äôt a\nvictim of that - also I don‚Äôt over tag my photos and I get a couple of likes\non my photos all the same.\n\nIn terms of content quality, the range is huge. For every professional there\nare ten, meme-posting teens. It doesn‚Äôt bother me - I just wonder why someone\nwouldn‚Äôt want to post there own pictures? That‚Äôs how I‚Äôve enjoy using the\nservice.\n","date":"Feb 17 2013","id":81,"title":"Instagram - First Impressions","type":"blog post","url":"/posts/2013-02-17-instagram-first-impressions/"},{"body":"At the weekend Bungie officially revealed Destiny - their next IP. While some\nconcepts and details were leaked and highly speculated a more concrete story\ncame on Sunday.\n\nIt‚Äôs called Destiny, and is the Halo project predecessor. It‚Äôs the got the\nbacking of Activision and will push to current and next generation consoles.\nThe project will last a minimum of 10 years - lets hope it works out.\n\nThe game was almost always going to be a FPS, it made sense. Bungie, no matter\nwhich side of the argument you‚Äôre on, changed FPS shooters for better and for\nalways. For the rather talented team to tackle something else would be a very\nrisky venture indeed. Despite it still being an FPS it looks, and sounds, like\nit‚Äôs going to play very differently indeed. At the press event there could have\nbeen a lot more given away however it would seem the game is going to evolve on\nthe current state of the MMO. It looked like a large world where players\nmatched beneath the surface and seamlessly integrated into the same world. It\nseems they‚Äôre not running of the server model though and so how they structure\nthe total mass of players will be interesting to see. With online shooters as\nthey exist today the peer model makes games massively expandable and very\nflexible to the total number of players online at that time. Every 8 players =\nAdd a new game, simple. With this tough I can see it being somewhat more\ncomplex, they‚Äôre going to need to keep players campaigns in sync or exist in\nsome form of harmony at least. This is going to be really challenging, this was\nmade quite clear at the press event, however if they can make it work then it‚Äôs\ngoing to be revolutionary.\n\nThe visual style also looks great, it was described as blend between Halo and\nMass Effect - a game with an outstanding visual appeal. The concept art looks\nvery interesting indeed - I was also pleased to see more fantasy/rural settings\n(even though it‚Äôs set in a city). It‚Äôs set in the last city on earth but I‚Äôm\ngoing to try and avoid story details - I find that ‚Äòover researching‚Äô before\nthe games release can totally damage the game before it‚Äôs even been released.\nI‚Äôm going to take an interest in the technology side but aim to reduce exposure\nto leaked/non bungie coverage.\n\nOn a final note, I couldn‚Äôt help but note some of the tech covered in the\nvideo. There was clearly an app for iOS in development alongside main game. I‚Äôm\ngoing to take a guess that this will be used during the game to view stats or\nmeet up with friends. There was a big emphasis on seamless menus or just a lack\nof them. It sounded like the player was to totally removed from any technical\ndetails - an interesting move too. I like the idea but in practise, my\nfavourite games at the moment are all reasonably technically 'involved‚Äô, Frozen\nSynapse comes to mind.\n\nAs long as I can use Bumper Jump or Recon as my button layout in the new game\nI‚Äôll be happy! Bungie are back?","date":"Feb 19 2013","id":82,"title":"Bungie \u0026 Destiny","type":"blog post","url":"/posts/2013-02-19-bungie-destiny/"},{"body":"I about a week ago I read a an\n[article](http://www.bbc.co.uk/news/technology-21371609) on by BBC on Script\nKiddies cheating Microtransation systems in games. I found the story\nannoying for a number of reasons. The first was the way that ‚Äòoffenders‚Äô were\ndiscussed.\n\n\u003e Most were written using basic coding languages such as Visual Basic and C#,\n\u003e and were written in a way that contain quite literal schoolboy errors that\n\u003e professional hackers were unlikely to make - many exposing the original\n\u003e source of the code.\n\nFirst off, C# and Visual Basic can be used to make¬†complex¬†systems. I fail to\nsee how the¬†readability¬†syntax has any meaning here - bear in mind these\nlanguages may have been the only/most accessible way to implement such a tool.\nThat doesn‚Äôt make them 'basic‚Äô - MSDN would lap up that tone‚Ä¶\n\nThe _11 year old kids_ made mistakes professional programmers with CS degrees\nand years of experience were¬†unlikely¬†to make. Isn‚Äôt this a¬†surprise? 11 year\nold kids make mistakes in spoken English too and we don‚Äôt turn our noses up at\nthat.\n\n\u003e You teach your children that you can‚Äôt take a toy without paying - so I think\n\u003e this type of a message needs to get to the kids when they‚Äôre writing software\n\u003e too.\n\nOkay, digital products can be replicated an infinite number of times - they\nhave no manufacturing costs. ¬†While it is perhaps wrong that they can get stuff\nfor free it also brings into question: should developers be able to sell\ncontent in this way? This is up for debate and is currently a subject\nI'm¬†undecided¬†on.\n\nI think what is damaging about the article through is it gives kids interested\nin programming a bad name, a reason to be watched and worried over. While it‚Äôs\nnot as good as making a website or an app, you‚Äôve got to start somewhere, and\nthe skills learned will eventually benefit the industry.","date":"Feb 22 2013","id":83,"title":"The BBC doesn't understand the Internet - again","type":"blog post","url":"/posts/2013-02-22-script-kiddies-and-the-micropayment-model/"},{"body":"I recently downloaded [Umano](http://umanoapp.com/). It‚Äôs an app that gives you\naccess to spoken news on your mobile device. I get most of my news via twitter\nthese days, and while I‚Äôm quite happy with that system it‚Äôs sometimes nicer to\nget a little more depth.\n\nI‚Äôve started listening to a few articles (each around 5 minutes) before I go to\nbed. There‚Äôs a pretty good range in the ‚ÄòGeeky‚Äô section and it‚Äôs kept\nup-to-date. I‚Äôve listened to interviews with Facebook engineers and some good\nGoogle analysis - it‚Äôs generally been very good.\n\nYou have a playlist, one of them. You press and hold on a story in the library\nto add it to your playlist. Then, on browsing to your playlist, you can wait\nfor all the articles to download over WiFi for later listening, or just listen\nto them then and there.\n\nThere are a few problems that can make it a little awkward to use however.\nStories count as played too quickly which means you can loose some if you stop\n5 seconds into the start to the playlist. Also when you select a story to\nread the summary it shouldn‚Äôt start playing¬†straight¬†away - true you should be\nable to play with it going into the playlist but it should be a choice. There\ncould also be more ways to sort and view the playlist.\n\nI think it‚Äôs a great service ¬†- I‚Äôve certainly enjoyed using it - and I think\nit‚Äôs got some potential with lazy readers like me. I‚Äôd recommend it.","date":"Feb 24 2013","id":84,"title":"Umano","type":"blog post","url":"/posts/2013-02-24-umano-is-unique/"},{"body":"A problem that I‚Äôve had since upgrading to 10.8¬†Mountain¬†Lion has been with the\nOpen With menu. I find that over time there is a build up of duplicates.\nPrimary offenders seem to be: Pixelmator and Evernote. This seems to solve the\nproblem:\n\n    /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/Support/lsregister -kill -r -domain local -domain user\n    killall Finder\n    echo \"Open With has been rebuilt, Finder will relaunch\"\n\nThen Relaunching the Finder with \u003ccode\u003e‚éá+cmd+esc\u003c/code\u003e¬†should solve the\nproblem. However I found that the issue kept coming back. I just decided to\nremove all the apps and assign apps manually. You can do the same by following\nthis guide [here](http://osxdaily.com/2011/02/03/clear-open-with-menu-mac/).","date":"Feb 26 2013","id":85,"title":"Remove Dupes from the 'Open With' Menu","type":"blog post","url":"/posts/2013-02-26-remove-dupes-from-the-open-with-menu/"},{"body":"Keymonk was really great and it felt like a truly innovative product. A few\nweeks ago I read\n[this](http://lifehacker.com/5974067/swype-adds-crowd+sourced-dictionaries-for-better-word-prediction?tag=android-downloads)\nand decided it was time for a change once again. Swype had been the keyboard of\nchoice before Keymonk. I think what I really want is a mix of the two.\n\nFrom Keymonk I want the two finger¬†capability¬†- I see that is a big plus point.\nIt just seems so odd that when you Swype you have to start only using one\nfinger? Especially when you can use two on all the others! Keymonk also allows\nswiping to remove whole words which is a really useful feature. Keymonk also\nlooks better - in my opinion - and is certainly easier to discover and install.\nI‚Äôm not really sure why Swype isn‚Äôt on Google Play.\n\nSwype however has the key feature for a sloppy typist - accuracy. Yes, it turns\nmy wild swipes into words better than any other option out there.\nIt's¬†predictive¬†options and integration are also a plus point. Recently it‚Äôs\nalso gained the third party libraries¬†making¬†it even more accurate.\n\nSo I'm back to Swype. However, I‚Äôll remain on the lookout for a keyboard that\nallows me to:\n\n* Utilise my two thumbs for swiping\n* Correct my sloppy swipes¬†across¬†the screen\n* + is reliable \u0026 kept updated","date":"Feb 27 2013","id":86,"title":"Keymonk and Swype","type":"blog post","url":"/posts/2013-02-27-keymonk-and-swype/"},{"body":"Online assessments are have become a big part of life since my move to\nuniversity last year. They‚Äôre typically in the form of a web based\nquestionnaire that are accessible via the university student portal.\n\nThe idea, in theory, is sound, however there numerous issues that really\nlet them down. Here‚Äôs what, I think, needs to change.\n\n* _Scoring Clarity_ - One of the most obvious issues with these tests is that\n  the candidate often has no way of determining the available marks and\n  penalties. For example: How much is this question worth in relation to the\n  total/last question? What is the penalty for getting this one wrong? It needs\n  to be a very clear statement about this either at the beginning, or next to\n  each question if they‚Äôre different per question.\n* _Undo / selecting none_ - Often multiple choice questions are done with radio\n  buttons, and once you‚Äôve chosen one you‚Äôre stuck with making an answer. This\n  is all well and good until you introduce negative marking - then it becomes\n  the biggest flaw in the book, as it almost forces you to answer. This is\n  again simple to fix, just add a none answer and give it 0 points, rather than\n  a negative score.\n* _How many answers?_ - This time it‚Äôs not the buttons but rather the question\n  wording, if you can select more than one answer then you need to know how\n  many to choose! But you would think that everyone would spot that‚Ä¶nope‚Ä¶\n* _Cut out cheating_ - This basically devalues the entire system. I think these\n  tests should be done in class time to ensure that they are valued correctly.\n  Until then no one is going to take them seriously.\n* _Select the essay_ - you can‚Äôt do that. Not all questions can be boiled down\n  to choosing the best answer. Some responses need written answers and these\n  really do need marking. Until we build a 'marker AI' this is going to remain\n  a big problem. It basically severely limits the scope of use for any testing\n  system.\n* _Dropped connections_ - Lots of testing systems seem to need a continuous\n  server connection, often to enforce a timer. This is fine, and I can see why\n  it‚Äôs done, however everyone knows that the network isn‚Äôt flawless and that\n  these things lead to problems. I‚Äôm not sure how you solve this one but it‚Äôs\n  really not fair to loose out because of a blip in the connection.\n\nFixing online testing is far from an easy task but there seems to be some low\nhanging fruit.","date":"Mar 16 2013","id":87,"title":"My problems with online tests","type":"blog post","url":"/posts/2013-03-16-evil-e-assessments/"},{"body":"Gamification is a new word for me, even while the concept isn‚Äôt. Gamification\nis the process of implementing a social system that borrows elements from video\ngames.¬†\n\nI‚Äôve been aware of such set-ups with services from Foursquare, Codecademy and\nstackoverflow. In many ways it‚Äôs odd that I‚Äôd never heard the word, I certainly\nlike the idea.\n\nI like the idea that we can keep people engaged in the system and push them to\nbehave in a way that benefits them and others online. I‚Äôve seen a number of\ntechniques adopted, here are some of the better ones.\n\nBadges, medals, ribbons - call them what you like - they all work the same way.\nUsers can be rewarded for certain actions, this will add to a collection, a\ncollection of virtual achievements that represents them and their online status\nor rank. Status online is of exceptional importance, with it being so open and\noften impersonal, users needs to be able to evaluate others. These rewards\npaint a picture that helps us suss each other out.\n\nBadges also help keep users returning to the service. People get hooked, and\nthese systems exploit that. But I don‚Äôt think this is a bad thing, in fact I\nthink it helps build more full, interconnected and active communities online.\n\nBadges don‚Äôt however rely on a large user base. True, many badges are awarded\nfor social activities, however, not all badges need to be for this. Other\nbadges can help encourage lone users to participate all the same - even if\nit‚Äôs while they wait for their friends to join up. This has certainly been\nthe case for me with Foursquare.\n\nPrivilege Unlocks. My next personal favourite. These, rather than useless tags,\nare abilities that can be earned. These might include things like moderation,\nvoting, correcting and so on. Certainly if you‚Äôre getting involved with an\nonline community, these privileges can be very highly valued, and are\ndesirable. Another advantage of course, these privileges, namely moderation,\ncan keep your system cream and healthy without incurring too much additional\nwork on your part.\n\nLeader-boards is another feature I‚Äôve seen, implemented again in Foursquare.\nWhile fairly easy to implement, and understand, this can really help give user\nactivities at competitive side. Providing point allocation is sound this\ncompetition can exponentially help a system grow and flourish.\n\nOne final feature that I believe deserves a mention is the idea of Bounties.\nI‚Äôve only ever seen these at stack overflow, however, I think they‚Äôre powerful\nenough to get a mention here. Bounties need to work alongside a user score\nsystem, they can be implemented manually or automatically to promote strong\npositive deeds that are highly desired within the community. This might be\nanswering a tough question all correcting and improving an existing resource.\n\nAll in all, I think the effects of a grounded Gamification system can be so\nmuch more than just a gimmick.","date":"Mar 23 2013","id":88,"title":"Gamification for Good","type":"blog post","url":"/posts/2013-03-23-gamification-for-good/"},{"body":"While I send the majority of emails and communication by typing out messages on\na Qwerty keyboard I‚Äôm sending more and more from my phone by the week. I‚Äôm also\nusing dictation to type longer documents when I‚Äôm alone. On my phone I use a\nthe Swype keyboard, I swear by gesture based typing in fact. However Dictation\nand Gesture Based Typing can lead to some pretty bad typos.\n\nOriginally there mistkes would look like that, missing or swapped letters and\nbad punctuation. Now though spelling mistakes are taking on a new form. I‚Äôll\nuse an example, I wanted to text a friend that I was: ‚ÄúJust heading off now‚Äù. I\nwas in a rush, I was about a minute or two on the late side and I wrote the\ntext out quickly. This, lead to me typing: ‚ÄúJust getting off now.‚Äù. Wonderful\nisn‚Äôt it?\n\nPerfectly formed, our new error-ridden sentences are much harder to decipher\nand scan for errors. Mis-typed words have no mistakes, no missing letters -\nit‚Äôs as if we‚Äôre not trained to spot errors of this kind. I guess in days gone\nby we would have seen each word as it was typed. During typing would normally\nbe the best time to spot if you‚Äôre typing a completely different word‚Ä¶\n\nThese new ‚Äòlazy‚Äô methods remove that first microscopic proof reading we make -\ncompletely. Dictation is the worst, it‚Äôll buffer your whole sentence before\nsplurging the whole lot out, ready for more text. This means you just skim the\n**whole** sentence, rather than each word, often, in my case leading to some\nsignificant errors in my text.\n\nI‚Äôm not saying this is a critical flaw, far from it, I enjoy these new ways of\ntyping. It‚Äôs just an observation of a change, one that seems to require\nnew skill in proof reading to fix.","date":"Mar 30 2013","id":89,"title":"Full-on Typos","type":"blog post","url":"/posts/2013-03-30-full-on-typos/"},{"body":"I like recording things. Spending so much time online there is a vast amount of\nactivity for me to log - I thought I‚Äôd have a go, in the future it might be\nquite interesting.\n\nI started with logging my Foursquare check-ins. This has already been fun to\nlook at, I have it recorded on my calendar and in a Google Drive Spreadsheet.\nThis means that I can know see exactly which day it was I went to¬†wherever¬†it\nwas. So far it‚Äôs only been actually useful once, however, the more data I\ngather (effortlessly) the more likely it is to be of use or interest.\n\nNext up was Spotify plays, it‚Äôs always bothered me that Spotify doesn‚Äôt make a\nusers play available. They are logged, and are¬†available¬†via applescript, but\nthey aren‚Äôt shown. This made me get last.fm and take a new approach to play\ncount logging. I can now log every play of every track I ever play, from iTunes\nor Spotify in a Google spreadsheet. Some people might not find¬†this¬†interesting\nin the slightest, I¬†agree¬†it‚Äôs far from useful. Still I like the idea that in\nfuture I‚Äôll be able to look back on it all. Maybe if I ever get better at Excel\nI‚Äôll be able to pull out some interesting facts.\n\nI‚Äôve tried recording other services, like Gmail, Twitter and Blogger, but\nI run these just more as a backup or test.\n\nI get that personal analytics might be seen as a little odd but the results can\nbe pretty interesting.","date":"Apr 6 2013","id":90,"title":"Personal Analytics","type":"blog post","url":"/posts/2013-04-06-personal-analytics/"},{"body":"My first year at university came to an end yesterday at 11:30 when I finished\nmy ‚ÄòWeb Technology‚Äô exam. It‚Äôs been a fast few months but in that time I feel\nI‚Äôve learnt enough to have an opinion on the university way.\n\nI should first mention that I‚Äôve actually really enjoyed my first year and I‚Äôm\nlooking forward to coming back again this September. There are still a few\n'issues‚Äô however, here‚Äôs my take.\n\n## Aberdeen Agitations\nThe first set of problems, are, from what I can see,\nunique to the Aberdeen Computer Science department, however somewhat limited\nexperience makes that rather hard to say with any degree of certainty.\n\n_Discipline Breadth_ there‚Äôs a system that requires us to choose subjects that\nare from other departments. This, as an option, would be quite a good idea,\ngiving people the choice to learn a little broader should perhaps be\nencouraged, but never compulsory. Compulsory was what school was for. In making\nit to university, and to study Computer Science, one hopes that you‚Äôll be able\nto spend all your time doing your chosen subject, something you‚Äôd hopefully\nenjoy. You should, but it‚Äôs not the case at Aberdeen. Here there are only five\nfirst year Computer Science courses, _five out of 8 'slots‚Äô_. This means you‚Äôve\ngot to take on extra courses that often aren‚Äôt relevant or enjoyable to you.\nNot a very good system, if you‚Äôve chosen Computer Science, you should be able\nto spend your time in pursuit of that.\n\n_Course Coordination_ There were a few poorly ran courses this year, out of the\nfive that is. This involved things like: irrelevant and expensive textbooks;\ntutorials with no answers or means of getting them; a lack of past papers;\nimpossibly 'full‚Äô lectures (90 sides); unclear assignments and so on. Most of\nthese issues, but not all, upon inspection, seemed to be related to the\nconstantly refreshed courses. There‚Äôs a definite trend in the department to\ncompletely revamp courses - all the time. In theory this sounds like a good\nidea, fresh content and all that, however the reality is the list above -\ncomplete absence of valid learning material and student support. It leads to a\nrather unsatisfactory 'test subject‚Äô feeling.\n\n## The Discipline\nWhile some problems point to issues at a university level I\nthink there are,¬† more fundamental, issues with the way the subject is taught\nin general.\n\nLet‚Äôs face it, Computer Science is programming. Knowledge of other areas\n(Networking, HCI‚Ä¶) are best learnt when explored with a programming project in\nmind and behind, at least that‚Äôs how I see it. Given the chance, I‚Äôd focus the\ncourses more on current technologies, with one or two that focused on cross\nlanguage skills. For example, one course on Applied Math that takes time to\ncover implementations in many languages. Another on design patterns such as the\nMVC, again with an effort to link it to real world technologies. This would\nleave the rest open to current trends such as a course in RoR or iOS\ndevelopment.\n\nThe common argument against such an approach is: the knowledge gained from\nlooking at particular technologies depreciates very quickly. My response would\nbe: most of the things you could cover will remain valid past the end of your\ndegree. All you need is skills that get you into the workplace after\nuniversity, from then on it‚Äôs a case of learning on the job and keeping up to\ndate. Also,_ it‚Äôs not like new languages and patterns are totally alien_, they\nneed to be attractive candidates for adoption where possible. This means\nthere‚Äôs often strong similarities between languages. University should expose\nyou to as many different techniques and languages as possible to broaden your\nhorizons and help you choose areas of interest and further study.\n\n## University Education\nThe highest level problems are with the way university education works as a\nwhole.\n\nI‚Äôve had one very poorly ran course this last session, the teaching was\nterrible and I turned to on-line resources at [Khan\nAcademy](https://www.khanacademy.org/),\n[cgcc](http://www.youtube.com/user/cgcclive)¬†and¬†[UCBerkley](http://www.youtube.com/user/UCBerkeley).\nThese videos and tutorials really got me thinking. Do we need universities to\nexist as physical institutions? Do degrees need to be so lengthy and\ninflexible?\n\nI don‚Äôt think they do. As long as there‚Äôs funding to make **one **good,\nEnglish, Computer Science, course then that‚Äôs all we need. _Don‚Äôt try and get\nresearchers to teach if they can‚Äôt_, have natural teachers doing the teaching!\nAbout feedback and assessments, programs are easy to mark automatically and if\npeople do the courses as alongside work then employers can see the results.\n[On-line testing](/posts/2013-03-16-evil-e-assessments) is getting better\ntoo.\n\nPutting these measures in place would greatly reduce the cost of the whole\nordeal. Not to mention how much more accommodating the system would be for\ndifferent learners with different paces.\n\nCommunities like [Coursera](https://www.coursera.org/), [Piazza\n](https://piazza.com/)and [Khan Academy](https://www.khanacademy.org/) are all\n_well on their way_ to achieving this. Given more funding for a greater student\nbase these places have the power to, not only change the way we teach and learn\nthe subject but the industry as a whole, and definitely for the better.\n\nIn conclusion, Computer Science is an exciting area to be (trying to) get\ninvolved in, however it‚Äôs being let down by an outdated teaching model.\n","date":"Jun 1 2013","id":91,"title":"1 year down","type":"blog post","url":"/posts/2013-06-01-25-quailified/"},{"body":"Lately I‚Äôve been playing quite a bit of Mass Effect 3, namely the multiplayer.\nIt‚Äôs been good fun and I‚Äôve gotten a lot more into it than I did last time -\nI‚Äôve enjoyed taking on the more difficult levels with friends. I really like\nthe game and I‚Äôll keep playing it for a good while yet - however I really\ndislike their levelling up implementation.\n\nIt‚Äôs the random element to it that I hate, I really dislike the idea that\nyou‚Äôre not in direct control of how to level up. In games you earn credits and\nthey can be spent on ‚Äòpacks‚Äô - these contain semi-randomly assigned pieces of\nkit and unlocks. There are different types of packs, allowing you to vaguely\nsteer out your path but there‚Äôs still next to no control. Basically you have to\nput up with what you get - there‚Äôs no player influence what so ever.¬†\n\nI don‚Äôt think they‚Äôve done it to be different or original either - I think\nthey‚Äôre just trying to make it addictive. The chance element keeps you coming\nback - I guess they hope that sometimes that involves spending some Microsoft\npoints to fast track. It‚Äôs like trading card packs - you might get the best\nplayer in the league or the god card, but most likely you‚Äôll only get rubbish\nor cards you don‚Äôt want.\n\nNot only do I think it‚Äôs marginally exploitative but I don‚Äôt l also dislike the\nlack of any foresight or goal. In other games you save up for certain items -\nMass Effect 3 lacks the motivation of a long term goal. I think this is really\nimportant to keep me interested, and is perhaps the reason I stopped playing\nthe first time round.\n\nOnce you‚Äôve got a few good weapons and characters the gameplay is very good,\nand great fun to play with friends. It‚Äôs just all the time spent in the lobby,\nout of game, that really lets the experience down.","date":"Jun 8 2013","id":92,"title":"Say no to lucky-dip level-ups","type":"blog post","url":"/posts/2013-06-08-lucky-dip-level-ups/"},{"body":"Two weeks ago I started working for a local IT business as a Technician, here‚Äôs\nwhat I‚Äôve learnt.\n\nPeople are really troubled by their computers, I would go as far to say the\nolder generation are, on average, scared of them. They think that the computer\nis far more intelligent than it is - they often think it does things\nautonomously. They believe that it does things that are impossible for it to\nhave done.\n\nThe fear is often caused by over estimation of the damage they might cause.\nChances are they couldn‚Äôt loose their data if they tried. There‚Äôs a common\nmisconception that computers are built with no safeguards.\n\nPeople don‚Äôt understand their needs. For instance, if they think they get ‚Äòa\nlot of email‚Äô they assume they need a large hard-disk. In fact, all they really\nneed is a small hard-disk. People still don‚Äôt understand common space\nrequirements.\n\nSo there‚Äôs the observations. In terms of trends‚Ä¶\n\nPeople want tablets but expected to pay a 10th of the price that they paid for\ntheir laptop. Or they bring in a 5 year old toshiba and expect a free tablet.\nEither that or want all their MS Office document in 'all the same places‚Äô. For\nthose who don‚Äôt know, this is not possible!\n\nPeople want new computers but want them to behave exactly like their old ones.\nThere‚Äôs something good in keeping things similar, but please for the love of\ngod don‚Äôt let that stop you seeing improvements in interfaces and designs!\n\nPeople are happy to pay for your time out of their sight but not for your time\nthat is their time also. Totally illogical, but is understandable -\nand¬†somewhat amusing.\n\nPeople seems to think that Mac and iPads are totally different. Macs are a rip\noff, iPads are the tablet bread and butter. This interested me. iPads are the\nmost expensive tablets money can buy. Do people see them as better value than\nmacs?\n\nIt‚Äôs been a very educational experience thus far, and not just about customers.\nThere‚Äôs more to come on the topic of the IT trade.","date":"Jul 20 2013","id":93,"title":"Getting another perspective on IT","type":"blog post","url":"/posts/2013-07-20-trends-and-observations/"},{"body":"Programs that help lighten the load:\n\n## Acronis TrueImage Plus Pack\nA common request at the shop is: ‚ÄúI want a new\ncomputer _exactly_ like my old computer - only faster‚Äù. This would not be\neconomically possible if it weren‚Äôt for Acronis¬†TrueImage with plus pack. This\nhas one crucial feature: disk restore _with a driver scan.¬†_This means you can\nkeep all the same data, _programs,¬†licensees¬†and settings._ While still pulling\nin new drivers to run them in the new environment - smart data migration if you\nlike. ‚ÄòUniversal Restore‚Äô, as it‚Äôs called, has made many a happy customer.\n\n## Produkey\nI don‚Äôt trust licences - they‚Äôre an almighty pain. Produkey solves\nthat - it pulls them out of the debris and saves a simple text file. Need to\nre-activate office? No problem. There are no other tools that do this so\nquickly and¬†effectively.\n\n## WhoCrashed\nDon‚Äôt have the time to read¬†ntbtlog.txt? No problem, this one\ndoes that for you. The only pain is that it needs a to run in a Windows\nenvironment - so you‚Äôll likely need to take the disk out.\n\n## Wise Portable Registry Cleaner\nIt‚Äôs portable. It doesn‚Äôt leave remnants and\npopups on users computers. That‚Äôs good enough. Not to mention that it‚Äôs a\npretty¬†thorough¬†registry cleaner too. It‚Äôs fun to tell people you single\nhelpfully fixed _1500¬†_registry errors!\n\n## Autoruns\nAn¬†**essential**! An¬†absolute¬†must. This is the first stop in the\ntune up routine. Get rid of¬†auto starting¬†bloatware - fast! It‚Äôs easy to clean\nout everything from¬†scheduled¬†tasks to hidden startup processes. You‚Äôll find\nyou can easily create Blue Screens if you‚Äôre not careful so don‚Äôt go overboard.\nAlso, some actually like the 'Ask Toolbar Updater‚Äô - watch out for that too.\n\n## driveridentifier.com\nA¬†catalogue¬†of links to the source - it‚Äôs free to\ndownload directly from the¬†manufactures. Solves the serious problem of\nmaze-like OEM support sites. Note - _it‚Äôs not pretty_, but also, and more\nimportantly, not a scam.\n\n## Ninite\nInstall the essentials in on fell swoop. Update java, flash - the lot\nall in a oner. It‚Äôs a really handy one for setting up new PC‚Äôs. I only wish it\nhad even more options - it‚Äôd be nice to have ESET in the AV list of options.\n\n## GetDataBack\nIt‚Äôs not cheap but can help generate very happy customers! It\nscans for lost data and does an incredible job of recovering dead fragments.\nThe only downside is the cost - it‚Äôs about ¬£140‚Ä¶\n\n***\n\nSo there you have it - a list of incredible tools for setting up and fixing\ncomputer problems to add to the old¬†favourites.","date":"Aug 10 2013","id":94,"title":"My IT Technician's Tools of the Trade","type":"blog post","url":"/posts/2013-08-10-tools-of-the-trade/"},{"body":"I‚Äôve spent most of the week (a series of 12 hour days) working with a web\napplication called [vend](http://www.vendhq.com/). It‚Äôs a program of my own\nselection and I was keen to get it working in the small business before I go\nback to university.\n\nIt‚Äôs a wonderful piece of¬†software¬†and is just what the business needs.\nIt's¬†fantastically¬†simple to use and has a great set of reports built right in.\nIt also ties in with Xero, the¬†companies¬†accounts web app of my choosing.\nThere‚Äôs capacity to¬†marry¬†them all up now and I think given a little time it\nwill happen. It‚Äôs just a shame I‚Äôm not going to see it happen!\n\nVend does have some foibles though and there were times when I really worried\nabout it. It‚Äôs actually quite locked down.\n\nFirstly you **can‚Äôt delete sale data**. Only void it. I guess this makes sense\nbut if there‚Äôs a 100 test sales that I fired in during testing then voiding\nthem all is just a pain. You can delete all your data during the trial but\nafter that point there‚Äôs no going back with a chat via email to support.\n\nIt‚Äôs not as easy as it should be to print a long run of barcodes for a lot of\nproducts. You need to make a sale order (as if you‚Äôd bought in the products and\nneeded to print the labels for them). That method aside there‚Äôs no way to print\na large number of barcodes. We used generic items to GOOJF on this issue but\nit‚Äôs a far from optimal solution.\n\nCustomer files are great but notes don‚Äôt have multiple lines for some reason.\nThis makes the notes (a field that we‚Äôd used a lot in the old system) really\ncumbersome. Still, it just shows us: this is not where to store that data - and\nit probably a good thing.\n\nThere‚Äôs no option to duplicate a product, this is really quite annoying if\nyou‚Äôre adding in loads of similar products in on go as we have been this week.\nI found however that you can press back after new product posting and edit the\ndata to resubmit a new product with slightly different details. Still, little\nhacks like that shouldn‚Äôt be needed to get the job done really.\n\nIt‚Äôs basically a pretty quick system but sometimes it really hangs on product\nediting. I have no idea why that should be. It also went down for about\n30minutes on Thursday this week. However this doesn‚Äôt mean that you can‚Äôt make\nsales. The javascript in the sales page and keep a track of it in local data. I\nwas _really_ impressed by this.\n\nVend‚Äôs support has been okay, once you‚Äôre into a ticket you‚Äôre laughing but\nthat can take a few hours. With other software I‚Äôve found the service to be\nfaster. Still, there‚Äôs always the option of phone support for $39 extra - it‚Äôs\nsomething the shop might need in the end I fear.\n\nIn comparison to other options out there it just blows them out of the water.\nThere was one other option that was only in beta called beanflow.com that\nlooked promising. However, I thought it might be a little cruel to set up the\nbusiness on a system that was hardly even in private beta! Still, it did look\npromising. It had built in job tracking that I think might have been quite a\nnice feature to add in. Still, that said, we have asana.com for that and it‚Äôs\ncoping really really well with that just now.\n\nBasically I‚Äôd recommend vend. It‚Äôs been really good and looks set to totally\nturn around another small business! However, I'd¬†recommend¬†you have a good\ngrasp of CSV‚Äôs and Mapping before you look into migrating over. SKU and barcode\nknowledge¬†and the time to sort that out is also required to make it work at\nall.\n\nIn short, vend is yet another example that _we do not need to be married to_\narchaic native apps and crappy licences.","date":"Sep 7 2013","id":95,"title":"Vend POS \u0026 replacing desktop apps","type":"blog post","url":"/posts/2013-09-07-vender/"},{"body":"I thought, seeing as the exams are over and I haven‚Äôt posted in a very long\ntime, that I‚Äôd put something new up.\n\nFor a time last year I thought I‚Äôd grown out of video games, it seems this is\nnot the case! There just wasn‚Äôt anything I really wanted to play. And now there\nis. Being a loyal xbox gamer the natural choice would be: Xbox One. It can play\nTitanfall, Witcher,¬†TC The Divison and most importantly the next Halo game.\nHalo is my game, more specifically, Halo Reach was my game.\n\nI‚Äôve now decided, after Halo-4-gate that I‚Äôm not buying the next title off\nrelease. 343i have disappointed and they‚Äôve got to claw it back to bring me\nback. So as a protest of my frustration with Microsoft and 343i‚Äôs abuse of a\nquality IP I left. I built a gaming PC and spent way too much money in the\nsteam Christmas¬†sale. I‚Äôm not saying that I‚Äôll never go back, or take on both,\njust that I‚Äôm no longer going to blindly support Xbox.\n\nBoth new consoles, that‚Äôs right there are only two (side note,¬†Nintendo need to\nget out of the hardware business and push their IPs to other platforms and\ndistribution systems), failed to impress me. Neither has really pushed the boat\nout in terms of hardware and the launch titles were a bloody joke. I don‚Äôt like\nSony because of the Geohot fiasco and Microsoft for butchering Halo so I quit.\nHaving Windows 8 on my pc is a necessary evil!\n\nSo [here](http://uk.pcpartpicker.com/p/2xmMM)‚Äôs what I got myself, more than\ntwo consoles worth of parts.\n\nYeah, little overkill in some places and not enough in others but I wasn‚Äôt keen\nto spend over a grand - the Gene and the 350D likely being the biggest\nluxuries.\n\nI started out with Torchlight II, a top-down RPG, it was quite good fun and I\nplayed it for about 10 hours before I moved on temporarily. I found it was\ngetting a little repetitive and seemed to require little skill. Player\nprogression was a little boring and you tended to be better to level a single\nskill or weapon. I did like the visual style though, and it was admitted a good\nstory with a wide range of play spaces. I will go back to it in time.\n\nFollowing that I started messing around with free-to-play titles. I tried\nBlacklight and Planetside out and stuck with Blacklight. This was my first go\nwith the WASD control method in a big way and while it was fun I found that\nyears or console gaming had set me back. The community chants that it‚Äôs _not\npay to win_ and it‚Äôs not as bad as some there‚Äôs certainly enough of that to put\nme off a little. It was a nice looking game but the micro transactions system\nwas quite oddly laid out.\n\nI then moved on to Assassins Creed IV. I got it free with my GTX 770 from Scan.\nThe only issue initially was the 23GB download! It‚Äôs the biggest download I‚Äôve\never completed. I‚Äôm glad I can say it‚Äôs been worth the wait. It‚Äôs a really good\nlooking game and I‚Äôm getting good fun out of it. It did seem to break my\ncomputer a few times, and not through overheating. It was a really bizarre\nissue that I still don‚Äôt quite understand, it appears to have been some driver\nissue. I‚Äôm quite happy that after [much fiddling\naround](https://forums.bit-tech.net/index.php?threads/lost-dedicated-gpu-output.266776/) it seems to have\ngone away. Annoyingly I lost a 4 hour save and had to start again. I will make\nsure I still finish¬†the main story at least and I have already started again.\n\nFor whatever reason I thought it was time to experiment yet again. I downloaded\nDota 2. At the time of writing I‚Äôve only played 5 games but I am really\nstarting to enjoy it. It‚Äôs been quite overwhelming, there are just far to many\nthings to think about as a beginner. Which hero is the major one, there are\nover 100 each ranked on about 10 very different and important attributes. Some\nare really item dependant, many are only for certain play styles and some that\njust aren‚Äôt for noobs. They‚Äôre well balanced but it seems everyone has\ndifferent advice on the matter. It‚Äôs a real minefield but one I‚Äôm keen to\npuzzle out. It‚Äôs crazy that it‚Äôs free too, not really sure how Valve gets\nenough from it to make it worth their time.\n\nSo that‚Äôs the story of my conversion to PC gaming, so far so good.","date":"Jan 31 2014","id":96,"title":"I Built a Gaming PC","type":"blog post","url":"/posts/2014-01-31-i-built-a-gaming-pc/"},{"body":"This summer, July through September, I completed a summer internship at [Unboxed Consulting](http://www.unboxedconsulting.com) - a Rails Consultancy in London. I had a really incredible time and was lucky enough to be invited back. I really can‚Äôt wait for next year.\n\nHere are some of my key learning milestones from this summer.\n\n## CSS Selector Weights\nOkay, I really ought to have known this before arriving in London. When I first started out I was mainly working in the front end and this was a pretty crucial first lesson! Before mid July of this year I didn‚Äôt know exactly how `#id \u0026gt; .class \u0026gt; tag` fitted together. Looking back I didn‚Äôt really know at all, and now I do.\n\n## Git Branching \u0026amp; Re-basing\nI knew how to create a branch \u0026amp; how to merge them - I also understood that new code should happen in a new branch, but that was pretty much about as far as my git knowledge extended. I quickly learned about how different levels of branches related to one and other and how these were used when deploying code. I checked out my first feature branch, opened my first Pull Request and completed my first code review. I‚Äôd like to see this knowledge and strategy being particularly useful this year on the group work software engineering project.\n\nRe-basing code was also a new one for me, I‚Äôd always just assumed that messy merge commits were a necessary evil in the history - it seems this is not the case! Being able to work on a longer running feature branch and regularly stacking those changes on a new copy of master or develop became a really great tool.\n\n## Slower Coding ‚à¥ Massively Better Code\nThis is really pretty darn obvious but I still think it‚Äôs worth mentioning. I‚Äôd worked semi-professionally on Rails applications before starting my internship but only at _Unboxed_ did I learn to think about my code in an entirely different way. Before, when cramming for a student deadline I‚Äôd never bothered with code quality/readability, it‚Äôs just not really a big thing in student code. I‚Äôd never really had code reviewed in greater depth than: ‚ÄòIt works‚Äô before. And that changes everything.\n\nBeing given the time to improve and really think about my code was a really great learning experience. After a few months of this I‚Äôve been a lot less trigger happy and have spent a great deal more time looking into best practices and refactoring my code before calling it done.\n\n## TDD\nBefore using [Test Driven Development](http://en.wikipedia.org/wiki/Test-driven_development)¬†in a pairing context I‚Äôd never really understood even the basic motivation behind TDD. It took a long time, but towards the end of my time at Unboxed while pairing on a client project I finally understood what it‚Äôs all about. I gave it a test run on a personal gem project of mine and it really does work, using¬†[vcr](https://github.com/vcr/vcr)¬†and [guard](https://github.com/guard/guard) felt like a breakthrough for me as a programmer.\n\n## Decorator Pattern\nThis is another thing that‚Äôs come up a lot this summer. Following on with the theme of thinking more about my code this is something I‚Äôve come to use in my apps. The idea of separating view logic into a separate presenter class to make a lot of sense - before I‚Äôd just never really thought about it. The [Destroy all Software](https://www.destroyallsoftware.com/screencasts) screencasts have been really great at teaching me how best to set this up in fresh projects.\n\nThis summarises the key lessons I took away from this summer, there are many others that are smaller or hard to put in words. It was a really great experience, the team at Unboxed were a real joy to work with - I was made to feel extremely welcome. I just can‚Äôt to get back into that world.","date":"Sep 29 2014","id":97,"title":"Unboxed","type":"blog post","url":"/posts/2014-09-29-unboxed/"},{"body":"I‚Äôve been trying to cut down on the time I spend reading news for a long time,\nI even built my own Ruby news aggregator and summariser. Lots of NLP and lots\nof mess.\n\nI‚Äôve recently settled on a simpler solution using\n[IFTTT.com](http://www.ifttt.com).¬†Most sources tweet new content, this is\ngives a **ready summarised** feed in a¬†**consistent form** for **all sources**.\n\n[Here is the ‚ÄòRecipe‚Äô I‚Äôm\nusing](https://ifttt.com/recipes/230600-tech-news-digest)\n\nClearly you‚Äôll want to follow different accounts, or maybe less of them. This\nlists around 200 items a day. It normally takes between 5 and 10 mins to scan\nthe list.\n\nI don‚Äôt read them in the morning, only add them to 'Reading List‚Äô from the iOS\nmail client to pick them up later on. (normally while walking or eating!)\n\nI still use _Yahoo! New Digest_ for world \u0026amp; UK news.","date":"Dec 18 2014","id":98,"title":"Digesting News with Twitter and IFTTT","type":"blog post","url":"/posts/2014-12-18-digesting-news-with-twitter-and-ifttt/"},{"body":"I recently finished reading [Masters of Doom](http://www.amazon.co.uk/Masters-Doom-created-transformed-culture/dp/0749924896/ref=sr_1_1?ie=UTF8\u0026qid=1422132733\u0026sr=8-1\u0026keywords=masters+of+doom) by David Kushner and thought I‚Äôd write a short response. It‚Äôs the first book I‚Äôve read from start to finish since I was a child.\n\nThe book was lent to me by a [friend](https://twitter.com/IllegalCactus) but it was my interest in shooters and a power cut that got me to _actually sit down_ and read it. I haven‚Äôt invested the time that some have in multiplayer FPS titles but I‚Äôve definitely had a go. I think networked shooters are the most technically interesting video games - the genre certainly faces the greatest challenges. Responsive networking and graphics are heavily punished by even the tiniest blips.\n\nSo a book that covers their inception and ‚Äòfounding fathers‚Äô had be interested and ultimately got me hooked. I‚Äôve never played Doom or even Quake for that matter and you really don‚Äôt need to get the book. The id story is a great one - with some even better characters - and while depressing at times, as a whole, it‚Äôs very inspirational.\n\nIt‚Äôd recommend it to anyone who‚Äôs ever been at all invested in an FPS - as well as to any student programmer who‚Äôs into video games. There‚Äôs a lot to learn from the book - it‚Äôll really make you think.","date":"Jan 24 2015","id":99,"title":"I read a book","type":"blog post","url":"/posts/2015-01-24-i-read-a-book/"},{"body":"I‚Äôve tried time and time again to make following tech news work for me. It seems to be an inordinately tough problem (for me) to solve (for myself).\n\nIn the early days it was simple, I followed _MacRumors_ and _Engadget_. I just visited their sites regularly. As I explored a little and discovered some other sites I wanted to follow the need for a basic news reader arose. I also started to read stories on more than one device. After some experimentation I settled on _Feedly_. I got into this around the time that _Google Reader_ was being phased out and _Feedly_ certainly served me well.\n\n_Feedly_ synced reading progress well but didn‚Äôt seem all that great at syncing the news! It was often the case that upon reaching the ‚Äòend‚Äô you‚Äôd find there were another 20, and then another 20 and so on‚Ä¶ This grew to annoy me quite a bit and I started to look elsewhere.\n\nAt the beginning of 2014 it became clear there wasn‚Äôt an app or service that offered all the features I saw as essential. Marking as read, reliable story sync being the main two. Polling feeds takes time and I realized that I needed to do this part myself - or get write something so Heroku did it for me.\n\nI built a Rails app that polled my chosen feeds and grouped stories if they were deemed to be the same. This solved the duplicate story problem that grew out of following a greater number of sources. Each day the application would send me an email of the the top stories, based on degree of reporting overlap. I used this for around 2 months.\n\nA combination of _IFTTT‚Äôs_ new twitter search trigger and wanting to follow _newsyc20_ on twitter made me leave _‚ÄòArticle Engine‚Äô_ behind (yep that‚Äôs what I called it‚Ä¶). Using the _IFTTT_ trigger I setup daily emails all the matching tweets. Luckily places like _Macrumors_ \u0026amp; _Engadget_ all tweet new articles. This was the longest lasting solution - It took about a few good months to show any signs of cracks and almost a year to be retired. In the end it had to go when it started missing tweets and sometimes emails summaries too.\n\nIn an effort to cut back on news consumption (to focus on some exams) I started **only** following _Hacker News_. I think _Hacker News_ is great and earlier this year I created a tool to make it work even better - for me.\n\nI wanted the content of _Hacker News_ in a **feed** and I wanted my progress along this feed to sync across devices. Enter _serializer_!\n\n_serializer_ became a hosted side project of mine with its own¬†domain after reading [this post](http://www.slashie.org/articles/shipping-side-projects/) about ‚Äòshipping‚Äô side projects. That inspired to change _serializer_ into something that might be useful to others.\n\n_hckrnews.com_ ticks the linear box but doesn‚Äôt have any syncing / saving of state.\n\nSo instead I built¬†_serializer¬†_into a tool with these features:\n\n* Read-to-here marker\n* Sessions for cross-device syncing without logins\n* Customizable source feeds for users who aren‚Äôt me (I currently track /all)\n* Reading time estimates (based on 300wpm and a simple page content extractor)\n* Tweet counts for recent items\n* Basic duplicate removal\n\nMy session also has a save to _Trello_ button who kindly host my 'multilevel reading list‚Äô. I plan on¬†‚Äòreleasing‚Äô this at a later date.\n\nI have _lobste.rs, slashdot, betalist, macrumors, qudos, designer news _\u0026amp;_ arstechnica_ as well as _HN_, _r/programming _\u0026amp; _producthunt_ as selectable sources on /custom.\n\nserializer feels like it‚Äôs here to stay as my source of news - perhaps largely due to the amount of work it‚Äôs been!\n\nIf you‚Äôre interested [take a look](https://serializer.io) and let me know what you think.","date":"Apr 14 2015","id":100,"title":"Enter: serializer.io","type":"blog post","url":"/posts/2015-04-14-enter-serializer-io/"},{"body":"\nI started lurking on Hacker News and Product Hunt at the beginning of the year. The motivation for building [serializer](https://serializer.io) came from an approaching assignment deadline, rather than cutting out news I thought I'd make following it more manageable. For me this meant a sequential feed - I didn't like having to scan the entire ranked list for new items.\n\n**March 6th:** The first working version took around 30 minutes to build and was running on Heroku soon after. This was prior to the new [dyno pricing](https://www.heroku.com/beta-pricing). I used kaffine (discontinued) to keep the site alive.\n\nI decided on a name pretty quick and bought serializer.io the next day. Students got a discount on *.io* domains at Namecheap (at the time).\n\n![Version 1](/posts/2015-05-22-building-and-shipping-my-side-project/v1.jpg)\n*Version 1*\n\n**March 11th:** I read this [blog post](https://news.ycombinator.com/item?id=9184448) (about 'shipping' side projects) it was probably shared on Hacker News. (edit: it was [shared on Hacker News](https://news.ycombinator.com/item?id=9184448)).\n\nI set to adding some extra features. Read-to-Here marker, 'loginless' syncing, pick and choose custom feeds and tweet counts came about pretty quickly.\n\nAt this point I was still the only user of serializer.\n\nI worked quite hard to get the page weight down. The only javascript on the site is Google Analytics and a timer to refresh the page in the background. The assets are all compressed and images served via [Cloudinary](https://github.com/cloudinary/cloudinary_gem). The feed pages should be around 150kb.\n\n**March 19th:** I posted comments on HN on a few [relevant](https://news.ycombinator.com/item?id=9206427) [posts](https://news.ycombinator.com/item?id=9282219) and acquired my first regular user (Fort Colins, Samsung Galaxy Note 3 if you're out there!).\n\n**April 2nd:** I added the word count feature (and a save to Trello Reading List feature that's live but isn't really ready for the world yet).\n\n**April 19th:** Switched to a $5 Digital Ocean droplet using [dokku-alt](https://github.com/dokku-alt/dokku-alt) and made the [source public](https://github.com/charlieegan3/serializer). This was in pursuit of better performance as well as in response to Heroku's pricing changes. serializer is still going strong on the same $5 droplet.\n\nI took a break for a while after that, only making very minor changes over the exam period when I needed a break from the books. I started using an ['over 10 url'](https://news.ycombinator.com/over?points=10) for Hacker News and made some other adjustments to marginally reduce the number of collected items. Pressure of exams and all that.\n\n**May 15th:** I added a link to [points](https://news.ycombinator.com/item?id=9462755), a cool app I'd seen on Product Hunt. I'd found it was pretty good at summarising articles wanted make it a feature. I reached out to points and heard back from [Hugh Jones](https://twitter.com/hjonesr), he liked serializer and the points connection and offered to setup a post on [Product Hunt](http://www.producthunt.com/posts/serializer-io).\n\n**May 18th:** serializer did better than I ever expected and finished the day in the top 10 with around 100 votes at the time. Since I hadn't done any promotion before I saw a vast increase in traffic, I was impressed that the droplet held up.\n\n![trend](/posts/2015-05-22-building-and-shipping-my-side-project/trend.jpg)\n*This weeks trend, can you spot the Product Hunt boost?*\n\nThis in turn led to some really great feedback from Product Hunters on Twitter and via the feedback form. In response to this I've made the following changes over the last few days:\n\n* Display unread count in the tab title.\n* Added an apple-touch-icon.\n* Reduce chances of users being assigned the same random session identifier (potential error)\n* Revise the feedback page, trying to move over to GitHub Issues and a [TypeForm](https://charlie43.typeform.com/to/tZWtCn) for feedback.\n* Add some pointers to the [GitHub repo](https://github.com/charlieegan3/serializer).\n* Added clarification to the way the session links work. You just need to visit the session path on each browser and then use the site as normal.\n\nLooking forward, I think serializer is fine where it is. I might add some more sources and tidy things here and there, but nothing major. **I'm never going to try and make money from it**.\n\nIt solves a problem for me (and apparently some others too). At the end of the day that's all it needs to do.","date":"May 22 2015","id":101,"title":"Building and 'shipping' my side project","type":"blog post","url":"/posts/2015-05-22-building-and-shipping-my-side-project/"},{"body":"\nWhile in London I've been lucky enough to attend 20 or so codebar events - over two summer internships. codebar is a weekly mentoring initiative with the goal of improving diversity in technology - for those who didn't know. They've got a website that you can check out [here](http://codebar.io/).\n\nLast year I tutored for HTML, CSS and Ruby - this year I've been tutoring with students working on their own Rails projects. During my time as a coach I've learned a lot about what I don't know, ways to phrase some common sticking points, and more - here are some suggestions for getting the most out of being a codebar coach.\n\n###Getting In\nThe first step to being a coach is to qualify as an attendee - host offices have limited space and there's only so much pizza to go round! Sign up to receive invitation emails for the meet up nearest you. Invitations are normally **sent out mid-afternoon on a Friday** and it's a first come first served process.\n\nNormally the event is full in a few hours for coaches - I've heard it can be an even shorter period for students. There is a waiting list but the only time I moved up the list enough to attend on a tube strike day!\n\n###What to Coach\nStudents attending say what they're working on when signing up for the event, some work on [codebar tutorials](http://tutorials.codebar.io/) but more and more people are working on their own projects. When all the pizza's gone students names and topics are called out and coaches volunteer to be paired up.\n\nI'd recommend trying out tutoring on both the tutorials and projects.\n\nTutorials are good if you want to review the content beforehand or want to hone your explanations of a certain set of concepts. However, I found doing the same set of tutorials each week to be somewhat repetitive.\n\nI think tutoring projects is much tougher, coming up against a wider range of concepts and problems further from the core issues is the best lesson around in what you don't know. We've all read that working outside your comfort zone is the only way to learn. If this aligns with your thinking then give tutoring projects a go. It's pretty tiring work but you're set to learn about as much as your two students!\n\n###Tips\n* Bring your **laptop to type out syntax examples** for one student to review while you talk to the other. It's also easier much to write code than explain it verbally. That said, exercise restraint and be deliberately vague where it feels appropriate to make sure you're not just spoon feeding solutions.\n\n* Encourage students to **listen in** if the work on the other student's project is relevant - you might be able to save an explanation or two this way.\n\n* Don't be a afraid to **do things the \"bad way\"** but always comment and give a suggestion as to how you might improve the structure given more time. Good students will always at least take notes on this - the best ones will complete the refactor task alone during the week.\n\n* Using **Google is ok**, as is making the suggestion that students read a blog post for an explanation instead of listening to you. Give them a pointer as to where to look, it's good practice for them apart from anything else but it also gives you a change to switch over to the other student. Teach 'Googling skills' where appropriate.\n\n* **Start with a clean git diff**. Don't change anything before finding out what half implemented features are all in progress.\n\n* Similarly, **only work on one feature at a time**. Often students dig really deep before getting the first step even half done. For example, they know that their users need to upload files and start there without implementing users or any of the related tasks. Another common one is creating **all** the models for the system at once, all with scaffolding (ofc), horrible mess. Know your database reset rake tasks and `rails destroy scaffold ...`.\n\n* **Only fuss over the most basic syntax style**. so often, given a syntax error, getting the student to correct their indentation (which can be *all over the shop*) enables them to spot that missing `end` themselves. Correcting much more only wastes time - sessions really don't last long.\n\n* **Pronounce *route* as *rowte***, it just makes 'root route' (a particularly common phrase) more easily understood.\n\n* We're not meant to suggest editors to students. But **GitGutter** is a plugin I always suggest if the student is working in Sublime (it's in Atom by default but I've never had a student using that). This along with teaching weariness of `git commit -am \"All the things\"` (`git add -p` to the rescue) goes a long way to making sure the project is in a better state when you come up against it the following week.\n\n###Summing Up\n\nI also tutored Rails for a year at university, I'd say this mind dump is based on around 80-90 hours of tutoring Rails to absolute beginners. I can't recommend it enough - you'll get just as much out of it as your students - just don't forget to bring a loud voice and as much energy as you can find! And, leave some room for pizza!","date":"Aug 25 2015","id":102,"title":"Tutoring Codebar","type":"blog post","url":"/posts/2015-08-25-codebar/"},{"body":"\nJust over a week ago I completed my second summer internship at Unboxed (here‚Äôs\nmy blog post from last year‚Äôs internship). I had the opportunity to work on an\nassortment of projects allowing me to experience a wide range of new tools and\ntechnologies. He‚Äôs a list of some key factors, for me that made this summer such\na success:\n","date":"Oct 2 2015","id":103,"title":"Summer at Unboxed - Round Two","type":"external blog post","url":"https://unboxed.co/blog/summer-at-unboxed-round-two/"},{"body":"\nI'm in the process of writing a security report on Heroku. I was interested to know the number of times that attacks on the service caused downtime. I wrote a script to collect all of the [815 incidents](https://status.heroku.com/past) and then checked each report against the following expression: `/hack|attack|ddos|malicious|virus|vunerability|breach/`.\n\nThis returned the following incidents:\n[156](http://status.heroku.com/incident/156),\n[157](http://status.heroku.com/incident/157),\n[245](http://status.heroku.com/incident/245),\n[308](http://status.heroku.com/incident/308),\n[489](http://status.heroku.com/incident/489),\n[539](http://status.heroku.com/incident/539),\n[665](http://status.heroku.com/incident/665),\n[709](http://status.heroku.com/incident/709),\n[738](http://status.heroku.com/incident/738).\n\nDiscarding false positives gave this list:\n\n* [156](http://status.heroku.com/incident/156), DDoS, 27 hours, May 2011\n* [157](http://status.heroku.com/incident/157), DDoS, 1 hour, May 2011\n* [245](http://status.heroku.com/incident/245), DDoS, 4 hours, Dec 2011\n\nIn summary, all attacks that resulted in a service outage were caused by DDoS attacks, and all of these were in 2011.\n\nAll collected incidents can be [downloaded here](/posts/2015-10-15-heroku-outage-analysis/incidents.zip).","date":"Oct 15 2015","id":104,"title":"Heroku Outage Analysis","type":"blog post","url":"/posts/2015-10-15-heroku-outage-analysis/"},{"body":"I wrote a risk assessment of the Heroku platform in Latex this term. It was my first large report using Latex, I ran into the following error and it look me a long time to figure out.\n\n```latex\nRunaway argument?\nAuthor(Year{\\natexlab {{}})]{natbib_key} Author. \\newblock Author u\\ETC.\n! Paragraph ended before \\@lbibitem was complete.\n\u003cto be read again\u003e\n          \\par\n          l.257\n```\n\nWhat this means is that the you cited more than 26 time for the same Author in the same year (*Heroku, 2015*). The problem was this: `\\bibliographystyle{plainnat}`. This style, quite reasonably, sets the letter limit. Too bad the error message is just so awful.\n\nThe key step was opening up `.bbl` file and see to see `\\natexlab {{}})natexlab` where the `{{}}` were missing a letter - vs the earlier citations.\n\n##TL;DR\n\n```git\n-  \\bibliographystyle{plainnat}\n+  \\bibliographystyle{abbrv}\n```\n\n`abbrv` is likely the correct format anyway for scientific publications and is likely already in your class file (but I didn't have one of these).","date":"Oct 17 2015","id":105,"title":"Running out of letters - A natbib journey","type":"blog post","url":"/posts/2015-10-17-running-out-of-letters-a-natbib-journey/"},{"body":"\nHere are the steps to deploy a \"Hello World\" Go app on Heroku as a binary - rather than the method outlined in their [tutorial](https://www.heroku.com/go#see-it-in-action) which uses GoDeps.\n\nThese instructions are based somewhat on those for the [heroku-binary-buildpack](https://github.com/ph3nx/heroku-binary-buildpack).\n\n1. Create a Heroku app for the project:\n\n        create APP_NAME --buildpack https://github.com/ph3nx/heroku-binary-buildpack.git\n\n2. Setup the working directory by cloning from the Heroku remote:\n\n        heroku git:clone --app APP_NAME\n\n3. Create a simple Go app, `server.go`:\n\n        package main\n\n        import (\n            \"fmt\"\n            \"net/http\"\n            \"os\"\n        )\n\n        func handler(w http.ResponseWriter, r *http.Request) {\n            fmt.Fprintf(w, \"Hello World. Path: %v\", r.URL.Path[1:])\n        }\n\n        func main() {\n            fmt.Printf(\"Port: %v\\n\", os.Args[1])\n            http.HandleFunc(\"/\", handler)\n            http.ListenAndServe(\":\"+os.Args[1], nil)\n        }\n\n4. Compile the app for Linux 64bit:\n\n        CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build\n\n5. Create a `bin` directory and move the Linux binary into it - called `server` or something similar.\n\n        mkdir bin\n        mv APP_NAME bin/server\n\n6. Create a simple `Procfile` in the project root to run the app under a web dyno.\n\n        web: program $PORT\n\n7. Add the app's `bin` directory to the PATH on Heroku:\n\n        heroku config:set PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app/bin\n\n8. Commit the lot and push it to Heroku:\n\n        git add --all\n        git commit -m \"First Commit\"\n        git push heroku master\n\n9. Scale the app on Heroku:\n\n        heroku ps:scale web=1\n\n10. Open the app that's now running:\n\n        heroku open\n\n- - -\n\n## Todo\n* Devise a way to keep the binary out of version control, likely using a temporary release branch to push to the Heroku master.\n","date":"Dec 26 2015","id":106,"title":"Deploying Go as a binary on Heroku","type":"blog post","url":"/posts/2015-12-26-deploying-go-as-a-binary-on-heroku/"},{"body":"\nAt the end of last term I wrote a paper titled *The effect of query length and\nsearch engine identity on computerised spelling correction of corrupted\nqueries*. I was interested to test the correction performance of a number of\nsearch engines - the third last and final graph are likely the most interesting\nplots.\n\nThe paper's\n[here](/posts/2016-02-09-query-corruption-paper/query_corruption_egan.pdf) if\nyou want to take a look.\n","date":"Feb 9 2016","id":107,"title":"Query corruption paper","type":"blog post","url":"/posts/2016-02-09-query-corruption-paper/"},{"body":"\nAt the end of 2015 I wrote a risk analysis of the Heroku PaaS. I learned a great deal about stack at Heroku and like to think the paper sums the workings up well.\n\nThe paper's [here](/posts/2016-02-09-heroku-security-paper/heroku_egan.pdf) if you're interested.\n","date":"Feb 9 2016","id":108,"title":"Heroku Security Paper","type":"blog post","url":"/posts/2016-02-09-heroku-security-paper/"},{"body":"\nIt's been almost a year since I started work on [serializer.io](https://serializer.io), my newsreader side project. Before putting serializer together I'd had trouble finding a solution for reading news across multiple devices in a way that made sense. So far it's worked well, and apparently for some others too, for $5 in hosting a month serializer serves a linear newsfeed to around 75 visitor a day.\n\nI've had to maintain serializer over the year. Product Hunt, a key source of items, has changed three times in a way that needed me to adjust the scraper. Other than that and the occasional stylistic change serializer has run without intervention and collected over 75,000 links in the process. I've even received some donations from by recently added PayPal Donate button.\n\nAttached are a number of graphs I put together from the first year's data.\n\n![items_day](/posts/2016-02-28-serializer-turns-1/items_day.jpg)\n*Items per day, shows the weekly item trend quite nicely*\n![items_source](/posts/2016-02-28-serializer-turns-1/items_source.jpg)\n*Item % by source*\n![sessions_read](/posts/2016-02-28-serializer-turns-1/sessions_read.jpg)\n*Of those that use the mark as read feature, how many times have they done so.*\n![topics](/posts/2016-02-28-serializer-turns-1/topics.jpg)\n*Trends for topics, no surprises really*\n![comp](/posts/2016-02-28-serializer-turns-1/comp.jpg)\n*Trends for mentioned companies, I'd have thought Twitter would have been higher.*","date":"Feb 28 2016","id":109,"title":"serializer turns 1","type":"blog post","url":"/posts/2016-02-28-serializer-turns-1/"},{"body":"\nAs part of my honours project I need to query dependency graphs. A dependency graph is a directed graph where edges represent grammatical relations and nodes are tokens. I use CoreNLP and it gives a nice visualisation:\n\n![dep_graph](/posts/2016-03-03-querying-dependency-graphs-with-cypher-and-neo4j/graph.jpg)\n\nSo I'm using CoreNLP, why not use tregex to query the dependency parse? Short answer I couldn't get it working for my questions. Longer answer, tregex and semgrex documentation wasn't great, I started playing around with Neo4j in a container and found it could do what I wanted. It also offered a means to persist graphs for many texts that I could query collectively at a later date.\n\nMy queries needed to extract a verb and it's arguments. Arguments need to be labelled correctly, this is dependent on the relation to the verb (or another one of it's arguments). For example, if we have the query `nounPhrase verb prep nounPhrase` then the `prep` is not linked directly to the verb but rather the second noun phrase, often the sentence object. Cypher implements an accessible syntax for expressing such patterns, this the query I used for the `np v prep np` pattern above:\n\n```\nMATCH (verb:Node)\nMATCH (verb)-[rel_nsubj:REL]-\u003e(nsubj:Node)\nMATCH (verb)-[rel_dobj:REL]-\u003e(dobj:Node)\nMATCH (dobj)-[rel_prep:REL]-\u003e(prep:Node)\nWHERE verb.part_of_speech =~ 'VB.?'\nAND rel_nsubj.label = \"nsubj\"\nAND rel_dobj.label =~ 'nmod.*'\nAND rel_prep.label = \"case\"\nRETURN nsubj, verb, prep, dobj;\n```\n\n(My real query is a little different to limit the verb to a set of allowed verbs)\n\nThis, Cypher syntax, is the nicest way I've seen so far to express patterns of this form. It also works well for more intricate queries such as this one:\n\n```\nMATCH (verb:Node)\nMATCH (verb)-[rel_nsubj:REL]-\u003e(nsubj:Node)\nMATCH (verb)-[rel_nmod:REL]-\u003e(nmod:Node)\nMATCH (nmod)-[rel_prep:REL]-\u003e(prep:Node)\nMATCH (verb2:Node)-[rel_advcl:REL]-(verb)\nMATCH (verb2)-[rel_prep2:REL]-(prep2:Node)\n\nWHERE verb.part_of_speech =~ 'VB.?'\nAND rel_nsubj.label = \"nsubj\"\nAND rel_nmod.label =~ 'nmod.*'\nAND rel_prep.label = \"case\"\nAND rel_advcl.label =~ 'advcl|ccomp'\nAND verb2.part_of_speech =~ 'VB.?'\nAND rel_prep2.label =~ 'mark|case'\nRETURN nsubj, verb, prep, nmod, prep2, verb2;\n```\n\nThis allows sentences like this one to be matched:\n![dep_graph2](/posts/2016-03-03-querying-dependency-graphs-with-cypher-and-neo4j/graph2.jpg)\n\nAll this is in the interest of information extraction, getting these 'points' extracted correctly is a key part of my project. I'm only just starting to move onto the next step which is using the extracted information for summarization tasks. I'm specifically interested in investigating the relationships between things that people say, for example, those who talk about X are also likely to talk about Y, but not Z. Below is the kind of graph I'm working with at the moment (abortion is one of my sample debate corpora), more fine-grained summaries including information about who talks about what comes next.\n\n![summary_graph](/posts/2016-03-03-querying-dependency-graphs-with-cypher-and-neo4j/summary.jpg)\n(I think this last one's quite interesting)","date":"Mar 3 2016","id":110,"title":"Querying dependency graphs with Cypher and Neo4j","type":"blog post","url":"/posts/2016-03-03-querying-dependency-graphs-with-cypher-and-neo4j/"},{"body":"\n![statement](/posts/2016-06-24-getting-a-full-refund-for-a-faulty-macbook-under-uk-consumer-law/statement.jpg)\n\nBelow I describe the process of making a UK consumer law claim for a faulty Apple laptop. This write-up represents almost 3 months of intermittent work, 10's of hours of phone calls and various special trips to Apple outlets.\n\n_TLDR;_ I was 'successful' in claiming a full refund for a 2012 Retina Macbook Pro totalling ¬£2288.20. I have now purchased a new 2016 Macbook m7. The process takes a long time. Also, I believe I was mis sold Applecare, it only appears to entitle you to faster service in the first three years, beyond your basic rights as a consumer in the UK.\n\n# Summary / How to\nThis is what I've learned from my experience. You will need:\n\n1. A machine that's had two major faults within a year (and had one repaired I guess?).\n2. A machine purchased less than 5 years ago (6 years in England).\n3. As much patience as you can muster. Use headphones and keep working while you wait, the Apple call center music quite good.\n\nI think the shortest path through the process is as follows:\n\n1. After the second fault take the machine to a third-party Apple installation (I went to Stormfront and that worked for me). Collect evidence if the fault is intermittent.\n2. Present the laptop as if ready for return. Bring the box and accessories if you have them. State that you need to have the fault verified and that **the machine is not to be repaired**. You will need an Apple repair case ID and a paper EU claim form before you're done with them. The machine may need to be sent away.\n3. Contact Applecare. Request to speak to Customer Relations, stating you have a case ID and a EU claim form. They should then arrange a collection and process the refund - this call will likely take some time and multiple sessions.\n\n_Bonus Tip:_ Be impossibly polite (yet firm) all the time - just let it play out.\n\n_Further Reading:_ [Apple Products and Consumer Laws in the United Kingdom](http://www.apple.com/uk/legal/statutory-warranty/)\n\n# My Story\nThe process outlined above took some time to learn, read on for the full story of my call center adventure.\n\n## June 2012\nWorking at weekend job while at school, where I worked for a period at ¬£4.15 per hour, I amassed sufficient funds to purchase the 'computer of my dreams'. I have been an Apple Fan since 2004 (and have the Macworld issues to prove it), this was the first time I was going to be 'up-to-date' and like 'everyone else' (on MacRumors).\n\nI ordered my laptop on the evening of the WWDC keynote - immediately after the event in UK time.\n\n## February 2013 - March 2015\nI had my laptop repaired a number of times.\n\n- (Feb 2013) Screen replacement for [image persistence issue](https://support.apple.com/en-gb/HT202580), for what it's worth [I had an LG screen](http://forums.macrumors.com/threads/the-ultimate-rmbp-image-retention-test.1422669/).\n- (Mar 2015) Logic board replacement\n- (Mar 2015) Top casing replacement\n\n## March 2016\nThis year I had a final repair completed on my laptop for a full logic board replacement. This repair was covered under 'Customer Satisfaction'. My Applecare had expired and this came as a relief.\n\nThe laptop, which previously didn't boot, now worked again. However, the laptop ran hotter than before and had regular 'visual anomalies' that I could reproduce.\n\nI visited the Apple store in Aberdeen, with my poorly functioning laptop which was taken in again. I also mentioned that I didn't want the laptop to be repaired again and would like a call to discuss my options. I wasn't clear enough in stating this preference. I was told on the phone that the thermal compound had been reapplied and that the machine was running hot but the issue was now resolved. This counts as a repair in my book but hey.\n\nI went into the store to discuss the issue. My machine was brought out to me and I was told the automated graphics switching had been disabled. For the record, I hadn't changed this setting. Apparently this setting had led to the machine overheating - oops my bad? This didn't line up with what I was told on the phone and I still don't know exactly what happened to the laptop this visit.\n\nI wasn't told about the graphics switching until I asked to speak to the manager. There wasn't a manager around and I spoke to a senior member of the repair team. This was the only negative interaction I have ever had with an Apple employee. The man, who reminded me of Tommy Davis, was impossible to reason with. He would not acknowledge that the machine had been repaired and that insisted that the laptop was now functioning correctly. I was told I had no options until another fault developed.\n\nWe had a discussion about the laptop's temperature. I had been monitoring the temperatures with iStat - he told me that these were inaccurate. My follow up was: \"How does an Apple customer that is concerned with their laptop's performance monitor its temperature?\". He attempted to show me the non-existent statistic in Activity Monitor. I knew fine well it wasn't to be found but let him look all the same. He went to the workshop and brought out a USB temperature prong. I didn't want to bother - I should have called his bluff. Returning to my original question, the response was: \"You don't need to worry. The laptop is designed to maintain a safe operating temperature.\".\n\nI asked to have the paperwork for all the repairs carried out on the laptop, thanked the unhelpful staff member and left the store.\n\n## April 2016 - June 2016\n\nPut off by my Apple store visit, I continued [discussing the case online](http://forums.macrumors.com/threads/after-a-series-of-repairs-id-be-interested-to-know-my-options.1964855/).\n\nI continued using the laptop and recording subsequent 'graphical anomalies' on video. Here's an [example](https://www.dropbox.com/s/60f42jdq8cgs9h1/flashing_screen.mov?dl=0), and [another](https://www.dropbox.com/s/o1ompbyqgu308gu/graphic_flickering_2.mov?dl=0).\n\nI was reluctant to return to the same Apple store after my last experience. I made a series of anonymous calls to Applecare and other Apple stores asking for the best course of action. This was useful; and, coupled with the forum suggestions; gave me the information I needed to return to the store. I gathered a printed copy of [Apple's Consumer Law page](https://www.apple.com/uk/legal/statutory-warranty/) and highlighted the relevant sentences.\n\nI returned to the same store to get the faults I had recorded verified. I spoke to a friendly mac specialist at the bar and was offered a replacement machine after showing the videos and reproducing the issue. I said \"I have lost confidence in the machine and am now seeking a refund under consumer law.\", this was the phrase I was advised to use by the Apple staff I'd spoken to over the phone. From what I've learned, there is no 'magic phrase' to use here.\n\nI was informed that since the machine had been purchased before 2013 (when Apple policy changed regarding the issue - newer purchases could have been handled by the store; apparently...) I would need to contact Applecare over the phone. Thanking the representative, I left the store. I actually thought this was almost over at this point.\n\nI called Applecare right away, only to learn after a very long conversation with multiple representatives that I would need to have the fault verified by a third-party Apple Authorized Service Point. This is part of the requirements for making a claim and I had half expected this - however, it was a pain after Apple had confirmed the fault that day in store. I had to wait a few weeks before moving back to my parents house. I needed to use the laptop to complete my final university term and there wasn't an Authorized Service Point in Aberdeen.\n\nI was told that I needed to present the laptop; inform them that I am making an EU claim; and await a case number when the fault had been verified. The third party was to complete the process for me.\n\nI took the machine in. It's clearly not a common process for staff and it took a little while to get all the paperwork sorted. Overall I had a very good experience with the Stormfront store. They were almost as lost as I was but **were able to verify the fault** and provide a Case ID. I contacted Apple with this only to learn that a Case ID is not enough. They were able to view the Stormfront work but also needed the EU claim form. I collected the returned laptop and incomplete form - the form is to be completed by the original retailer, not the verifying third party.\n\nWhile originally wary of the empty claim form, Apple accepted this evidence and I was able to move forward in the process. This was the first time I was able to talk directly with Customer Relations - this felt like a huge milestone. In the end they required: my case ID, all the Stormfront paperwork (invoices etc.) and the incomplete claim form.\n\nThis took a few days to get approved by a senior member of the Customer Relations team. The representative called back and took my account details (you will need to have your international transfer information ready as they pay from Ireland). This request was still to be okayed by a member of the finance team. A few days later this was approved and I was told to expect the refund within 15 days. I had not yet returned the laptop and was told to wait until the refund had been processed and the money was in my account before returning the machine to a service point.\n\nI got another call saying it'd be less than 7 days. A few days later I received a refund for ~¬£1740, I was expecting ¬£2288.20. I was abroad at the time and was going to wait until I returned before following up on the issue. However, Apple called me first, they hadn't noticed the error and requested that I return the laptop before receiving the refund (which I already had - mostly). I arranged a collection date for the day after my return.\n\nUpon my return I made an inquiry regarding the ¬£500 discrepancy. Interestingly I had been refunded in Euros. Another refund was processed for the difference, this one was to \"take up to 2 months\" and was only processed after the laptop had been returned.\n\nToday I received that final refund - after being without a laptop for almost two weeks (luckily I've been able to use a 2015 13\" Macbook Air, only costs around ¬£700 refurbished and other than the screen is a nicer laptop, IMHO). My replacement laptop is scheduled to arrive later today.\n\nConcluding; I still believe Apple laptops are the best around (though I was tempted by a Linux Dell XPS 13), I never needed dedicated GPU anyway, Apple customer support is great but needs to improve on this process to offer consistently good service.\n","date":"Jun 24 2016","id":111,"title":"Getting a full refund for a faulty Macbook under UK consumer law","type":"blog post","url":"/posts/2016-06-24-getting-a-full-refund-for-a-faulty-macbook-under-uk-consumer-law/"},{"body":"\nI had [a little trouble with my 2012 rMBP](/posts/2016-06-24-getting-a-full-refund-for-a-faulty-macbook-under-uk-consumer-law.html) and chose an m7 Retina Macbook as a replacement - after reading positive comments about the performance. I was looking for a different type of machine; namely something more portable; in line with travel plans of the upcoming year. I wanted to find out if the Macbook could do the job.\n\n## My Setup\nI use docker for development; via dlite. I use iTerm and use vim as my main editor. I do not install any additional runtimes or development tools beyond git and my editor to the Macbook itself. I have not installed Xcode (but have the CLI tools installed). I use Safari as my browser, though I have Chrome installed for the occasion when I'm doing something with a frontend focus.\n\nYou can get a better feel for my setup from my Brewfile (Edit: removed, see [here](/posts/2017-02-09-phase-out-laptop-setup-script.html))\n\n## Docker\nDocker works really great and I've yet to notice a task that runs noticeably faster on my old Macbook Pro. This system continues to be a nice way to isolate projects and is a system that the Macbook seems to be very capable of hosting.\n\n## Keyboard\nAt first I found the keyboard odd - I had tried it in the store but it's hard to get a real feel when typing all at the wrong angle. It takes 3-4 days of heavy use to fully 'get it' - or at least it took me that long. I'm not going to lie; it is different and I'm sure not for everyone. I personally have grown to really like the feel.\n\nIf I had a complaint to make it'd be that it's a little louder.\n\n## Charger Bonus\nI miss the MagSafe adapters of old, that said, perhaps the move towards something of a charging standard is a good thing. There is a bonus, with the USB-A/C adapter I can charge all my devices from the same wall adapter. Swapping the 65w and iPhone adapters for a smaller USB-C charger seems like a good deal.\n\n## Screen\nI find the screen plenty large enough to be productive in my editor and terminal. I find the Safari developer tools a little fiddly.\n\n## Battery and Usage\nI get around 6/7hrs out of the battery. I don't know what the average or advertised figures are but this seems pretty good to me. Being smaller, I'm been using the laptop more like an iPad in that it's always on or in sleep. I don't bother turning it off like I did with my old laptop. Being less... imposing on trains and in public places; coupled with being always on has made a big difference in convenience.\n\n## Outstanding Issues\nSafari seems to lag when using `cmd+L` to focus the address bar - this originally seemed to be fixed by disabling all the 'smart' features associated with the address bar and moving to an empty new tab page. However, the issue seems to have returned. I fear there is a network request (with a short but noticeable timeout) triggered on a UI blocking thread on this action. There seems to be very little I can do about this and it was present on my last machine too. I could perhaps investigate the request (if any) and block it in hosts.\n\nThere are sometimes minor graphical slowdowns. These have never lasted over a second and are not an issue for me at all. Such issues seemed to be most common when using the Photos application.\n\nWhile the battery lasts very well the time to recharge is quite slow (And it's missing MagSafe...).\n\n## Responses to problems reported by others\n\n* I haven't noticed any throttling as described by commenters on r/Apple and MacRumors. I don't really do any very long running encoding or compilation tasks so I guess this perhaps not all that surprising.\n* I have music playing most of the time playing from the Spotify desktop client. It seems that any issues once present have been fixed and using the application in this way seems to cause no issues - overheating or otherwise.\n\n***\n\nIn conclusion; it seems I got lucky - in that the laptop, for me, is a good fit. It's not perfect, but, so far, it's been all I could have hoped for. It's a real shame the 2015 model under-powered planted this under-powered idea. I am of the opinion that it's common to go more than a little overboard on laptop specs (I know I did with my 2012 BTO rMBP). You don't need a dedicated GPU to be a productive web developer. There are downsides (heat and battery consumption) to higher spec components. Go try a Macbook, at least they're out and available.\n","date":"Jul 11 2016","id":112,"title":"2016 Macbook for software development: First Impressions","type":"blog post","url":"/posts/2016-07-11-2016-macbook-for-software-development-first-impressions/"},{"body":"\nI've recently completed my first useful Rust application. standpoint is a demo of an information extraction approach implemented as part of my honours project. I have implemented a dependency graph query tool as part of this approach; the application's Rust component. This is short post about some things I've enjoyed about Rust so far.\n\n## Cargo\nComing from Ruby, Bundler is a tough act to follow. I've never needed had to push the edges on Bundler or Cargo but Cargo absolutely feels of a comparable quality. As part of my project I implemented a crate for querying a generic graph. Cargo made this process really easy. This is more than connecting and publishing to crates.io, cargo is the whole picture - running tests \u0026 compiling too. Whether one should publish packages which are likely only for personal use is another matter, in this case it seemed to make sense for me.\n\n## Error messages (with codes)\nRust errors seem to be universally described as easy to understand. While I think they're good, when coming from an interpreted language, there's still something of a learning curve. I found the error codes useful for doing a little extra reading when things didn't make sense (which happened quite often at the beginning).\n\n## Modules make sense\nOver the last 4 years I've been thrown in at the deep-end for various languages. Rust's system of modules and code imports from external packages stood out as one of the better ones - all helped along by Cargo. Certainly compared to Go (which I spent a similar amount of time with last Christmas), Rust and Cargo are far simpler than anything I could find at the time (glide seemed to be the best match).\n\n## Enum Types\nRust has been my first real exposure to Enum types (I have only spent a very short while with Haskell). `Result` and `Option` are really satisfying to use and have helped me get better at reporting neat and meaningful errors (as opposed to rescuing a function call in Ruby). I have however found my code growing into a *Pillar of Doom*, while the `try!` macro can help, there are cases where I've found this harder to resolve (when not all functions return a `Result`).\n\n***\n\nI have also come across some areas that are a little rough around the edges. *Rustfmt* isn't at the same level as *Gofmt* and often seems to struggle with longer statements broken over multiple lines. I've also found that the compiler times can grow quite quickly. I fear this is as much my fault for perhaps not including code with in the best way.\n\nIn summary, I'm really liking Rust and I'm keen to keep using it for future side projects.","date":"Jul 11 2016","id":113,"title":"Things I like about Rust","type":"blog post","url":"/posts/2016-07-11-things-i-like-about-rust/"},{"body":"\nWhen starting out development on a Dockerized application, adjusting Dockerfiles\nand rebuilding images is a common task. Bringing together two separate services,\nas well as applying some gem security patches, we found ourselves doing this a\nlot. The repeated bundle installations quickly became very painful.\nMulti-tasking and regularly switching development branches only increased the\nnumber of repeated builds required.\n","date":"Sep 5 2016","id":114,"title":"Development Re-bundling in Dockerland","type":"external blog post","url":"https://unboxed.co/blog/docker-re-bundling/"},{"body":"\n![Container Camp\nScreen](/posts/2016-09-09-til-at-container-camp/container_camp_screen.jpg)\n\nI was lucky enough to get today off and attend [Container\nCamp](https://container.camp/uk/2016/schedule). Still relatively new to\ncontainers I had a lot to learn, here are some of the most important things I\ntook away from the day.\n\n## There are all sorts of different types of container\n\nSo far I've only ever used Docker containers. I knew there were other container\nruntimes around but hadn't taken the time to compare them. Mark Shuttleworth's\npresentation about 'Snaps', a sharing container format for application\ndistribution; Jonathan Boulle's talk on rkt and rktnetes and Dustin Kirkland's\nabout Linux Containers in a high-performance computing environment all\nintroduced (to me) a different take on the container idea.\n\nHowever, Liz Rice's talk about building a container from scratch was the first\none that really filled in some big blanks in my understanding of the container\ndefinition.\n\n## Containers are actually still pretty new\n\nFor some reason I hadn't picked up on just how much still needs doing to fully\ndeliver on the container promise. There are all sorts of scheduling systems, and\nall sorts of things that aren't quite there yet. Ben Firshman's talk about\nserverless web apps in Docker showed both gave a glimpse of some cool\npossibilities despite some of the current limitations in Docker Swarm.\n\nVarious speakers made reference to OCI and the value that comes from having a\nstandard.\n\n## Other people got fed up waiting on container image pulls\n\nGeorge Lestaris's talk about the problems and alternatives to layer based image\ndistribution really got my attention. CernVM-FS \u0026 the IPFS apparently offer\nsomething of a solution to superfluous layer pulls, and this is already in use\nin Mesoshere? Something I clearly need to read up on.\n\n## There are various benefits of containers in production\n\nI hadn't fully taken on board the other benefits of containers in production.\nWhen playing with Kubernetes I'd thought it was fun but perhaps overkill for a\nsimple Rails app \u0026 database. Turns out that using a distributed scheduling\nsolution, Kubernetes or otherwise, can also give you faster deploys, rollbacks,\nmonitoring, cheaper hosting among many others with _relatively_ little extra\nwork.\n\n## Persistent workloads are (quite a bit) more complex\n\nSome Orchestration tools seem to be missing a clear means of running persistent\nworkloads. It also seems to be a new feature in those that do. This seems to\nline up with my experience of getting a SQL database running. I think I need to\nspend some time looking into [ways to do this\nproperly](https://github.com/helm/charts/tree/master/stable/postgresql).\n\n***\n\nI got a great deal out of the day - lots of really interesting things to test\nout and perhaps even contribute to. Really well run event and a great venue.\n","date":"Sep 9 2016","id":115,"title":"TIL at Container Camp","type":"blog post","url":"/posts/2016-09-09-til-at-container-camp/"},{"body":"\nPaula and I attended last Friday‚Äôs NEXT London 2016 - Google Cloud Platform‚Äôs\nUser Conference - the largest Google developer and IT gathering in the UK,\nexploring the latest developments in cloud technology.\n","date":"Oct 28 2016","id":116,"title":"NEXT London 2016 - Google Cloud Platform's User Conference","type":"external blog post","url":"https://unboxed.co/blog/next-london-2016-google-cloud-platform-s-user-conference/"},{"body":"\nFor some time I've been looking to consolidate my 'side project infrastructure'. I have two projects running Digital Ocean droplets ([serializer.io](https://serializer.io) and standpoint); json-charlieegan3 runs on [hyper.sh](https://hyper.sh) making use of an S3 bucket; [charlieegan3.com](https://charlieegan3.com) runs on [Netlify](https://www.netlify.com) (but is about to move to App Engine standard) and I have the usual handful of tasks \u0026 apps running on Heroku.\n\nThese projects mostly run for free but when the credits run out early in the new year they'd cost almost $30 a month. So - with that budget in mind - I've been looking around for somewhere to run and manage all these projects from the same provider. I'm also trying to learn something new in the process.\n\nAt the end of October I attended GCPNext here in London and have since been trying to find the time to play GCP. I managed to do this, finally, two weekends ago weekend. Here's my take.\n\n# Billing\n\nI started out setting up billing alerts; price pretty important to me. I don't expect any of my projects to generate income so I'm doing this on the cheap.\n\nGCP billing information could be more accessible. Projects on your account are associated with a billing account - using the platform as an individual you'll likely only have one of these for all your projects. On the billing accounts view one can set budgets and alerts; but these only seen to update after quite long intervals - I think daily. There's also no way to see a cost breakdown without looking at an invoice (delivered monthly) unless you set up a _Billing Export_.\n\nBilling Exports seem to update more regularly and come in CSV and Big Query flavours. These list itemised expenditure for different platform usage (CPU time, storage use, static IP etc.). The Big Query export is cool and is nice to use but the fact that I need to write code to get a cost breakdown by project or find out the most expensive service for my usage feels frustrating. It would be a cool addition to a more standard billing \u0026 usage view. I also find the idea that the if you'd burnt though your free TB Big Query allowance for the month you'd pay to get your billing breakdown quite amusing!\n\n```sql\n-- Query to get an expenditure breakdown by project\n-- (Information not available in the UI)\n\nSELECT project.id, sum(cost)  FROM [billing-bucket]\nGROUP BY project.id\nLIMIT 1000\n```\n\n# Projects\n\nNext I moved on to projects. I knew I could host a static site using the App Engine Standard Environment. Standard Environment applications can be python, node, go or java and run with a number of restrictions. Using a simple app.yaml to define a number of handlers for my static site I was able get my site up and running. I used wercker to configure a simple CI/CD pipeline.\n\nBeing able to deploy with `gcloud app deploy` from my machine and using the wercker step was nice but Netlify was infinitely easier; and considerably faster to deploy. The app deploy command seems to be pretty slow from my laptop over a fibre connection but quite quick on wercker. Not sure why this was.\n\nI added certs from Let's Encrypt quite easily using the UI. It's also pretty clear how to set the DNS records for the bare and www domains.\n\nIn summary this first project went well. I didn't gain anything over my previous Netlify setup; arguably loosing some speed and ease of use. I did learn more about the tooling though which helped later on.\n\n# Rails \u0026 Cloud Datastore\n\nNext I opted to deploy the [Rails Cloud Datastore sample](https://github.com/GoogleCloudPlatform/getting-started-ruby/tree/master/2-cloud-datastore). Most of my projects are Rails simple apps and I was keen to find out what the options were for these too. This brings me on to the Google App Engine Flexible environment. The flexible environment will let you run applications that can be deployed as a container - but this flexibility comes at a cost. This cost is again, I found, impossible to calculate and only with real usage could I tell how expensive it was. After spending ¬£6 in the first day I opted to switch off the application.\n\nI also had a bit of a hard time with the Cloud Datastore. I'm aware CloudSQL but I remind you that cost is the key factor here (running a CloudSQL instance - per app - would put me over my monthly budget). I found that I was able to get something working but [as you can see the example](https://github.com/GoogleCloudPlatform/getting-started-ruby/blob/master/2-cloud-datastore/app/models/book.rb) there's a fair bit of re-writing ActiveRecord-like functionality and it just felt wrong. Additionally I found the the Cloud Datastore would fail on the first request after a period of inactivity - only to work immediately on the subsequent request. I found the Ruby documentation for the product thin at best.\n\nThese cost issues coupled with the Cloud Datastore problems mean I no longer have any Rails apps running in the Flexible Environment.\n\n# CloudDNS\n\nIn my efforts to double-down on GCP I also tried moving the DNS config of a domain over. I quickly found that I wasn't able to do redirections nor email catch all forwarding so I've move the staging domain back to Namecheap's basic DNS.\n\n# Concluding\n\nThe one big positive of has been Big Query. I really enjoyed querying the Github dataset on there - I had a specific task for this though that I'll write up in another post. I spent almost ¬£15 on Big Query though so I'll need to hold off until next month now...\n\nGenerally though, my opinion is pretty mixed - the console UI is simple; the CLI is great \u0026 the standard environment is also nice to have. The biggest let down is the sorry state of billing/expenditure information.\n","date":"Dec 26 2016","id":117,"title":"A Weekend with The Google Cloud Platform","type":"blog post","url":"/posts/2016-12-26-a-weekend-with-the-google-cloud-platform/"},{"body":"\nSometime last year I listened to a [Changelog podcast episode](https://changelog.com/podcast/209)\nabout the GitHub data that's been made available on [BigQuery](https://cloud.google.com/bigquery/pricing),\nGoogle's tool for querying large datasets. Over Christmas I finally thought up\na query worth running.\n\nI'd been looking for a self-hosted Evernote alternative and was interested in\nwhat people might have already built that was ready to 'self host' on Heroku.\n\nSo after a little fiddling about I settled on the following query:\n\n```sql\nSELECT repo_name FROM [bigquery-public-data:github_repos.files]\nWHERE id IN (\n  SELECT id FROM [bigquery-public-data:github_repos.contents]\n  WHERE content CONTAINS 'https://www.herokucdn.com/deploy/button.png'\n)\n```\n\nThis gets a list of all the repos that have the Heroku button in some file\nsomewhere.\n\nNext I wrote a simple script to get the extra data from the GitHub API\n(description, homepage etc). After sorting by stars (because I couldn't think\nof a better thing to sort by) we get the following:\n\n1. [Huginn](https://github.com/cantino/huginn) - Create agents that monitor and act on your behalf.\n2. [RocketChat](https://github.com/RocketChat/Rocket.Chat) - Slack like online chat.\n3. [Keystone](https://github.com/keystonejs/keystone) - node.js cms and web app framework.\n4. [Wekan](https://github.com/wekan/wekan/wiki) - The open-source Trello-like kanban.\n5. [Paperwork](https://github.com/twostairs/paperwork) - OpenSource note-taking \u0026 archiving.\n\nFull set of Heroku ready apps can be found [here](/posts/2017-01-12-heroku-treasure/heroku-deploy-button-results.csv).\nA few days later I got an invoice ¬£11.76 so I guess it'll be a while before I\nhave such reckless fun like this again...\n","date":"Jan 12 2017","id":118,"title":"Heroku Deploy button treasure hunt with GitHub and BigQuery","type":"blog post","url":"/posts/2017-01-12-heroku-treasure/"},{"body":"\n**TL;DR** - I switched from a complicated script that personalised my development laptop to a setup minimal enough to make it irrelevant.\n\nI've used an Mac as my main computer since 2005, in 2005 I was 11 years old. Recently, a number of factors have pushed me to consider alternatives, the reasons for which are beyond the scope of this post. The consequence is that I've been looking to port my development environment to a Linux workstation.\n\nI have a dotfiles repository on GitHub. This contains a number of config files for programs that I use use such as bash and vim. It also includes bootstrap script for setting up a developer laptop on MacOS and I've used this laptop bootstrap script to save me multiple hours on more than one occasion. However - recently, I've come to see it as something of a burden that's a pain to maintain and makes me less adaptable.\n\nMy bootstrap script installs Homebrew packages, casks and configured the OS just the way I liked it. This won't work on Linux. I've also decided that I no longer want to automate this task but rather alter the way I work to make it irrelevant. To get to this point I've had to make some key changes:\n\n* Using a command line text editor (2015)\n* Using docker and compose\n* Replacing tools that require a native app, e.g. Day One\n* Implementing some self hosted tools\n* Using a web based password manager\n* Hosting my own bookmarks\n* Using a cross platform browser like Firefox (2017)\n\nIf that list doesn't make it clear, I'll state it upfront: **this isn't a guide to setting up your development environment but rather a justification for a change I'm making to mine**. So why ditch the bootstrap script? I don't think I need one. I've spent some free time arriving at the following setup, again - this is largely enabled by the changes implemented in the list above.\n\n1. Install the Manjaro i3 distro. i3's config files and keybindings really resonated with me, this was the first distro I found that bundled i3. Sure it's easy to install i3 over an Ubuntu system but I like pacman \u0026 this new setup is all about making use of sensible defaults already available.\n2. Install Firefox and sync settings (side note, FF extensions don't sync settings well - reduce dependencies on them where possible, also).\n3. Import my GPG and ssh keys.\n4. Clone my config files repo into my home directory.\n5. Install docker.\n\nLooking at that list, would you bother to write a script for that? I've decided I don't need to.","date":"Feb 9 2017","id":119,"title":"Phasing out my laptop setup script","type":"blog post","url":"/posts/2017-02-09-phase-out-laptop-setup-script/"},{"body":"\nOn the Bookmetrix project we used a different process for managing our releases\nin git. We took feature branches from master and opened pull requests with a\n‚Äòrelease branch‚Äô as the base. If a PR wasn‚Äôt approved before it‚Äôs base release\nwas merged then it‚Äôs base was moved to the branch for the next release instead.\n","date":"Mar 20 2017","id":120,"title":"Deploying indigo-falcon to production!","type":"external blog post","url":"https://unboxed.co/blog/deploying-indigo-falcon-to-production/"},{"body":"\nWhile _charlieegan3.com_ has been my 'online home', my personal site has gone through various revisions. Personal websites are subject to an above average level of tinkering and experimentation. I thought it time to pause and write about the interesting elements of the current setup.\n\nI've been using a static site generated using middleman since the [end of 2015](https://github.com/charlieegan3/personal-website/commit/5687489e86540f48d6760d7f1de6060a04b7abc6). What started happily as a GitHub pages site [moved](https://github.com/charlieegan3/personal-website/commit/5a09e57763d8b16b5f6a4efe4630736e8718606c) to Netlify last year to take advantage of their free HTTPS on custom domains. I really enjoyed using Netlify and think it's a great tool. At the end of last year; after attending GCPNext I got into playing with the the (free) App Engine Standard environment - _charlieegan3.com_ was the simplest project of mine that I could use to try it out. Most of my projects are Ruby-based and you can't run Ruby on the free App Engine tier. It was time to move, again. Here's the setup I've arrived at - it's one I'm fairly happy with.\n\n## Wercker\n\nPrior to my App Engine experiments, I also looked into the Google Container Engine for hosting some of my other more involved projects. I found a tool called [wercker](https://www.wercker.com) that I liked the 'interface' for and was keen to keep using on the App Engine for my personal site. The wercker config file; where each pipeline starts from a container image (e.g. one ready to build my site or equipped with the gcloud tooling for deployment) made a lot of sense. So my wercker workflow looks a little like this:\n\n1. **Build** - starting point image: `ruby:2.3.3`\n  * clone the source\n  * bundle install\n  * middleman build -\u003e build directory\n\n2. **Deploy** - starting point image: `google/cloud-sdk`\n  * (only ran if build completed successfully)\n  * (using the build artefact from the last pipeline)\n  * deploy the site to the app engine using the gcloud toolchain.\n\nI've found basing each pipeline step in the workflow on a 'pre-loaded' image like this makes the deployments quite quick - under 2 mins to build and publish the site.\n\n## Automatic broken link checking\n\nI had imported my old posts from Tumblr some time ago but hadn't bothered to check them all for broken links (I found the [broken-link-checker](https://www.npmjs.com/package/broken-link-checker) to be the best thing for this task). I also realised that my `app.yaml` handlers where not serving some of my post attachments. After fixing this issues I thought it'd be nice to automate checking internal links before deploying.\n\nI toyed with the idea of serving the site with one process and running the checker against it in a `Link Test` wercker pipeline. After some looking around I found [HTMLProofer](https://github.com/gjtorikian/html-proofer), a gem for checking validating static HTML. With the help of a little monkey patching to sort out an encoding issue I was able to set this up as an [after_build callback](https://github.com/charlieegan3/personal-website/commit/0e7eac36c74d41baf89f6ac50d02ac930bc5aaea#diff-2620489c404159b5404f13f82e470fbdR109).\n\nNow if I try and deploy a broken link or a page that renders to invalid HTML the site won't deploy.\n\n## Live Status\n\nI have a task that runs every 10 mins on Heroku to update the live sections on the homepage. This is a [separate repo](https://github.com/charlieegan3/json-charlieegan3) and is basically a set of scrapers and API clients for a number of sites and services I use around the web. The task gathers all the latest data from each service and pushes the result to a [json file](https://github.com/charlieegan3/json-charlieegan3) in a storage bucket. There is also a [fallback copy](/status.json) of the status file saved into the build directory when this site is built. This project is pretty hacked together; I'd like to make it more modular but for now it does the job. I think the live sections on the homepage are a nice feature.\n\n## Noscript Friendly\n\nUntil this last weekend the site didn't load correctly without Javascript, shame; shame. The good news is that it now works fine. The live status panels only display in when the data's there and the page re-arranges itself. I've also opted to add turbolinks to the site as it makes switching pages faster and gracefully degrades in the absence of Javascript.\n\n***\n\nI think it's interesting to explore what we can do with a \"static site\" and think simple projects like this are often a place to try out new things that might be harder to justify in more involved applications.\n","date":"Mar 21 2017","id":121,"title":"How this website (currently) works","type":"blog post","url":"/posts/2017-03-21-how-it-works/"},{"body":"\nAfter working closely with a proficient vim user on a summer internship; I\nstarted experimenting with vim during the summer of 2015. I made the usual\nmistakes of copying the other's vimrcs and installing too many plugins - I\nthink everyone has to go though that phase though. Eventually I settled; after\nmuch tinkering; on a configuration I could be productive in and left it largely\nunchanged for a year.\n\nIn the same way it's easy to procrastinate while refining the 'perfect' vim\nconfig, it's also possible to let it stagnate and stop learning. With vim,\nthere's usually something you could improve upon.\n\nRecently, I've been looking to minimize the areas of my development environment\nthat require manual setup. I've also been looking to reduce the difference\nbetween my Linux and macOS machines. While my vim journey started earlier,\nit's been key in making this possible.\n\nI recently revisited my vim config and made some improvements.\n\n# Neovim \u0026 terminal buffers\nI've been using i3 and urxvt on my Linux machine. On macOS I use iTerm.\nThis means the process for opening new editors windows and terminal sessions\ndiffers - something i'm keen to avoid. I considered working at the various\nkeyboard shortcuts to make the two comparable but in remembering that neovim\nadds terminal buffers I thought I'd give that a shot. I'd already got vim\nworking consistently cross platform - using neovim makes terminal session\nusage consistent too.\n\nI was able to port my vim config to nvim very easily. I had an addon that\nwouldn't work and added the following commands.\n\n```vim\nif has('nvim')\n  tnoremap \u003cesc\u003e \u003cC-\\\u003e\u003cC-n\u003e\n\n  augroup terminal\n    autocmd TermOpen * set bufhidden=hide\n    autocmd TermOpen * setlocal nospell\n  augroup END\nendif\n```\n\nI also added a script to link the nvim configs to the old vim ones. I might\njust move them all to nvim at a later date.\n\n```bash\n#!/bin/bash\n\nln -s ~/.vim ~/.config/nvim\nln -s ~/.vimrc ~/.config/nvim/init.vim\n```\n\n# The `\"+` buffer\nI struggled along with a thoroughly inadequate and irregular system for\npasting in and cutting out of vim for too long. I knew about the `\"+`\nregister but didn't make use of it.\n\n```vim\nvnoremap \u003ccr\u003e \"+y\u003ccr\u003e\nvnoremap \u003cBS\u003e \"+p\u003ccr\u003e\n```\n\nThe best this about this is that it makes it possible to copy to and from the\nclipboard on both Linux and macOS (in the same way).\n\n# buftabline\nI lived with a similarly awkward setup for managing open buffers. I have\n`\u003ctab\u003e` mapped to `:bn` in normal mode - as well as `leader w` to write\nand close the buffer. However I didn't have a means of tracking open files; I\njust used to cycle round until I got the one I wanted - or I'd just use\nFZF (ctrl-p) again. `buftabline` simply shows a list of open buffers at the\ntop edge of the window - like almost any other editor... Getting caught up.\n\n```vim\nPlug 'ap/vim-buftabline'\n\nhighlight BufTabLineCurrent ctermbg=black\nhighlight BufTabLineActive ctermbg=white\nhighlight BufTabLineHidden ctermbg=darkgrey\nhighlight BufTabLineFill ctermbg=grey\n```\n\n# base16 colorscheme\nI'm fairly indifferent about color schemes - though I like them to be the\nsame across my different computers. I came across\n[the base16 standard](https://github.com/chriskempson/base16)\nand decided to use that as I was able to find `.Xresources`, `.terminal` and\nvim files for without needing to set my own values. I found a preset called\nLondon Tube (which seemed fitting) and have just committed that into my\ndotfile repo.\n\n# autocmds for formatting\nWhitespace trimming and tabs -\u003e spaces had also been inconsistent in the old\nconfig. I had been using `vim-stripper` for trimming trailing whitespace but\nit turns out a simple regex does the job:\n\n```vim\nautocmd BufWritePre * :%s/\\s\\+$//e\n```\n\nI also opted to use a plugin, `vim-super-retab`, to correct the tabs before\nsave. This is in addition to a number of other settings available by default.\n\n```vim\nautocmd BufWritePre * :Tab2Space\n\nset smarttab smartindent expandtab \" sane tab settings\nset tabstop=8 softtabstop=8 shiftwidth=2 \" indentation quantities\nset backspace=indent,eol,start \" backspace behavior\n```\n\n---\n\nThose are just the vim ones; I've also changed i3 around and dropped some\nthings on macOS in the year long task of adopting a developer environment that's\nhighly portable.","date":"Mar 26 2017","id":122,"title":"Fixing up my vimrc","type":"blog post","url":"/posts/2017-03-26-vim/"},{"body":"\nWe were recently tasked with adding a feature that showed the status of a IoT\ndevice as it changed. It was important that the status page updated quickly; and\ngiven we‚Äôd recently upgraded to Rails 5, this was a prime opportunity to learn\nActionCable. What follows is a post detailing how we overcame a number of\nchallenges in getting the feature deployed.\n","date":"Mar 31 2017","id":123,"title":"Running ActionCable behind Elastic Load Balancers on AWS","type":"external blog post","url":"https://unboxed.co/blog/actioncable-on-aws/"},{"body":"\nI've been working on [char.gy](https://char.gy) for the last few months. I fear\nit may be becoming all consuming as I felt compelled to write up our EV centric\nholiday...\n\nAt either end of a week long stay in South Uist we drove this route in our\nRenault Zoe:\n\n![image](/posts/2017-06-24-west-coast-ev/map.png)\n\n* `A` **Home in Muir of Ord**\n* `B` Inverness Cathedral Car Park\n* `C` Drumnadrochit Tourist Info\n* `D` Fort William Lidl\n* `E` Mallaig Ferry Terminal Car Park\n* `F` Lochboisdale *(no charger)*\n* `G` **Howmore (holiday cottage for a week)**\n* `H` Liniclate School\n* `I` Berneray *(no charger)*\n* `J` Leverburgh Harbour Car Park\n* `K` Tarbert Ferry Office Car Park\n* `L` Stornoway Council Car Park\n* `M` Ullapool Ferry Terminal Car Park\n\nBetween E \u0026 F, I \u0026 J and L \u0026 M there are ferry journeys.\n\nSome notes on charge points:\n\n* Poor signal makes validating RFID cards slow.\n* Multiple charge points actually worked that were labeled as faulty. (I guess\n  people failed to get them to work or they started working again)\n* When you're using the car driving about on holiday you need a 22kw+ charger,\n  anything longer takes too long to be useful.\n* All the chargers seemed to be EVOLT (APT Technologies, Circontrol Software)\n* All the chargers were free for us to use with a\n  [chargeplacescotland.org](http://chargeplacescotland.org/) RFID card. Charge\n  Your Car Stickers were also there for English drivers.\n\nSome pictures:\n\n![image](/posts/2017-06-24-west-coast-ev/inverness.jpg)\n*Finn marking the charge point in in Inverness*\n\n![image](/posts/2017-06-24-west-coast-ev/road_to_fort_william.jpg)\n*Zoe on the road to Fort William*\n\n![image](/posts/2017-06-24-west-coast-ev/linaclate.jpg)\n*Liniclate(48kw) was our favorite charger for the week on Uist*\n\n![image](/posts/2017-06-24-west-coast-ev/linaclate2.jpg)\n*Topping up at Liniclate (5:30am) - start of a long day traveling home*\n\n![image](/posts/2017-06-24-west-coast-ev/leverburgh.jpg)\n*Leverburgh was actually working - great charge point location!*\n\n![image](/posts/2017-06-24-west-coast-ev/tarbert.jpg)\n*Tarbert was also working (though a little rusty) - Zoe had a nice view for the charge again*\n\n![image](/posts/2017-06-24-west-coast-ev/stornoway.jpg)\n*Competition for charge point space in Stornoway*\n\n![image](/posts/2017-06-24-west-coast-ev/ullapool.jpg)\n*Quick boost at Ullapool, the start of the home straight*\n\n![image](/posts/2017-06-24-west-coast-ev/home_straight.jpg)\n*On the home straight at Glascarnoch dam*\n\n![image](/posts/2017-06-24-west-coast-ev/home.jpg)\n*Home is where the charge point starts automatically*\n","date":"Jun 24 2017","id":124,"title":"500 kilometres of electric-car, west-coast exploring","type":"blog post","url":"/posts/2017-06-24-west-coast-ev/"},{"body":"\n![Host a static site](/posts/2017-07-16-host-a-static-website-5-minutes/misleading.jpg)\n\nI was in the market for a project to spend some more time with Terraform when I\nsaw this prompt in the AWS console. Terraform can do all those things \u0026 I have\na static site (this site) that I could use to test it out. As usual, it took a\nlittle longer than expected. This post is a list of mistakes I made and\nproblems encountered.\n\n## Bucket URLs\n\nAs far as I can tell there are up to three URLs for a given bucket:\n\n```\ns3.amazonaws.com/BUCKET_NAME\nBUCKET_NAME.s3.amazonaws.com\nBUCKET_NAME.s3-website-REGION.amazonaws.com\n```\n\nThe difference between the final two forms confused me - only the s3-website\nendpoint will perform static hosting features such as redirects but the `.s3.`\nendpoints (bucket domain name) still return files with the correct content\ntypes. In Terraform you're after the `website_endpoint` rather than the\n`bucket_domain_name`.\n\n## Subdomain Redirects\n\n(`*` Sorry, the 'to-or-not-to redirect www to the apex domain' debate is out of\nscope!)\n\nThere is an S3 website hosting feature to redirect all requests to another\nhostname. The bucket will return a redirect status.\n\nThis seemed ideal - point my `www` subdomain to a bucket in Route53 and have it\nredirect everything to the apex domain.\n\nI had this all wired up with an Alias record pointing to the redirect bucket's\ns3-website endpoint. Sadly this doesn't work. Turns out that since the website\nendpoints don't serve certificates other than those for the S3 `amazonaws.com`\nsubdomains. This means that _HTTPS_ redirects don't work.\n\nInstead - you need to create an separate Cloudfront distribution for the\nredirect bucket. I had an ACM certificate for both domains and had this served\nby the redirect Cloudfront distribution as well as the main site distribution.\nThis isn't really ideal but it does work.\n\n## ACM region\nI was able to request ACM certificates in various regions for my domains\n(charlieegan3.com \u0026 www.charlieegan3.com) via the UI. For some reason;\nTerraform was only able to find certificates in `us-east-1`. I wasn't able to\npoint it at any other region. I requested a new certificate in that region and\nactually moved the project default to that region too for the sake of\nsimplicity.\n\n## All I want is a redirect\n\nThis is the fun bit. Imagine you have the following files in your static site:\n\n```\n/index.html\n/subdir/index.html\n```\n\nNow imagine that you'd like to have the following paths and redirects:\n\n```\n/\n/subdir\n\n/index.html        -\u003e /\n/subdir.html       -\u003e /subdir\n/subdir/index.html -\u003e /subdir\n/subdir/           -\u003e /subdir\n```\n\nThis is actually a bit of a fiddle to set up in S3.\n\nFirst to the get `/` path you need to tell S3 that `/index.html` is your index\ndocument. Easy, this can be done in Terraform bucket config.\n\nGetting the `/subdir/` path is also easy as it works by default with S3 static\nweb hosting enabled. `/subdir` without the trailing slash is harder - to get\nthis you need to create a file called `subdir` with an HTML content type. I use\nthe AWS CLI to do this in my deploy script:\n\n```\naws s3 cp s3://#{S3_BUCKET}/subdir/index.html s3://#{S3_BUCKET}/subdir --content-type text/html\n```\n\nUnlike Unix, in S3 it's possible to have a file and a folder with in the same\n'directory' with the same name.\n\nTo get `/subdir/index.html` to redirect to `/subdir` (our new file) we need to\nset a property (`website-redirect-location`) on the 'file'.\n\n```\naws s3api copy-object --bucket #{S3_BUCKET} --copy-source #{S3_BUCKET}/subdir/index.html --key /subdir/index.html --website-redirect-location /subdir\n```\n\nStill a little way to go yet. Next up: `/subdir.html -\u003e subdir`. This one's\npretty similar, spot the difference in `--key`:\n\n```\naws s3api copy-object --bucket #{S3_BUCKET} --copy-source #{S3_BUCKET}/subdir/index.html --key /subdir.html --website-redirect-location /subdir\n```\n\nThe following redirections remain:\n\n```\n/index.html -\u003e /\n/subdir/    -\u003e /subdir\n```\n\n**Warning:** If you don't like the sound of Javascript 'redirects' stop here.\n\nYou can't set a website-redirect-location on `index.html` since it'll created a\nredirect loop back to itself when it's served on `/`. In my JS I'm checking the\npath and cleaning it up as required.\n\nUsing the same mega hack; I'm stripping trailing '/'s from not root paths.\n\n```js\nif (path === \"/index.html\") {\n  window.location = \"/\"\n} else if (path.length \u003e 1 \u0026\u0026 path.slice(-1) === \"/\") {\n  window.location = path.substring(0, path.length - 1);\n}\n```\n\n_Disclaimer:_ Some of these redirects can be configured using the bucket redirect\nconfig. I opted to use object properties since the bucket list is limited to 50\nrules which is less than I required.\n\n## Terraform Chicken \u0026 Egg\n\nI also ran into an issue when provisioning the site from scratch. The redirect\nCloudfront distribution wouldn't create because the 'origin' was down. The\nredirect bucket had been created but since it redirected to my domain, and the\nother distribution for the apex domain wasn't yet active, it failed.\n\nI just ran parts of the Terraform config in different orders but I wonder if\nI might be able to create a tag on the redirect distribution with the apex\nCloudfront distribution ARN to make sure it works out the order correctly. One\nfor another day.\n\n\u003chr/\u003e\n\nSo that's how to 'Host a static website ~1 weekend'. I'm not sure how to do\nit in 5 mins yet.\n\nI've yet to move this under charlieegan3.com (currently it's on\ncharlieegan.com) while I test it out. Currently\n'production' is running for free on App Engine so I'm interested to compare the\ncosts before switching.  Renewing Lets Encrypt on GAE is a pain so this\nsolution with ACM is likely to win anyway. CDN's are also 'cool'.\n","date":"Jul 16 2017","id":125,"title":"Terraform adventures: Host a static website ~5 minutes","type":"blog post","url":"/posts/2017-07-16-host-a-static-website-5-minutes/"},{"body":"\n`terraform destroy` has been a game changer for my public cloud toy projects.\nBuilding up a reproduceable config that can brought straight back the next time\nyou come to work on the project is invaluable.\n\nQuite coincidentally, after my [last\npost](/posts/2017-07-16-host-a-static-website-5-minutes) about migrating my\npersonal site to AWS I found myself grappling with another X minute getting\nstarted tutorial in Terraform. I've had this idea for a web app that scans\nwebsites for broken links that isn't generally terrible and might even be\nconsidered fast. The idea was that the work would be done on Lambda and the\nresults returned to a largely simple S3 hosted site (now that I know how to do\nthat!).\n\nThis took me a week of evenings and a weekend to get working. The main reason\nthat it took so long was that I didn't really have any idea what API gateway\neven was beforehand. API Gateway also has _a lot_ to configure - which makes it\na little overwhelming to configure - even for the basic case.\n\n![Deploy AWS lambda and API gateway with terraform](/posts/2017-08-10-terraform-lambda-api-gateway/lambda-in-2.png)\n\n## Things I tried\n\nI started out doing my usual trick of copying the Terraform docs. There's an\nexample on there that appears to cover this very topic - [this\none](https://www.terraform.io/docs/providers/aws/r/api_gateway_integration.html#lambda-integration).\nI followed that but found that I my requests weren't getting into the function\nat all. This example also doesn't have logging configured so it's hard to work\nout exactly what's going wrong.\n\nI also looked [at this\npost](https://andydote.co.uk/2017/03/17/terraform-aws-lambda-api-gateway/) but\nfound that it was missing some config for the integration responses.\n\nI found the [hello-lambda repo](https://github.com/TailorDev/hello-lambda)\nuseful but in the end opted for a more minimal API gateway config using a\nPROXY. I opted to do this since it meant fewer resources to match up for\nintegration and method responses.\n\n## The Method\n\nUnless you're very familiar with API Gateway this likely isn't going to work\nfirst time. I learned the hard way: set up Cloudwatch for the function and API\ngateway before doing anything else.\n\nNote: I'll try and explain all the components in here but my final version's\nalso here on\n[GitHub](https://github.com/charlieegan3/borked/tree/b70ddcd141fe4fd2a8d3cf669f044d55ccbb4a7d/terraform)\n\nThis is my Lambda config. It describes a function with an attached policy for\nlogging to Cloudwatch.\n\n```conf\nresource \"aws_lambda_function\" \"lambda\" {\n  filename         = \"../handler.zip\"\n  function_name    = \"${var.project}\"\n  role             = \"${aws_iam_role.lambda.arn}\"\n  handler          = \"handler.Handle\"\n  runtime          = \"python2.7\"\n  timeout          = \"30\"\n  source_code_hash = \"${base64sha256(file(\"../handler.zip\"))}\"\n  memory_size      = \"512\"\n}\n\nresource \"aws_iam_role\" \"lambda\" {\n  name = \"${var.project}-lambda-role\"\n\n  assume_role_policy = \u003c\u003cPOLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"Service\": \"lambda.amazonaws.com\"\n      },\n      \"Effect\": \"Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}\nPOLICY\n}\n\nresource \"aws_iam_role_policy\" \"logging\" {\n  name = \"${var.project}-lambda-logging-policy\"\n  role = \"${aws_iam_role.lambda.id}\"\n\n  policy = \u003c\u003cEOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"cloudwatch:*\",\n        \"logs:*\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nEOF\n}\n```\n\nWhile we're on the topic of logging I think it's also worth explaining how to\nadd it to your API Gateway account. **Note** this isn't attached to your\nGateway but rather your 'Gateway Account' - you don't need Gateway resource\nyet. (but there is a little extra config to make sure the API Gateway logging\nis all setup - this comes later)\n\n```conf\nresource \"aws_api_gateway_account\" \"default\" {\n  cloudwatch_role_arn = \"${aws_iam_role.apigw.arn}\"\n}\n\nresource \"aws_iam_role\" \"apigw\" {\n  name = \"${var.project}-apigw-role\"\n\n  assume_role_policy = \u003c\u003cEOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"apigateway.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nEOF\n}\n\nresource \"aws_iam_role_policy\" \"cloudwatch\" {\n  name = \"${var.project}-apigw-cloudwatch-policy\"\n  role = \"${aws_iam_role.apigw.id}\"\n\n  policy = \u003c\u003cEOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogGroups\",\n                \"logs:DescribeLogStreams\",\n                \"logs:PutLogEvents\",\n                \"logs:GetLogEvents\",\n                \"logs:FilterLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nEOF\n}\n```\n\nNow for the tricky part - creating the API Gateway. I'll break this down. First\ncreate the Gateway 'REST API':\n\n```conf\nresource \"aws_api_gateway_rest_api\" \"default\" {\n  name        = \"${var.project}\"\n  description = \"API for the ${var.project} lambda function\"\n}\n```\n\nNow create a resource for it. This is a resource in the API route sense rather\nthan the Terraform one.\n\n```conf\nresource \"aws_api_gateway_resource\" \"default\" {\n  rest_api_id = \"${aws_api_gateway_rest_api.default.id}\"\n  parent_id   = \"${aws_api_gateway_rest_api.default.root_resource_id}\"\n  path_part   = \"process\"\n}\n```\n\nWith a resource and API we can configure the resource with methods and\nintegrations.  How I understand it, a _Method_ is a type of request into an API\nGateway resource that is matched to an _Integration_. The Integration is\nresponsible for interacting with the backend - in our case a Lambda function.\n\nBelow I create a GET method on our resource that has no authentication. This\nbacks onto an Integration that POSTs to the function (Lambda functions can only\naccept POSTs as a trigger).\n\nI use an AWS_PROXY integration as it seemed to be the easiest way to just pass\nthe request down the stack.\n\n```conf\nresource \"aws_api_gateway_method\" \"default\" {\n  rest_api_id   = \"${aws_api_gateway_rest_api.default.id}\"\n  resource_id   = \"${aws_api_gateway_resource.default.id}\"\n  http_method   = \"GET\"\n  authorization = \"NONE\"\n}\n\nresource \"aws_api_gateway_integration\" \"default\" {\n  rest_api_id             = \"${aws_api_gateway_rest_api.default.id}\"\n  resource_id             = \"${aws_api_gateway_resource.default.id}\"\n  http_method             = \"${aws_api_gateway_method.default.http_method}\"\n  type                    = \"AWS_PROXY\"\n  uri                     = \"arn:aws:apigateway:${var.region}:lambda:path/2015-03-31/functions/${aws_lambda_function.lambda.arn}/invocations\"\n  integration_http_method = \"POST\"\n}\n```\n\nSo far all we can do is call the function, we can't get anything back out of\nthe Gateway. This was the most minimal config I could find to get the responses\nback out:\n\n```conf\nresource \"aws_api_gateway_method_response\" \"response_method\" {\n  rest_api_id = \"${aws_api_gateway_rest_api.default.id}\"\n  resource_id = \"${aws_api_gateway_resource.default.id}\"\n  http_method = \"${aws_api_gateway_integration.default.http_method}\"\n  status_code = \"200\"\n}\n\nresource \"aws_api_gateway_integration_response\" \"response_method_integration\" {\n  rest_api_id = \"${aws_api_gateway_rest_api.default.id}\"\n  resource_id = \"${aws_api_gateway_resource.default.id}\"\n  http_method = \"${aws_api_gateway_method_response.response_method.http_method}\"\n  status_code = \"${aws_api_gateway_method_response.response_method.status_code}\"\n}\n```\n\nNext create a 'stage'. Many of the other getting started examples have multiple\nstages but this was only supposed to take two minutes so I only have one -\nsorry. The stage is going to be attached to the other API Gateway resources and\nallows us to avoid some extra resources and stages.\n\n```conf\nvariable \"api_gateway_stage\" {\n  default = \"production\"\n}\n```\n\nFinally we tie our config to a deployed 'stage', exposing it to the world.\n\n```conf\nresource \"aws_api_gateway_deployment\" \"default\" {\n  depends_on  = [\"aws_api_gateway_integration.default\"]\n  rest_api_id = \"${aws_api_gateway_rest_api.default.id}\"\n  stage_name  = \"${var.api_gateway_stage}\"\n}\n```\n\nBut wait, it's not over yet. This final settings resource is required to enable\nAPI Gateway logging - told you there was an extra bit, this is it:\n\n```conf\nresource \"aws_api_gateway_method_settings\" \"default\" {\n  rest_api_id = \"${aws_api_gateway_rest_api.default.id}\"\n  stage_name  = \"${var.api_gateway_stage}\"\n  method_path = \"${aws_api_gateway_resource.default.path_part}/*\"\n\n  settings {\n    metrics_enabled = true\n    logging_level   = \"INFO\"\n  }\n\n  depends_on = [\"aws_api_gateway_deployment.default\"]\n}\n```\n\nSo far we've creatd the function, setup it's logging and configured a minimal\nAPI Gateway. The final step is to allow the two to communicate. This permission\nresource lets that happen.\n\n```conf\nresource \"aws_lambda_permission\" \"apigw_lambda\" {\n  statement_id  = \"AllowExecutionFromAPIGateway\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = \"${aws_lambda_function.lambda.arn}\"\n  principal     = \"apigateway.amazonaws.com\"\n\n  source_arn = \"arn:aws:execute-api:${var.region}:${data.aws_caller_identity.current.account_id}:${aws_api_gateway_rest_api.default.id}/*/*/*\"\n}\n```\n\nBut... that won't work without this Terraform data object to get the current\nAWS account info.\n\n```\ndata \"aws_caller_identity\" \"current\" {}\n```\n\nAs I mentioned before, all the config is in the [project's GitHub\nRepo](https://github.com/charlieegan3/borked/tree/b70ddcd141fe4fd2a8d3cf669f044d55ccbb4a7d/terraform)\nif you'd rather see it all together.\n","date":"Aug 10 2017","id":126,"title":"Terraform adventures: Deploy a serverless microservice ~2 mins","type":"blog post","url":"/posts/2017-08-10-terraform-lambda-api-gateway/"},{"body":"\nSince starting work full-time I've been lacking the energy and time for side\nprojects. Recently I've had a few free weekends and have completed initial\nversions of two new projects.\n\nI've been trying to write more in the Go language as well as experimenting with\nnew AWS products. The first project came out of a way to experiment with these.\nI generally like to learn new things as part of a project that does something -\nthis time *Borked* was what I made.\n\nI had the idea while I was working on charlieegan3.com. I'd introduced a few\nbroken links as I'd migrated the site to different setups over the years. I'd\nscanned the site with\n[BLS](https://github.com/stevenvachon/broken-link-checker) in a docker\ncontainer and fixed the links. I also added\n[HTMLProofer](https://github.com/gjtorikian/html-proofer) to my site to help\nstop me introducing any more.\n\nWith that completed, I was on the look out for another project. A concurrent\nbroken link scanner seemed like a good Go starter project - BLS was quite slow.\nThe very first version of Borked was a dead simple tool to scan sites for broken\nlinks.\n\nI came back to it sometime later to build v2.0 - the web version. I'd learned\nabout Go [context](https://golang.org/pkg/context/) in the meantime and was also\nkeen to try out AWS Lambda.\n\n[![borked](/posts/2017-08-28-back-in-the-side-project-groove/borked.png)](https://borked.charlieegan3.com)\n*Borked v2.0*\n\nI built Borked with the [eawsy Go lambda\nshim](https://github.com/eawsy/aws-lambda-go-net) - behind an API\nGateway. The web client is hosted on S3 and cloudfront - I'd enabled CORS on the\nfunction endpoint.\n\nThe client makes requests to the function with a root URL. The function must\nrespond within 30 seconds when behind an API Gateway. I have it configured to\nreturn the results after 10 seconds. The function returns a list of scanned as\nwell as incomplete links. The web client then sends subsequent requests to\ncomplete all the site's internal links - the function doesn't really have any\nrepresentation of a 'job'.\n\nStoring the progress of the scanning job on the client allows the function to be\nquite simple. It's not ideal but it works and seems to work reasonably quickly\ntoo.\n\nI'd like to add some extra features to Borked - I'd like to allow for\ncustomisations like exclude patterns or User-Agent settings.\n\nBased on the structure for Borked this weekend I was able to assemble RSSMerge\npretty quickly.\n\n[![rssmerge](/posts/2017-08-28-back-in-the-side-project-groove/rssmerge.png)](https://rssmerge.charlieegan3.com)\n*RSSMerge Homepage*\n\nI've been using [Feedbin](https://feedbin.com/) as an RSS reader for about 6\nmonths - it's got some good features but I'm really looking for a different\nsetup. I'm no longer commuting by train and would rather have a regular roundup\nof my followed feeds than trying to keep up with every item in each.\n\nCreating a merged RSS feed would allow me to get all my feeds as a single\nroundup email from IFTTT.\n\nRSSMerge will take a list of RSS feeds; fetch them; interleave them in order;\nremove the content (leaving only the title and link) and return a new RSS feed.\nFeeds are again fetched concurrently so it's quite quick (even a list of about\n50 feeds only takes a few seconds).\n\nThe list of feeds is passed to the API as a link to a raw Gist. This is a bit of\na hack but it means that RSSMerge doesn't require any state at all. The feed URL\nof the RSSMerge feed is just the endpoint + raw feed list parameter. E.g.\n\n```\nhttps://rssmerge.cluster.charlieegan3.com/build?source=...\n```\n\n([Example link](https://rssmerge.cluster.charlieegan3.com/build?source=https://gist.githubusercontent.com/charlieegan3/9190409c458bc4aa17ef52a8b682aba4/raw/60043038a1c8f668acddb506db58bdc049f144bd/rssmerge_sample))\n\nThis can be used by IFTTT to create roundups like this:\n\n![roundup](/posts/2017-08-28-back-in-the-side-project-groove/roundup.png)\n*IFTTT Roundup - note multiple feed sources*\n\nI have a history of not sticking to a news consumption strategy for more than a\nyear but we'll see how this goes.\n\nI think for my next project I'm going to make it something longer term. I might\neven deliberately avoid projects that could be immediately useful for me. I've\nfound that having something I can _almost_ use a little stressful and I'll spend\ntoo many late nights trying to get it working. I'm not sure that's particularly\nhealthy or productive. Having a project that that was more complicated and\nlonger term might help keep things more manageable while being a more valuable\nlearning experience.\n","date":"Aug 28 2017","id":127,"title":"Back in the side-project groove","type":"blog post","url":"/posts/2017-08-28-back-in-the-side-project-groove/"},{"body":"\nFor a series of reasons I recently bought a Google Pixel 2. I've been wanting a\nchange of scene and to go back to Android for some time - so far it's going\nwell. On top of all the benefits I was expecting - improved camera, speed and\ncustomisability over my iPhone 6 - there were a number of other surprising\nfeatures that I've found really helpful.\n\nThe first feature is background tab opening from _other apps_. I get email news\nroundups and open links in them from my email client. This feature allows me to\n'queue' a whole series of links for the next time I open Firefox - without\nleaving my email client. There's a little pop-up if I did ever want to open a\nlink right away.\n\n\u003cimg alt=\"background tab animation\" style=\"max-width: 300px; display: block;\" src=\"https://i.imgur.com/wvQpYET.gif\"\u003e\n_Background tabs from another app_\n\nFirefox also allows me to set my homepage to open when I open new tabs. I have\nbuilt my own page with a series of links I like to start from so having this as\nmy default new tab page is nice - if a small touch.\n\niOS is pretty fast to kill background apps. I often had issues with larger\nuploads to Slack and Dropbox. In Android these uploads get a dedicated task and\npersistent notification so I can see when and if they complete. No more\nreturning to a Slack conversation only to find the file never sent. Slack is\nnoticably faster on Android. Even vs an iPhone 8 which I tried out - I'm not\nsure what's going on with the iOS slack client...\n\nI've saved the simplest of these 'windfall' features for last. Android has what\nis effectively an alt-tab feature activated by double pressing the square menu\nbutton. This is really useful for copying and pasting from 1password. I find\nthis faster than the iOS browser integration.\n\nI also haven't even taken my pixel charger out of the box #USBC #future.\n","date":"Nov 3 2017","id":128,"title":"Welcome to Android","type":"blog post","url":"/posts/2017-11-03-welcome-to-android/"},{"body":"\nI enjoy using Instagram to share my photos. The restricted feature-set;\npublic-by-default; similarities to Twitter \u0026 (original) square format\nconstraint all appeal.\n\nLast year I completed a [_#365photochallenge_ on\nInstagram](https://www.instagram.com/p/BdX-nBXnuks/). Over the year, this took\nquite a lot of effort. As I switched to posting photos on the day they were\ntaken it became clear to me that this was a valuable record of my activities\nfor years to come.\n\nThis prompted me to find a way to extract a copy of the data I was creating and\nmanaging via Instagram. It turns out this is surprisingly hard.\n\nI originally started using Zapier/IFTTT to save photos, as they were taken,\ninto my Dropbox. Sadly, these services stopped saving the full-res images and\nsometimes missed posts(?). This also only saved new images - really I wanted my\nentire catalogue including all images and metadata.\n\nI tried using Airtable and Zapier for the metadata but found the Zapier free\ntier too limited and their plans too expensive. This also only worked new\nposts.\n\nI tried various Instagram media downloader extensions for Chrome; these save\nthe images but not the data. I also wanted the process to be largely automated\nwhich this wasn't.\n\nI found a project called [InstaLooter](https://github.com/althonos/InstaLooter)\nand opted to use that when I saw it was also able to save json post data.\n\nThis is the process: first, download the json post data in instaLooter to a\n`looted_json` folder.\n\n```bash\ninstaLooter USERNAME looted_json -v -D -T {date}-{id} --new --time thisyear\n# or in docker\ndocker run -v \"$(pwd)/looted_json:/out\" -it python:alpine3.6 sh -c \"pip install instaLooter \u0026\u0026 ls /out \u0026\u0026 instaLooter USERNAME /out -v -D -T {date}-{id} --new --time thisyear\"\n```\n\nNext, fill out the data by visiting each post's public page. Save the completed\ndata in `completed_json`.\n\n```ruby\n#!/usr/bin/env ruby\n\nrequire \"json\"\nrequire \"date\"\nrequire \"open-uri\"\n\nDir.glob(\"looted_json/*\").shuffle.map do |file|\n  completed_file_name = \"completed_json/#{file.split(\"/\").last}\"\n\n  next if File.exists?(completed_file_name)\n  puts file\n\n  raw_data = JSON.parse(File.read(file))\n\n  doc = open(\"https://www.instagram.com/p/#{raw_data[\"code\"]}\").read\n  page_data = doc.scan(/\\{[^\\n]+\\}/).map { |r| JSON.parse(r) rescue nil }.compact.first\n  graph_image = page_data.dig(*%w(entry_data PostPage)).first.dig(*%w(graphql shortcode_media))\n\n  caption = graph_image[\"edge_media_to_caption\"][\"edges\"].first[\"node\"][\"text\"] rescue \"\"\n  tags = caption.scan(/#\\w+/).uniq\n\n  completed_data = {\n    id: raw_data[\"id\"],\n    code: raw_data[\"code\"],\n    display_url: graph_image[\"display_url\"],\n    media_url: raw_data[\"is_video\"] ? graph_image[\"video_url\"] : graph_image[\"display_url\"],\n    post_url: \"https://www.instagram.com/p/#{raw_data[\"code\"]}\",\n    is_video: raw_data[\"is_video\"] == true,\n    caption: caption,\n    location: graph_image[\"location\"],\n    tags: tags,\n    timestamp: raw_data[\"date\"],\n    dimensions: raw_data[\"dimensions\"]\n  }\n\n  File.write(completed_file_name, JSON.pretty_generate(completed_data))\nend\n```\n\nI have an optional step here to download the location data. Using the location\ndata from each post, I can get a list of all the locations used and visit each\npage on the Instagram site to get it's coordinates. All the locations are saved\nas their own json file in a `locations` folder.\n\n```ruby\n#!/usr/bin/env ruby\n\nrequire \"json\"\nrequire \"open-uri\"\n\nDir.glob(\"completed_json/*\").map do |file|\n  JSON.parse(File.read(file))[\"location\"]\nend.uniq.compact.each do |location|\n  puts location[\"name\"]\n\n  location_file_name = \"locations/#{location[\"id\"]}.json\"\n\n  next if File.exists?(location_file_name)\n\n  html = open(\"https://www.instagram.com/explore/locations/#{location[\"id\"]}\").read\n  page_data = html.scan(/\\{[^\\n]+\\}/).map { |r| JSON.parse(r) rescue nil }.compact.first\n  location_data = page_data.dig(*%w(entry_data LocationsPage)).first[\"location\"]\n\n  location.merge!(lat: location_data[\"lat\"], long: location_data[\"lng\"])\n  location.delete(\"has_public_page\")\n\n  File.write(location_file_name, JSON.pretty_generate(location))\nend\n```\n\nFinally, all that's left to download is the media files (images and videos).\nLuckily, we have all the information we need in our `completed_json` files.\n\n```ruby\n#!/usr/bin/env ruby\n\nrequire \"json\"\nrequire \"open-uri\"\nrequire \"fileutils\"\n\nDir.glob(\"completed_json/*\").shuffle.map do |file|\n  data = JSON.parse(File.read(file))\n\n  format = data[\"is_video\"] == true ? \"mp4\" : \"jpg\"\n\n  media_file_name = \"media/#{file.split(\"/\").last.gsub(\"json\", format)}\"\n\n  next if File.exists?(media_file_name)\n  puts data[\"post_url\"]\n\n  begin\n    File.write(media_file_name, open(data[\"media_url\"]).read)\n    FileUtils.touch media_file_name, mtime: data[\"timestamp\"]\n  rescue\n    puts \"#{media_file_name} Failed\"\n    File.delete(media_file_name) if File.exists?(media_file_name)\n  end\nend\n```\n\nNow you'll also have a media folder with all media in!\n\nThe 'posting experience' on Instagram is the most satisfying I've found, it's\njust a shame it's such a pain to get your data out. I'd really love to see a\n\"Download my Instagram Archive\" feature like Twitter has.\n\nHaving access to the raw data is really fun. I was able to make a calendar for\nMum this Christmas for example. When I added the location data download I also\nbrought back a (rudimentary) version of the discontinued Instagram photo map.\n\n![map of instagram posts](/posts/2018-03-04-backing-up-instagram/map.jpg)\n\nThis features all the countries I've been to (didn't go abroad before getting\nmy Instagram account). I wonder what this will look like after a few more\nyears...\n\nI do worry about depending on Instagram as a means of personal archive in this\nway. However, I feel like now I have this format for extracting my data, that I\nhave some portability to continue this elsewhere if Instagram stops working for\nme.\n","date":"Mar 4 2018","id":129,"title":"Backing up Instagram","type":"blog post","url":"/posts/2018-03-04-backing-up-instagram/"},{"body":"\nMy [last post](/posts/2018-03-04-backing-up-instagram)\nexplained about how I create backups of my Instagram posts. I had a few people\nask about why this was useful. Other than the Instagram doomsday-scenario, in\nit's current form (a git repo with some images and JSON metadata) it wasn't.\n\nI was ill over the Easter weekend and decided to make something with the data\nI'd extracted. Enter [photos.charlieegan3.com](https://photos.charlieegan3.com).\n\nThe purposes of this new site are:\n\n- Compliment rather than replace my public Instagram profile.\n- Implement a number of features lacking or removed from Instagram.\n- Offer a place to browse only photos, no ads, comments, popups etc.\n\nExamples of some features:\n\n- [A map of all the places I've been](https://photos.charlieegan3.com/locations/) -\n  This was originally an Instagram feature but it was removed in 2016. The other\n  nice thing about this is that I got Instagram before I first left the British\n  Isles (we didn't go abroad when I was younger). This means that all the\n  countries I've been to feature on the map.\n- [List of photos with a given tag](https://photos.charlieegan3.com/tags/shotonmoment/) -\n  Many of my hashtags are just junk but there are some I use reliably such as\n  [#shotonmoment](https://photos.charlieegan3.com/tags/shotonmoment/) or\n  [#architecture](https://photos.charlieegan3.com/tags/architecture/). Instagram\n  doesn't offer a way to view all your photos for a given tags.\n- [Nearby places](https://photos.charlieegan3.com/locations/hereford-cathedral-344225019/) -\n  For a given 'place' (Facebook Graph Location), I've gotten my build scripts\n  to do some Trigonometry and find other places nearby. So even if a place only\n  has one photos, chances are the site will show some others nearby too.\n- [Full Calendar](https://photos.charlieegan3.com/calendar/) -\n  I made use of this [calendar](https://gohugohq.com/calendar/) implemented for\n  Hugo. It means I can link to show all the photos on a given\n  [day](https://photos.charlieegan3.com/archive/2018-02-20/),\n  [month](https://photos.charlieegan3.com/archive/2018-02-20/),\n  [year](https://photos.charlieegan3.com/archive/2018/),\n  [calendar month](https://photos.charlieegan3.com/archive/02/) or even\n  [weekday](https://photos.charlieegan3.com/archive/tuesday/).\n- I also have a link to search for the original in my Dropbox/Google Photos\n  accounts. This really is only useful to me as it requires a logged in session\n  on those other sites!\n\nAll in, it's about 5k pages of html. Hugo can build this in about 16 seconds;\nNetlify can deploy it in about 6 mins. It's also auto-updated 4 times a day\nfrom a task running on Hyper.sh. With over 10,000 pages, [felixonline](http://felixonline.co.uk/)\nis still the biggest static site I've made but if I post enough I might catch\nup in a decade or so!\n","date":"Apr 7 2018","id":130,"title":"I made an interactive portfolio site with Hugo","type":"blog post","url":"/posts/2018-04-07-i-made-an-interactive-portfolio-site-with-hugo/"},{"body":"\n**Update** _You can try the project [here](https://charlieegan3-stackr.herokuapp.com/). Code available [here](https://github.com/charlieegan3/stackr)._\n\n**Update 2** I have taken down the live demo - there are now many apps that make this easy on phones.\n\n## Intro\n\nAt the end of last year I started paying for a Creative Cloud subscription so\nthat I could use the [Adaptive Wide-Angle Filter](https://helpx.adobe.com/photoshop/using/adaptive-wide-angle-filter.html)\nto correct distorted images taken with my [Moment Superfish Lens](https://www.shopmoment.com/shop/new-superfish-lens)\nand play with [image stacking](https://helpx.adobe.com/photoshop/using/image-stacks.html)\nto take more interesting night-time shots.\n\nWhile the Adaptive Wide-Angle in Photoshop is really well implemented, (Example\n[before](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/lens_distorted.jpg)\nand\n[after](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/lens_corrected.jpg))\nI found that with the\n[SKRWT](http://www.skrwt.com/) 'suite' of apps I could get much the same result\nfrom using just my phone.\n\nHowever, I've been unable to find an app that does image stacking on Android.\nThis is a stacked-edit from Photoshop using photos taken on my phone over\nChristmas. Note the stars and roads in the distance.\n\n[\n  ![stacked image from photoshop](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/photoshop_stacked.jpg)\n](https://photos.charlieegan3.com/photos/2017-12-29-1680667219972842009/)\n\nI'd love to be able to create images like this without needing to sit down and\nwait for Photoshop as well as various uploads and downloads to Dropbox in\nbetween.\n\nI'm opposed to using Photoshop in my 'workflow' for the following reasons:\n\n- I need to use my laptop, often this means uploading many photos and loading\n  them into a stack takes ages.\n- It is not free\n- Pouring over Photoshop for hours on your laptop is hardly in the spirit of\n  \"the best camera is the one you have with you\"\n\nSo with SKRWT covering my Adaptive Wide-angle needs I set out to create a\ntool for image stacking I could use from my phone.\n\n\n## Fundamentals\n\nAt the most basic level I need to be able to turn videos into stacked stills\nwith the option of tuning various parameters (I can turn sets of images into\nvideos on my phone if need be - Google Photos to create an Animation, then GIF\nto MP4).\n\nYou can do this with the following commands:\n\n```\nffmpeg -i video.mp4 -r $FPS -f image2 frame_%07d.png\nconvert frame_* -evaluate-sequence mean stacked.jpg\n```\n\nSometimes I also need to align the stills picked from the video:\n\n```\nffmpeg -i video.mp4 -r $FPS -f image2 frame_%07d.png\n\nif [ \"$ALIGN\" = true ] ; then\n\talign_image_stack -m -a aligned_ frame_*\n\tconvert aligned_* -evaluate-sequence mean stacked.jpg\nelse\n\tconvert frame_* -evaluate-sequence mean stacked.jpg\nfi\n```\n\nIt'd also be nice to use more of the evaluate-sequence modes:\n\n```\nMODE=median\nMODE=min\nMODE=max\nconvert frame_* -evaluate-sequence $MODE stacked.jpg\n```\n\nI tested this out on my laptop and it worked well enough. Aligning the images\nwas slow but it takes forever in Photoshop too.\n\nThis is the easy bit though. Next I needed to work out how to run this from my\nphone.\n\n## I called my thing stackr\n\nFrom the commands above I knew I needed FFmpeg, Hugin and ImageMagick. I also\nknew that I couldn't run this on Heroku because of timeouts. Similarly, it's\nawkward to run on Lambda as sometimes a job might take over 5 minutes to\ncomplete.\n\nWhat I needed was to be able to run an arbitrary container with some params.\nHyper.sh to the rescue!\n\nI'd recently enjoyed using Hyper.sh to [automate my photo\nwebsite](/posts/2018-04-07-i-made-an-interactive-portfolio-site-with-hugo)\nand thought it'd work well here too - mainly because containers can run for as\nlong as required.\n\nI also needed to run a frontend that could handle the video uploads and present\nthe options form. I decided to just run this as a sinatra app on Heroku - using\nwhat I knew best in the hope of just getting something working.\n\nThis is what it looked like by the time I had an MVP. First upload a video with\nframes to use in the stack. Choosing the mode, whether to auto align images and\nFPS at the same time.\n\n![form](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/form.png)\n\nNext, something like this happens:\n\n![system diagram](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/diagram.png)\n\nUsing a bucket to store the video, the Heroku app spawns a container to\ngenerate the stacked image. When it's done, it sends me a notification with\n[Pushover](https://pushover.net/).\n\nWith this I can quite easily experiment with different stack modes from my phone\n- and with minimal battery impact too.\n\n## Some Examples\n\nI've been inside all weekend so I've got to draw from my past videos for this\nbit. I only keep the stacked image - not the all the source images.\n\nStandard blurring out the motion of the waves example:\n\n![gun](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/gun.jpg)\n[Source Video](https://photos.charlieegan3.com/photos/2016-05-29-1261099882943047095/)\n\nStacking is also good for removing people from busy shots:\n\n![crowd](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/crowd.jpg)\n[Source Video](https://photos.charlieegan3.com/photos/2014-10-22-836785782090590192/)\n\nThese are both pretty poor quality videos taken on a Galaxy Note II. Here's one\nof a plane coming in to land at Heathrow this afternoon. No tripod, 4k, pixel 2.\n(Click for full-size)\n\n[\n![4k](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/4k.jpg)\n](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/4k.jpg)\n[Source Video](https://photos.app.goo.gl/9tvOK7kZRoKtke9p1)\n\nOr one from a video I took on the way to work:\n\n[\n![4k](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/church.jpg)\n](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/church.jpg)\n[Source Video](https://photos.app.goo.gl/jl985Kju1IuqPtX32)\n\nThis one's based of a short video waiting for the lights in putney, just\nholding it in my hand.\n\n[\n![4k](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/putney.jpg)\n](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/putney.jpg)\n[Source Video](https://photos.app.goo.gl/yEZkWcx0NK2VgXIs2)\n\nYou can also create some more unusual ones:\n\n[\n![4k](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/trainblur.jpg)\n](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/trainblur.jpg)\n[Source Video](https://photos.app.goo.gl/zTSZhgbvMN90K02c2)\n\nSometimes it's not always best to attempt to align the shots!\n\n[\n![4k](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/art.jpg)\n](/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/art.jpg)\n[Source Video](https://photos.app.goo.gl/zTSZhgbvMN90K02c2)\n\n## How can I get it?\nIf you're interested to try this out let me know and I can let you in. I'm\nkeeping the site private as each upload as the potential to incur a cost.\n\nI expect to post the code on GitHub in the next day or so.\n","date":"Apr 22 2018","id":131,"title":"Bringing Photoshop image-stacking to mobile with FFmpeg, Hugin, ImageMagick \u0026 Hyper.sh","type":"blog post","url":"/posts/2018-04-22-bringing-photoshop-imagestacking-to-mobile-with-ffmpeg-hugin-imagemagick-hypersh/"},{"body":"\n**Update**: I've posted about a small refinement\n[here](/posts/2019-03-02-running-a-cheap-gke-cluster-revisted/).\n\nJoining Jetstack earlier this year finally convinced me that I needed a\nside-project cluster. All the cool kids had one and I wanted one too.\n\nI didn't want to spend any money though, or at least I wanted to do this on the\ncheap. I also didn't want to run a real node in my flat.\n\nWhat I really wanted was a GKE cluster. I needed to just focus on getting things\nrunning. Eventually, I want to run all my side-projects on Kubernetes - 5\nyear plan!\n\nThere were some problems with the money though. I was doing this on the cheap.\n\nI could get nodes on Digital Ocean really cheap. I also liked the idea of running\na cluster on Scaleway. I just didn't really want to deal with the actual\nmanaging the cluster - hmm.\n\nI looked into the [Single Node Kubernetes Cluster](https://github.com/kelseyhightower/kubeadm-single-node-cluster)\nguides a bit. I tried the one for [Digtal Ocean](https://github.com/julianvmodesto/kubeadm-single-node-cluster-digitalocean).\n\nI didn't want to bother with the upgrades \u0026 faff. In the end I came\nup with a better idea.\n\nOne of the main reasons I'd been avoiding using GKE was the load balancer\npricing. I was down to get setup with a single node cluster on there - I just\ndidn't want a load balancer (I didn't want the cost of running one for my\ningress controller service).\n\nI don't have remotely enough traffic to my side projects to even come close to\n_needing_ a load balancer and I only have one node anyway... At the same time I\nwanted public ingress - obvs.\n\nThen I had my idea. Why not give my free-tier f1-micro a static IP and have it\nrun my ingress controller? Taint it stop other pods running there and run\neverything else on a preemptible node?\n\nHappy to take the (pretty short) downtime hit each day, this is what I went for.\n\nHere's how it's setup in Terraform. Note the 'ingress' node pool, machine type \u0026\ntaints.\n\n```hcl\nresource \"google_container_cluster\" \"main\" {\n  name = \"main\"\n  zone = \"${var.cluster_zone}\"\n\n  lifecycle {\n    ignore_changes = [\"node_pool\"]\n  }\n\n  node_pool {\n    name = \"default-pool\"\n  }\n}\n\nresource \"google_container_node_pool\" \"ingress\" {\n  name       = \"ingress\"\n  zone       = \"${var.cluster_zone}\"\n  cluster    = \"${google_container_cluster.main.name}\"\n  node_count = 1\n\n  management = {\n    auto_repair  = true\n    auto_upgrade = false\n  }\n\n  node_config {\n    preemptible  = false\n    machine_type = \"f1-micro\"\n    disk_size_gb = 20\n\n    taint = {\n      key    = \"ingress\"\n      value  = \"true\"\n      effect = \"NO_EXECUTE\"\n    }\n\n    labels = {\n      ingress = \"true\"\n    }\n\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/compute\",\n      \"https://www.googleapis.com/auth/devstorage.read_only\",\n      \"https://www.googleapis.com/auth/logging.write\",\n      \"https://www.googleapis.com/auth/monitoring\",\n    ]\n  }\n}\n\nresource \"google_container_node_pool\" \"main\" {\n  name       = \"main\"\n  zone       = \"${var.cluster_zone}\"\n  cluster    = \"${google_container_cluster.main.name}\"\n  node_count = 1\n\n  management = {\n    auto_repair  = true\n    auto_upgrade = true\n  }\n\n  node_config {\n    preemptible  = true\n    machine_type = \"n1-standard-2\"\n\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/compute\",\n      \"https://www.googleapis.com/auth/devstorage.read_only\",\n      \"https://www.googleapis.com/auth/logging.write\",\n      \"https://www.googleapis.com/auth/monitoring\",\n    ]\n  }\n}\n```\n\nThere is one more thing - that static IP.\n\nI created a static IP ([see\nhere](https://cloud.google.com/network-tiers/docs/using-network-service-tiers#creating_static_external_addresses))\nand manually edited the network interfaces on the f1-micro vm in GCE :O\n\nDon't judge, I wasn't able to find a way to do this in Terraform and figured\nthis was 'good enough' for a side project cluster.\n\nFinally, I needed to make sure that the ingress controller landed on that node.\nHere's a simplified snippet from my ingress controller deployment. Note the\ntolerations and nodeSelector.\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: ingress-nginx\nspec:\n  template:\n    spec:\n      tolerations:\n      - key: ingress\n        value: \"true\"\n        effect: NoExecute\n      nodeSelector:\n        ingress: \"true\"\n...\n```\n\nFinally finally, I point my DNS at the static IP and I'm done.\n\n```\n$ dig cluster.charlieegan3.com\n\n...\n\n;; ANSWER SECTION:\ncluster.charlieegan3.com. 300   IN      A       35.197.243.26\n```\n\nSo there you have it. If I need to run more stuff I can just use a bigger\npreemptible node or add another. They're cheap enough for me at the moment and\nworth it for the convenience of GKE - imo.\n","date":"Aug 15 2018","id":132,"title":"Running a cheap GKE cluster with public ingress \u0026 zero load balancers","type":"blog post","url":"/posts/2018-08-15-cheap-gke-cluster-zero-loadbalancers/"},{"body":"\nI've long been\n[searching](/blog/2017/01/12/heroku-treasure) for a\ngood, self-hosted, personal wiki. I went as far to build my own with client-side\nencryption running on Heroku as a Rails app. I guess this was version 1. This\npost is about version 2.\n\n## Why\n\n**Why was version 1 not good enough?**\n\n- It ran on the free tier of Heroku and had to boot for 10 seconds whenever I\n  needed to use it, this was a pain as my use was quite infrequent.\n- I didn't want to pay $7 a month to keep it running full time.\n- The client side encryption method I'd built using SJCL was cool but clunky. All\n  the decryption needed to happen on the client, this made exporting data in\n  bulk harder than I'd expected.\n- The decryption key was also stored in local storage which seemed to get\n  cleared out more regularly than I'd expected.\n\n**Why even do this at all?**\n\nI'm moving all my side-projects to Kubernetes. Some deets on my cluster [here](/posts/2018-08-15-cheap-gke-cluster-zero-loadbalancers).\n\n**Why not run version 1 on Kubernetes then?**\n\nI didn't want to run the database for it in-cluster. Some of the information in\nthe wiki I'm really keen to keep. I want to store the wiki in git.\n\n## What\n\nWhile looking for a git based wiki, I came across\n[gollum](https://github.com/gollum/gollum). gollum is a ruby gem that runs a\nlocal server that interacts with the git index to both serve and store content.\nI liked it and decided to experiment with getting it running. There were some\nadditional requirements:\n\n- This needs to be stored encrypted\n- Only I should have access.\n\nThese pose some problems. Before, if I'd needed files to be encrypted in git,\nI'd used [git-crypt](https://github.com/AGWA/git-crypt). git-crypt is easy to\nuse, you add GPG keys, specify files to be encrypted in a `.gitattributes` file\nand that's it really. Sadly, it's not possible to use this on a repo with\ngollum. gollum reads files to show them on wiki pages and forms _from the git\nindex_ - not the local file system. With git-crypt, the files are stored in the\nindex encrypted. I needed something else.\n\nEnter, [`git-remote-gcrypt`](https://github.com/spwhitton/git-remote-gcrypt).\nThis is a package that adds some functionality to git. It is invoked\nautomatically with a URI prefix in a git remote:\n\n```\ngcrypt::https://example.com/user/repo.git\n```\n\nAs opposed to:\n\n```\nhttps://example.com/user/repo.git\n```\n\nRather than individual files being encrypted in the index as they are with\ngit-crypt, the entire git index is encrypted when interacting with the remote.\n\nThis means that the local copy of the git index is clear and can be read by\ngollum - hooray!\n\nThe only other feature I needed to replicate in the new version was\nauthorization. I needed to make sure that only I was able to read and update the\nwiki. gollum is designed to be run as a local server in a local repo so doesn't\nhave any features for this out of the box. I wanted to have access to it from my\nother devices too.\n\nI decided to solve this with the [bit.ly oauth2 proxy](https://github.com/bitly/oauth2_proxy).\nIt's possible to configure nginx running as an ingress controller to use the\noauth2_proxy for certain backends. This is easily configured with the\n[following guide](https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/auth/oauth-external-auth/README.md).\n\nWith this setup, I had a means of only allowing traffic into the service that\npassed my oauth check (to have my email).\n\n### 'gollum-server'\n\nI should also explain how this all fits together and works with gollum running\nin a container. There's a tricky bit and it's to do with GPG...\n\nThis is the Dockerfile for the service, pretty harmless right?\n\n```Dockerfile\nFROM ruby:2.4\n\nRUN apt-get remove gnupg -y\nRUN apt-get update \u0026\u0026 apt-get install -y gnupg2 git-remote-gcrypt vim expect\n\nRUN gem install gollum -v 4.1.2\n\nCOPY entrypoint.sh /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD []\n```\n\nWhat about that entrypoint though? Not so much. What does this container need to\ndo?\n\n1. Download the wiki\n1. Decrypt the wiki\n1. Serve the wiki\n1. Push updates made by gollum to back to the wiki's repo\n\nI'll break it down.\n\n#### 1. Configure the container's git installation:\n\nNote that we're settings some flags for gcrypt here too. I learned what to\nset here from [this\npage](http://git-annex.branchable.com/tips/fully_encrypted_git_repositories_with_gcrypt/) - I think...\n\n```bash\ngit config --global user.email \"wiki@example.com\"\ngit config --global user.name \"Wiki Robot\"\n\ngit config --global --add gcrypt.publish-participants true\ngit config --global --add gcrypt.participants $GPG_KEY_ID\ngit config --global user.signingkey $GPG_KEY_ID\ngit config --global commit.gpgsign true\n```\n\n#### 2. Save the credentials to download the repo from GitHub:\n\nThese are stored as a secret in Kubernetes and available as environment\nvariables.\n\n```bash\nmkdir -p ~/.ssh\necho $SSH_PUBLIC \u003e ~/.ssh/id_rsa.pub\necho $SSH_PRIVATE | awk '{gsub(/\\\\n/,\"\\n\")}1' \u003e ~/.ssh/id_rsa\necho $GITHUB_COM_KEY \u003e ~/.ssh/known_hosts\nchmod 0400 ~/.ssh/*\n```\n\n#### 3. Do the same for GPG and configure it.\n\nI set the cache-ttl to be long so that I only need to do the pinentry once.\nGPG pinentry is a major pain \u0026 the hardest part about making this while\nproject work.\n\n```bash\nmkdir ~/.gpg\necho $GPG_PUBLIC | awk '{gsub(/\\\\n/,\"\\n\")}1' | base64 -d \u003e ~/.gpg/public.key\necho $GPG_PRIVATE | awk '{gsub(/\\\\n/,\"\\n\")}1' | base64 -d \u003e ~/.gpg/private.key\ngpg --pinentry-mode loopback --passphrase=\"$GPG_PASSPHRASE\" --import ~/.gpg/*.key\n\necho \"pinentry-mode loopback\" \u003e\u003e ~/.gnupg/gpg.conf\necho \"pinentry-mode loopback\" \u003e\u003e ~/.gnupg/gpg-agent.conf\necho \"default-cache-ttl 34560000\" \u003e\u003e ~/.gnupg/gpg-agent.conf\necho \"maximum-cache-ttl 34560000\" \u003e\u003e ~/.gnupg/gpg-agent.conf\necho \"max-cache-ttl 34560000\" \u003e\u003e ~/.gnupg/gpg-agent.conf\ngpg-connect-agent reloadagent /bye\n```\n\n#### 4. Create a passphrase expect script to handle the first and only GPG prompt:\n\n```bash\ncat \u003e /usr/local/bin/passphrase \u003c\u003cEOF\n#!/usr/bin/expect\n\nset timeout 60\nset command [lindex \\$argv 0]\n\neval spawn \"\\$command\"\nexpect \"Enter passphrase:\" { send -- \"$GPG_PASSPHRASE\\n\" }\nexpect eof\nEOF\n\nchmod +x /usr/local/bin/passphrase\n```\n\n#### 5. Use this massive, great, whopping HACK to clone the repo from GitHub:\n\nSince we set a high ttl, we aren't prompted on future operations with GPG,\nthank _god_.\n\n```bash\npassphrase \"git clone $REPO_REMOTE site\" \u0026\u0026 cd site\n```\n\nFinally the entrypoint starts gollum with our 'pyramid of doom' custom config:\n\n```ruby\n#!/usr/bin/env ruby\nrequire 'fileutils'\nrequire 'gollum/app'\n\nwiki = Gollum::Wiki.new(\".\")\n\nThread.new do\n  loop do\n    sleep 3\n    if File.exists?(\"sync\")\n      if system(\"git pull origin master\")\n        if system(\"git push origin master\")\n          content = File.read(\"Home.md\").gsub(/^Updated:.*/, \"Updated: #{Time.new}\")\n          File.write(\"Home.md\", content)\n          system(\"git add Home.md; git commit -m update\")\n\n          File.delete(\"sync\")\n        end\n      end\n    end\n  end\nend\n\n# Per https://github.com/gollum/gollum-lib/issues/12\nGollum::Hook.register(:post_commit, :hook_id) do |committer, sha1|\n  FileUtils.touch(\"sync\")\nend\n```\n\nThis a little more complex than it needs to be really, but I wanted to have some\nkind of acknowledgement that the wiki had update in the gollum interface. The\neasiest way to get this working was writing a timestamp to the homepage. Yeah,\nyeah...\n\nI run the updates to git in another ruby thread to keep the wiki responsive.\n\n### Deploy that thang\n\nThe deployment is really boring. Just run the container with some secrets\navailable.\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wiki\n  labels:\n    app: wiki\n  namespace: wiki\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wiki\n  template:\n    metadata:\n      labels:\n        app: wiki\n    spec:\n      containers:\n      - name: web\n        image: charlieegan3/wiki:1e6007ff832f6afaa7c2b15e1044f907\n        args: [\"make\", \"server\"]\n        envFrom:\n        - secretRef:\n            name: wiki-config\n        ports:\n        - containerPort: 4567\n        resources:\n          limits:\n            cpu: \"100m\"\n            memory: \"100Mi\"\n          requests:\n            cpu: \"100m\"\n            memory: \"100Mi\"\n```\n\n`make server` just runs `gollum -c config.rb` which starts the gollum server\nwith our config above.\n\nThe ingress is a little more interesting, we can see the annotations for the\noauth proxy:\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: wiki\n  namespace: wiki\n  labels:\n    app: wiki\n  annotations:\n    certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\"\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/auth-url: \"https://subdomain.example.com.com/oauth2/auth\"\n    nginx.ingress.kubernetes.io/auth-signin: \"https://subdomain.example.com/oauth2/start?rd=https%3A%2F%2Fsubdomain.example.com\"\nspec:\n  tls:\n  - hosts:\n    - subdomain.example.com\n    secretName: wiki-tls\n  rules:\n  - host: subdomain.example.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: wiki\n          servicePort: 80\n```\n\n## Conclusion\n\nIt works - just about.\n\nThis gives me a wiki that only I can access, that's encrypted and available on\nall my devices. I didn't even need to build the interface #winning.\n\n(but wow, GPG UI so hard...)\n","date":"Sep 1 2018","id":133,"title":"Running a wiki with Gollum on Kubernetes","type":"blog post","url":"/posts/2018-09-01-gollum-wiki-on-kubernetes/"},{"body":"\nJetstack‚Äôs current course offering is largely aimed at those in an operational\nrole - deploying and managing clusters. With this new course, we‚Äôre introducing\na new ‚Äòtrack‚Äô for a developer audience. This course is for developers building\nand architecting applications to be deployed on a Kubernetes-based platform.\n","date":"Oct 9 2018","id":134,"title":"Introducing Jetstack's Kubernetes for Application Developers Course","type":"external blog post","url":"https://blog.jetstack.io/blog/app-dev-deploy/"},{"body":"\nFor my latest side project I built a tool to better track the music I listen to.\nI now have a single BigQuery table with my play history from Spotify, Youtube,\nSoundcloud and Shazam. I also have a [simple\nsite](https://music.charlieegan3.com/) to present the data.\n\nIn this post I explain why I did this, exactly what I've built and what I might\ndo next with the project.\n\n# _Why:_ I was loosing play data\n\nI've been a long time [Last.fm user](https://www.last.fm/user/charlieegan3).\nWhen I first started 'scrobbling' to the service I used iTunes but the bulk of\nthe data on my profile there has come from the Spotify integration.\n\nI'm very grateful for this feature. Had it not been there, the benefits to me of\nusing Spotify on multiple devices would have outweighed the hassle in tracking\nmy plays. Thanks to this feature, I've got a great starting point for my new\nproject.\n\nHowever, I found that the integration was a little flakey and I was loosing play\ndata. Sometimes I'd be signed out of Last.fm after an update and I'd not realise\nI needed to sign back in.\n\nThis was the first reason that prompted me to think about investigating this\nproblem.\n\n# _Why:_ I wanted to 'own' my data\n\nLast.fm are very nice to me. They store my plays and present them back to my\nwith a some nice graphs and views.\n\nHowever, it's not very flexible. I can't write my own views and I don't have\naccess to the raw data to ask the questions I want. On top of that, I don't\nthink the few visualizations I use are that hard to recreate.\n\nI've been trying to follow on from my [photos\nproject](https://photos.charlieegan3.com/) project and maintain a separate\npresentation of my social profiles that I control.\n\nMy play data seemed like a good candidate. It was 'larger' than the photos\ndataset and I thought it would provide some new challenges and learnings (which\nit did, spoiler alert, oops, that was all backwards, sorry-not-sorry).\n\n# _Why:_ I wanted to track other sources without a Last.fm client\n\nLast.fm clients were an option and something I experimented with. I found the\nAndroid options to be OK but ultimately unsuitable.\n\nI found myself listening to music on Soundcloud and\n[Youtube](https://youtu.be/67HOjCV8dqA) a (little) more often and wanted to save\nthis play data too.\n\n# _Why:_ I wanted to track other events and information\n\nEventually, I wanted to start tracking additional events beyond plays. Some\nexamples:\n\n- Which tracks were added and removed from my playlists and when\n- How long the tracks were that I was listening to (Last.fm didn't seem to\n  expose much here)\n- erm, that's about it actually\n\n\u003chr/\u003e\n\nNow onto the what I did to address this pressing first-world problem I faced.\n\n# _What:_ I 'built' a BigQuery table \u0026 a program to push data from Spotify\n\nFirst I wrote a simple program that make a call to the Spotify API to get the\nrecent plays for the current user (me). This list is limited to 50 so it needs\nto be run quite regularly to ensure all the data is captured.\n\nThis program is packaged as a container and run every 15 mins as a `CronJob` on\nmy personal cluster.\n\nIt queries the list of already saved tracks to get the most recent one. It then\niterates over each of the played tracks and imports all of the tracks played\nafter the latest of the last import (in ascending order).\n\nThis has been the first project where I've done the whole Terraform from the\nground up thing too (which involved a little legwork to move some existing\nresources under Terraform). There are a few resources outside of Kubernetes\nincluding a Google Cloud project, BigQuery table, some storage buckets and a\nservice account.\n\nThere have been a few iterations on the table schema but this is the current\nfield list. Everything just goes into one big table.\n\n```\ntrack\nartist\nalbum\ntimestamp\ncreated_at\nduration\nalbum_cover\n\nsource\n\nspotify_id\nyoutube_id\nyoutube_category_id\nsoundcloud_id\nsoundcloud_permalink\nshazam_id\nshazam_permalink\n```\n\nWith this data, I can do some fun things...\n\nCount the plays for each track:\n\n```sql\nSELECT\n  track,\n  artist,\n  count(track) as count,\n  STRING_AGG(album, \"\" ORDER BY LENGTH(album) DESC LIMIT 1) as album,\n  ANY_VALUE(duration) as duration,\n  ANY_VALUE(spotify_id) as spotify_id,\n\tSTRING_AGG(album_cover, \"\" ORDER BY LENGTH(album_cover) DESC LIMIT 1) as artwork\nFROM `charlieegan3-music-1.music.plays`\nGROUP BY track, artist\nORDER BY artist ASC, count DESC\n```\n\nGet my 10 most recent Shazams (see next section):\n\n```sql\nSELECT track, artist, timestamp\nFROM `charlieegan3-music-1.music.plays`\nWHERE source = \"shazam\"\nORDER BY timestamp DESC\nLIMIT 10\n```\n\nGet a sorted list of my top Halo soundtrack plays across all the different\nalbums.\n\n```sql\nSELECT artist, track, album, count(track) as play_count\nFROM `charlieegan3-music-1.music.plays`\nWHERE REGEXP_CONTAINS(album, r'Halo')\nGROUP BY artist, track, album\nORDER BY play_count DESC\n```\n\n\n# _What:_ Next, I built a website \u0026 some more integrations\n\nI wanted to go end-to-end quickly and have a simple 'view' into this dataset I\nwas accumulating. The site is at\n[music.charlieegan3.com](https://music.charlieegan3.com/).\n\nIt has a few pages. Each page is based on a single JSON file built from one or\nmore queries made against BigQuery. The recent plays file is updated every 15\nmins, the others are less regular. They're all stored in a Google Cloud Storage\nbucket. The site itself (not the JSON data) is served from an nginx container in\nthe cluster (makes the subdomain config and updates more similar to my other\nprojects. General rule for new projects if it's stateless, it runs in the\ncluster).\n\nI also built some more integrations (a key requirement of the project). Youtube\ncame first after Spotify - it was really hard. Youtube's API no longer exposes\nthe 'Watch History' 'Playlist'. It used to be accessible as a playlist with the\nID `WH` but that was disabled a few years ago - I suspect for privacy reasons.\nStill, it seems a shame to me that I can't get the data for my own account...\n\nTo get around this I needed to fall back to my roots and build a scraper - just\nlike old times. Youtube's page is rendered from 'initial data' served as part of\nthe page. This is JSON, yay. It is also a complete labyrinth, boo. I was able to\nuse [gron](https://github.com/tomnomnom/gron) and to grep out the parts\nof the JSON I was interested in and used\n[gojson](https://github.com/ChimeraCoder/gojson) to generate a struct to\nunmarshal it into.\n\nThis took ages but in the end I was able to get the metadata from the content ID\ntable for each video in a format that was good enough to save into my table.\n\nI also built similar scrapers for Shazam and Soundcloud but they're DOM /\nprivate APIs were much less surprising.\n\nSo far I've been running these for about a month and they've not broken -\nfingers crossed. I'm used to playing the scraper time bomb waiting game.\n\n# _Where Next:_ Some ideas for the future\n\nVisualizations. At the moment there is only one graph on the site - the plays\nmonth. This is about as good as my charts get. Luckily ma boi, @tlfrd is on the\ncase with a [proof-of-concept 'viz'](https://beta.observablehq.com/@tlfrd/plays)\nshowing the lifetime play history for individual songs. I'm really interested in\nimproving the presentations of the data in time. However, now the collection\nside of the project is in place I'm going to be pausing for a bit.\n\nAnother idea I had was to send email summaries each week showing my play data. I\nthink this might also serve as quite a good monitoring function (i.e. to check\nthat the data matches what I remember listening to).\n\nI'm also considering syncing the data back to Last.fm or\n[Libre.fm](https://libre.fm/) but I think this is close to the bottom of the\nlist for me at the moment.\n\n\u003chr/\u003e\n\nSo there we have it. Another rushed post about a side project that's been\ndistracting me recently.\n\nI'm enjoying this as a general direction for my side projects though. Personal\nAnalytics / Quantified Self is so much more than calorie counting. The data's\nthere, you've just got to get it.\n","date":"Nov 20 2018","id":135,"title":"Tracking all the plays or how I learned to stop worrying and build my own Last.fm","type":"blog post","url":"/posts/2018-11-20-how-i-learned-to-stop-worrying-and-build-my-own-lastfm/"},{"body":"\nLast year I wrote a\n[post](/posts/2018-08-15-cheap-gke-cluster-zero-loadbalancers/) about how I run\nmy personal cluster in a cost-effective way on GKE. I've since been able to\nautomate the main manual step and thought I'd post a short update.\n\n## Recap\n\nThis is what I used to do:\n\n1. Deploy cluster with Terraform\n1. Connect a reserved static IP manually to the single node in the ingress node\n   pool where NGINX runs.\n1. Update the `externalIPs` of the NGINX ingress controller service to the\n   internal IP of the node.\n\nThis has two manual steps and they caused some pain recently.\n\n## V2\n\nI knew I could use `hostNetwork: true` on the NGINX deployment instead (I'm just\nrunning a single pod - personal cluster etc.). So that'd save the pain of\nupdating the internal IP.\n\nI needed a solution for the external IP though where I point\n`*.charlieegan3.com`. I did some digging and came across\n[kubeip](https://github.com/doitintl/kubeip).\n\nkubeip's intended use is for assigning static IPs from a pool to nodes in a GKE\ncluster so traffic comes from a known set of IPs. This isn't what I'm trying to\ndo but it works for me, I have one IP and want to assign it to nodes in my\ningress node pool of size 1.\n\nWith this running in the cluster it watches for instances in the given pool\nwithout a label and assigns a labeled IP addresses to the node.\n\n\u003chr\u003e\n\nSo that's it really - just a short update on how I made my cluster slightly\neasier to run. No load balancers needed!\n","date":"Mar 2 2019","id":136,"title":"Running a cheap GKE cluster: Revisted","type":"blog post","url":"/posts/2019-03-02-running-a-cheap-gke-cluster-revisted/"},{"body":"\nJetstack works with many customers using Google Cloud‚Äôs Kubernetes Engine.\nWe work closely with teams to configure their clusters to conform with best\npractises.\n","date":"Aug 8 2019","id":137,"title":"Introducing our best-practice GKE Terraform module","type":"external blog post","url":"https://blog.jetstack.io/blog/terraform-gke-module/"},{"body":"\nJetstack often works with customers to provision multi-tenant platforms on\nKubernetes. Sometimes special requirements arise that we cannot control with\nstock Kubernetes configuration. In order to implement such requirements, we‚Äôve\nrecently started making use of the Open Policy Agent project as an admission\ncontroller to enforce custom policies.\n","date":"Sep 5 2019","id":138,"title":"How a simple admission webhook lead to a cluster outage","type":"external blog post","url":"https://blog.jetstack.io/blog/gke-webhook-outage"},{"body":"\nI went to Japan last month. As we walked around I found myself enjoying the\nlittle differences more than anything else. Since returning, I've found myself\ntalking about them a lot and thought it'd make for quite a good alternative\nwrite up of the trip.\n\n\u003c!-- more --\u003e\n\n(if you're interested in seeing some selected pictures, you can find them on my\n[photos site](https://photos.charlieegan3.com/archive/month/2019-09))\n\n## The doors on the left side will open\n\nWe made pretty good use of the metro in both Tokyo and Osaka in quite a short\nspace of time. The summary here is that it's an amazingly easy set of\ninterconnected systems that runs like clockwork, as expected. Some features I\nparticularly liked...\n\n- Ticket gates are open when you arrive. Scanning the IC card is instant and the\n  barriers don't need to close if you have enough money. The barriers only close\n  if there is a problem. This makes moving a crowd through a set of gates much\n  faster than in London where gates sometimes take longer to open than others\n  and require you to stop walking.\n- Some older trains have a special design of seat that can be flipped to make a\n  reasonably comfy seat for both directions of the route or to make a group of\n  facing seats. There's a [video\n  here](https://www.youtube.com/watch?v=DxDrPeb2mxs) of a similar more modern\n  design in Sydney.\n- The TFL map is famous for being easy to use, however one thing I'd love\n  help with is navigating complex underground stations such as Bank or Green\n  Park. I think the main problem is that it's hard to picture how the tunnels\n  connect in 3D space.\n\n  This is something many stations in Japan had sorted - I think they're a work\n  of art.\n\n  ![map of station](map.jpg)\n\n- For quite a long time before the doors are about to close a nice little\n  jingley tune plays.\n- There is excellent signage on the platforms to explain how to get on and off\n  the train. In London we are still very bad at this deceptively simple task.\n\n  ![on off signage on platform](on-off.png)\n  _this is where you stand for this door_\n\n  ![where to wait](where-to-wait.jpg)\n  _this is how you choose where to wait based on the destination stop_\n\n- There is amazing support for wheelchair users. We saw two station staff arrive\n  30s before the train arrived, stand at exact spot for the correct door,\n  and prepare a ramp. When the train arrived the wheelchair user was there ready\n  and the staff helped them off the train without any delay to the train. TFL is\n  getting better but this simple system was amazingly effective too.\n- In Osaka, many of the subway cars had holding handles at three different\n  heights for different heights of passenger.\n- Something that was confusing for us was that some trains turn into trains for\n  other lines when they get to a central station, and you can just remain on the\n  train.\n- Dot matrix indicators have better resolution, colors, etc. We just have\n  orange...\n\n## Was there a peach fanta in that one?\n\nWhile playing real life pokemon to collect our favorite soft drinks we covered\nquite a lot of ground on foot too.\n\n![strangely located vending machine](wat-machine.jpg)\n_vending machine in the middle of a residential area with very few people\naround_\n\nRandom street-based observations follow...\n\n- Some busy streets have directional pavement lanes - sadly they didn't seem to\n  be very effective.\n- Public spaces generally have much greater availability of seating.\n- Some areas have amazing painted manhole covers. Like this one near the instant\n  ramen museum in Osaka.\n\n  ![ramen chicken manhole cover](chick-ramen.jpg)\n\n- Using horns in cars seems to be much less common. Streets are generally quiet\n  in comparison.\n- It's frowned upon to use your phone while walking along. People do it but it's\n  far less common. I found myself trying to stop and use my phone instead.\n- In Osaka, it seems one can cycle anywhere, pavement, road, crowded shopping\n  centre. Watch out for them but they're probably already watching out for you.\n- There are chair lifts in some public spaces to help with steps. This isn't\n  very common but I did notice this in a few different places. I wonder if\n  this would be possible without vandalism here...\n\n## Shinkansen\n\n![strange boi waiting for train](tea.jpg)\n_waiting for the train experience of my life - refreshing drink in hand_\n\n- Each row of train seats lines up with a window. oh. my. gawd. Sure, we have\n  bigger windows but our trains still feel more claustrophobic and dark. I love\n  looking out the window while travelling so this was a win for me.\n- We might have missed something but it seems there's a set rate for the trains\n  for each route. We paid about ¬£120 to go from Tokyo to Osaka and it didn't\n  seem to make any difference how far we looked ahead. I'd take predictability\n  of over the mess we have but I still think there are better ways to price\n  tickets than either system.\n- Shinkansen drivers dress like pilots - seems appropriate for a train who's\n  ancestry is based largely on aeroplane design.\n- All coaches are quiet coaches. If you want to make a call then you can just\n  walk to the vestibule area. People play by the rules too, unlike the fuckers\n  that chat in the quiet coach here...\n- Arrow displays light up to say which side to alight while the train approaches\n  the platform. There is also an announcement when the stop will not last long\n  and encourages people to prepare to exit around 10 mins before the train\n  stops. We stopped for just over 2 mins in Nagoya.\n- Passengers leaving the train take rubbish and bin it on platform. This matches\n  the general rule of taking rubbish home with you. I really appreciated this\n  too.\n- Shinkansen trains have numbers like flights. This number is printed on your\n  ticket. While it's a small touch, it makes it easy to know you're on exactly\n  the right train.\n\n![space for my knees](knees.jpg)\n_look at all that space for my knobbly knees!_\n\n## Please feel free to use a toilet\nI'm not sure what the rest of us were doing while the Japanese were designing\ntheir toilets but we got a long way behind. Many have heated seats and\nsome even play music.\n\n  ![toilet that plays music](poop-tunes.jpg)\n\nRelative to my experience of finding public toilets in the UK, public toilets in\nJapan are _everywhere_. They are mostly very clean and well looked after if not\nmodern. Foam soap (to reduce the amount of waste) is much more common, while\nhand dryers are much less so. I noticed (after some time) that many carry a\nsmall hand towel with them - I've tried to make this a habit of my own too.\nThis means I don't need to use hand dryers (I hate hand dryers but that's\nanother post...).\n\n  ![my bullet train handtowel](handtowel.jpg)\n  _my bulltet train themed hand towel from the Kyoto rail museum_\n\nIn the UK there are 'public' toilets in places like pret. However, locked unless\nyou buy something and get the code on the receipt. In Japan, having a toilet was\nsomething to bring people in. Feel free, don't mind if I do!\n\n  ![shop advertising toilet](free-toilet.jpg)\n\n## Would it be OK to have the bill so we can start waiting for the card machine?\n\nIn Japan they don't seem to have that dance at the end of the meal out. When you\nare ready to go you just walk up, pay, and leave. I appreciated that. Bells on\ntables were also a great idea.\n\nEating while walking around was also not a thing - we learned that people just\nthink you're strange if you do. To help with this, even in little corner shops\n(Family Mart etc) they often have a small table areas and even a sink to wash\nyour hands with after eating (things purchased in their shop).\n\nSome things didn't make much sense to me though... like these pre-fried chips...\n\n![refrigerated fries](fries.jpg)\n\nor this bum beer\n\n![bum advert for beer](bum-beer.jpg)\n\nthis salad\n\n![salad at breakfast](breakfast.jpg)\n\nor this small butter child advert\n\n![butter child on toast ad](butter-child.jpg)\n\n## Is that it?\n\nNope, I'm going back but not sure when.\n\nSome more photos that didn't really fit above...\n\n![vending machine selling handmade bags](bag-vending.jpg)\n_vending machine selling handmade bags_\n\n![an unattended gacha shop that locked automatically at 11pm](gacha.jpg)\n_an unattended gacha shop that locked automatically at 11pm_\n\n![umbrella covering machine](umbrella-cover.jpg)\n_machine to cover umbreallas in a plastic bag to stop drips_\n","date":"Oct 26 2019","id":139,"title":"'Please feel free to use a toilet' \u0026 other things I liked about Japan\n","type":"blog post","url":"/posts/2019-10-26-please-feel-free-to-use-a-toilet/"},{"body":"\nI recently got a faulty 2016 12\" Macbook refunded by making an EU consumer law\nclaim. I've done this [once\nbefore](/posts/2016-06-24-getting-a-full-refund-for-a-faulty-macbook-under-uk-consumer-law/)\nin 2016; now more familiar with the process I thought I'd write up round two.\n\n## TL;DR\n\nThis isn't a short process so here's the summary.\n\nIf you have a Macbook on [this list](https://support.apple.com/en-gb/keyboard-service-program-for-mac-notebooks)\nand you have a faulty keyboard, you can get a refund under EU consumer law.\n\nYou must have the original invoice. Order confirmations are not good enough,\ninvoices look like this and are sent as a PDF once an order is completed.\n\n![my invoice](invoice.png)\n\n1. Get it verified as having a faulty keyboard and get the laptop back\n   **unrepaired**.  The company needs to be an ASP (Apple Service Provider), it\n   can even be Apple Retail *iif* you bought your laptop from a non-Apple\n   company - it cannot be where you bought it from.\n2. Contact Apple support if you purchased online or visit a store if you bought\n   it in _any_ store (Apple Retail counts as the same entity here). Ask to start\n   the EU claims process _for refund_ of a device under a repair program.\n3. Follow their guidance but, send them the laptop.\n4. Give bank deets\n5. Profit?\n\n## My timeline\n\nThis is the exact sequence of steps I completed.\n\n**14th June 2016**\n\nBought my Macbook with the money from [my last EU consumer law claim](/posts/2016-06-24-getting-a-full-refund-for-a-faulty-macbook-under-uk-consumer-law/) from the Apple Online Store.\n\n**Sometime in 2017**\n\nHad the keyboard replaced. (Note, this is not an essential step to get a refund)\n\n**11th August 2019**\n\nLaptop deposited at [Stormfront](https://www.stormfront.co.uk/) Inverness.\n\nJust ask for a diagnosis only, they charge some money for this service - I paid\n¬£50. I think Apple can refund you for this too but I forgot to ask for it -\noops.\n\nMake it really clear the laptop isn't to be repaired and that you are buying a\nsheet of paper that says the device has a faulty keyboard - no more. This is\nquite an unusual request so make it very clear.\n\n**15th August**\n\nI had all the paperwork describing the fault and my laptop returned. I had\noriginally hoped that Apple could work with Stormfront to have the laptop\ncollected but they needed the charger so it was easier to have it returned.\n\n**20th August**\n\nBegan the EU claims procedure. Provided my invoice and told I'd hear back within\na week.\n\n**22nd August**\n\nI have another call and provide the repair notes. They called at 2am but I\ndidn't answer so we spoke later in the day instead.\n\nI had to speak to customer relations before they understood that this was to be\na refund rather than a repair. When you get to customer relations you know\nyou're in the right place and these folks are actually pretty great in my\nexperience. (Customer support != customer relations)\n\n**23rd August**\n\nTalked about setting up of TNT collection\n\n**28th August**\n\nTNT collection set up, advised they'd call to arrange the collection.\n\n**30th August**\n\nTNT called, collection set for 6th of September\n\n**6th September**\n\nTNT arrived to collect but had no proof of collection. This meant we were unable\nto complete the collection on this day.\n\nOn a call to TNT, we made arrangements to deposit the packaged laptop at the\ndepot.\n\n**10th September**\n\nLaptop dispatched and proof of sending given. This was sent to Apple via email\nto customer relations.\n\n**12th September**\n\nLacey (my CR rep) called to take my bank details (while I happened to be in Japan!).\n\nI provided my Revolut details via email.\n\n**16th September**\n\nLacey confirmed receipt of the bank details.\n\nApple need to have an IBAN number. This is not something available at Monzo\nwhere I have my current account. Sadly, Revolut dropped the ball, see below...\n\n**24th September**\n\nLaura called while Lacey was on holiday. Payment sent for processing.\n\n**30th September**\n\nLacey called to check if the money had arrived. It hadn't and she was to follow\nup with the finance team.\n\n**2nd October**\n\nAnother CR rep called to confirm my bank details.\n\n**10th October**\n\nLacey called to say the payment had been reprocessed. Advised to wait 10\nbusiness days.\n\n**15th October**\n\nLacey called to check if the payment had been received. It sadly had not.\n\n**21st October**\n\n10 days were up. Lacey called to check, action to follow up with accounts.\n\n**23rd October**\n\nFinance acknowledge the request to look into it. Expected to respond within 3\ndays.\n\nI asked for a goodwill gesture due to the delay and was refused one.\n\n**25th October**\n\nLacey called me to confirm that the payment had been rejected by Revolut.\nRevolut never told me anything...\n\nI sent my TransferWise details instead.\n\n**29th October**\n\nCall from Lacey to confirm that the new details had been submitted. Scheduled a\ncall back for the 12th to check.\n\n**31st October**\n\nReceived the payment in my TransferWise account. Woop. Just in time for what\nmight have been Brexit - which might have impacted my rights to refund here?\n(who knows...)\n\n![refund screenshot](refund.jpg)\n\nEmailed Lacey to request the case be closed, say thanks, good bye etc.\n\n**2nd November**\n\nCompleted Apple feedback form suggesting better communication between customer\nrelations - as well as good feedback for Lacey and the process.\n\n## Lessons Learned\n\nDo\n\n* ...keep notes of call dates and what was agreed\n\nDon't\n\n* ...try and get the repair centre to liaise with Apple when sending back the\n  laptop (didn't realise the charger was required)\n* ...use Revolut\n* ...buy a butterfly keyboard Macbook and expect it to last more than 18 months\n\n## Phew...\n\nThis was a pretty painful process but mostly better than the last time. I had a\nvery good experience with the customer relations team. Having a single rep for\nyour case is a huge help.\n\nI bought myself an iPhone with the proceeds this time. Phones are expensive\nnow...\n\nPlease contact me if you would like any more details, reference numbers etc.\n[me@charlieegan3.com](mailto:me@charlieegan3.com)\n","date":"Nov 2 2019","id":140,"title":"Refunding my faulty Macbook with another EU consumer law claim","type":"blog post","url":"/posts/2019-11-02-refunding-another-macbook/"},{"body":"\nThis year at work I've been building out a platform on Google's managed\nKubernetes offering GKE. While GKE makes it easier to automate the provisioning\nand management of clusters - managing updates of anything more than a handful of\nclusters can quickly becomes painful without the right tools.\n\nThis post outlines some features I've either come to value or will prioritize in\nfuture when faced with cluster sprawl. I limit these features to the practical\nmanagement of clusters updates so as to keep the scope of the post small.\nMonitoring \u0026 alerting would be another interesting list but you won't find that\nhere...\n\n## Where did all these clusters come from anyway?\n\nIt's easy to underestimate the number of clusters you'll end up managing. Sure\nyou'll maybe have one for each environment. Usually that'd be three at a\nminimum, for me this number was five.\n\nThis isn't a post about managing five clusters though. There are lots of\ndimensions that have a multiplicative effect. These will be different for each\nof us. My other dimensions were separate hardware for data processing reasons;\nlong term migrations to and from different cloud accounts and deployments in\ndifferent regions.\n\nThe obvious one is different teams or groups. However, I didn't find myself in\nthis situation.\n\nAll in all, in the end I found myself managing a fleet of around 30 clusters.\n\nI'm of the view that the best way to create and update GKE clusters right now is\nusing Terraform. Where I work at Jetstack we have a [Terraform\nmodule](https://github.com/jetstack/terraform-google-gke-cluster) for this job\nif you've not started yet. This post will make some references to Terraform use.\n\n## Multi-dimensional cluster deployments\n\nWith my environments multiplied by my other dimensions I found myself with N\ndevelopment, N stage and N production (etc. etc.) clusters. This quickly grew to\na number that made it impossible to complete a master+node pool upgrade within a\nworking day when deploying updates in series.\n\nI want a means of controlling which clusters can be updated in parallel and\nwhich can't. For example, I might want to roll out all my dev clusters at the\nsame time.\n\nMany continuous deployment tooling is not set up in this way and as more cluster\nconfigs land it can quickly get out of hand if they're run in series. I suppose\ncluster upgrades are likely to be the longest deployments in the whole business\n(and perhaps by some margin too).\n\nThis raises the question - are synchronous pipelines really the best way to roll\nout cluster updates? In the long term, with things like Cluster API perhaps\nnot...\n\n## One plan to rule them all\n\nWhen you're sitting in front of a multi-hour pipeline run, it's nice to know\nwhat's coming up next. With Terraform version prefixes or GKE alias versions\nit's possible to find that you're actually about to roll out a minor update to\nall your clusters.\n\nI guess this comes down in part to one's use of Auto-upgrade. I was not using\nthis feature in this case.\n\nTerraform `plan` you might say. Perhaps, but I think there's a benefit to\nkeeping the Terraform stacks small to reduce the blast radius and enable the\nabove feature of concurrent cluster deployments.\n\nIn the end this was a script for me. It parsed our cluster manifests and made\ncalls against the GKE APIs to check the master and node versions against the\nvalues we had in config. This allowed me to warn of inconsistencies from missed\nruns and any unexpected updates.\n\nI suppose that ideally these might be better implemented as a Prometheus\nexporter that alerts when config falls behind the available GKE version for that\nprefix in that location.\n\n## Terraform `version_prefix` and `timeouts`\n\nThere's a data resource in Terraform called `google_container_engine_versions`.\nThis enables the selection of cluster versions from the available versions in\nthat location in a predictable manner.\n\nThis can be used to give a similar behaviour to the [GKE alias\nversion](https://cloud.google.com/kubernetes-engine/versioning-and-upgrades#specifying_cluster_version)\nfeature - however that can lead to version mismatches when deploying in\ndifferent regions due to differences in availability. With this feature it's\npossible to use a prefix (read GKE alias) but also drop down to a fixed version\nif required. There's an example of this in my [personal infrastructure\nrepo](https://github.com/charlieegan3/infrastructure/blob/3fa0a3da49ba06b5f63edf5fa62e66ba0acd0436/gcp/charlieegan3-cluster/cluster.tf#L1-L13).\n\n[`timeouts`](https://www.terraform.io/docs/configuration/resources.html#operation-timeouts)\nare probably part of your config already but they deserve a mention. Set these\nallowing for 30 mins for a regional master upgrade and 12 mins for a node\nupgrade if you want to be safe. Those are around the maximums I've seen this\nyear anyway.\n\n## Pesky PDBs\n\n`PodDisruptionBudget`s can easily be misconfigured by tenants and can delay node\nupgrades to their maximum deadlines. Something I'd to play with is an OPA policy\nto block PDB resources where minAvailable is set to an inappropriate value for\nthe desired replicas of that deployment.\n\n\u003chr/\u003e\n\nThat's my half wish list, half brain dump of GKE cluster upgrade tools.\n\nOnce feature that would make a lot of this less painful would be be some dials\nto tune on the rate at which GKE cycles nodes. Something like a deployment with\nmaxSurge and maxUnavailable would be nice but on a basic level a feature\ncomparable to AWS CodeDeploy's AllAtOnce would be good to have for some pre\nproduction environments. GKE Product team if you're listening...\n\nI was working with relatively small clusters of big nodes, thoughts and prayers\nfor those with more nodes... hope you found an entertaining book to read...\n","date":"Nov 6 2019","id":141,"title":"What to do with all these cluster updates...","type":"blog post","url":"/posts/2019-11-06-all-these-clusters/"},{"body":"\nI've been working on fun project recently for the 'Christmas Show \u0026 Tell' event\nat the London Computation Club. At club show and tells there are imaginary\npoints available for matching the theme. I like imaginary points and decided to\nmake a system that would validate drawings of Christmas trees using\n[OPA](https://www.openpolicyagent.org/) \u0026\n[Rego](https://www.openpolicyagent.org/docs/latest/policy-language/).\n\nIf you're interested in playing with the tool I demoed, you can find it\n[here](https://xmas-trees.charlieegan3.com/). This post is more about the Rego I\nfound myself writing as part of the project.\n\n## OPA 'handlers' \u0026 gathering it all up\n\nQuite some time ago I\n[learned](https://github.com/open-policy-agent/opa/issues/1818) about the\n(apparently little known) `v0` [Data\nAPI](https://www.openpolicyagent.org/docs/latest/rest-api/#get-a-document-webhook)\non the OPA server. This makes it easy to map different requests to different\npolicies on the same OPA server instance. Even though I have only one policy to\nvalidate my trees - I used this method to run my `main.rego` policy\n[here](https://github.com/charlieegan3/policing-christmas-trees/blob/b90a6cfe064defebaec5aee89b48f11aed78a044/server/main.rego).\n\nThe most interesting part of this policy is how the messages from my various\n`deny` policies are aggregated.\n\n```go\noutput = {m | m := deny[_]}\n```\n\nThere's a few things going on here.\n\n- I see `deny[_]` as finding all solutions to `deny` (where `deny` is a policy\n  that returns a message if the input is invalid) - basically gathering all the\n  messages for the input.\n- `{m | m := deny[_]}` is gathering up all these messages using a\n  [comprehension](https://www.openpolicyagent.org/docs/latest/policy-language/#comprehensions).\n  Crucially, these messages are aggregated into a Set not a list (e.g. `[m | m\n  := deny[_]]`) - this is important because sometimes there are many identical\n  reasons an input can be invalid for a given input.\n\nI [asked](https://stackoverflow.com/questions/58895492/limit-opa-rego-to-a-single-rule-solution)\nif it were possible to halt the execution once a solution had been found.\nThinking about the domain for which OPA was built, it's perhaps\nunsurprising that it is not.\n\n## A humble membership test\n\nI'm not including\n[this](https://github.com/charlieegan3/policing-christmas-trees/blob/dda4a8f92e9c09781782a11610769bc85ccfd596/server/topper.rego#L4-L11)\nbecause I think it's especially wow but rather as an\nexample of something basic I'd never needed to do before in the policies I'd\nwritten at work.\n\n```go\nallowed_toppers := {\"angel\", \"star\"}\nallowed_toppers \u0026 {input} == set()\nmessage :=\n  sprintf(\"topper '%s' not in list: %s\", [input, concat(\",\", allowed_toppers)])\n```\n\nThis allows me to validate that the input is within some known good static set\nof strings. We assert that if the intersection of the set of the input and the\nknown set is empty (`set()`) then we must bind to the error message.\n\nIn this particular it's quite easy to explain why the input is invalid in the\nmessage too.\n\n## Universal Quantifications\n\nI ended up reaching for Universal Quantifications twice in this project -\nsomething I'd not used before in Rego.\n\nWhen validating [baubles](https://github.com/charlieegan3/policing-christmas-trees/blob/b90a6cfe064defebaec5aee89b48f11aed78a044/server/baubles.rego#L9-L12)\nI found that I needed to validate that it was placed on a single point on the\ntree outline. This lead me to create a simple test to ensure that there as\nmatching point.\n\nYou might be thinking that this sounds more like an _Existential\nQuantification_. All my policies are written as `deny` - I'm not sure why but\nthis is how I'd always done it at work and it seemed to make sense to continue.\n\nWhat it does mean is that to assert that there exists a point on the outline,\nyou need to describe the error case as being when the count of all matching\npoints is zero. It's kinda backwards.\n\n```go\ncount({point |\n\tpoint := input.outline[_]\n\tpoint == bauble\n}) == 0\n```\n\nI realise now this can be condensed some to the form below, but I think the\noriginal is more readable.\n\n```go\ncount({point |\n\tinput.outline[point] == bauble\n}) == 0\n```\n\n\nI needed to do a similar thing for [tinsels](https://github.com/charlieegan3/policing-christmas-trees/blob/b90a6cfe064defebaec5aee89b48f11aed78a044/server/tinsels.rego#L21).\nTinsel start and end points must be on a segment of the tree outline. Here the\nchecking logic is more involved so I've written it up in the next section. The\nidea is the same though really where I'm using Universal Quantification to\nvalidate a solution exists.\n\n## why-eeq-wals-em-ex-plus-see\n\n`y = mx + c` was also something I'd not needed to use when validating Kubernetes\nYAMLs.\n\nThe reason tinsels were harder than baubles is that they can lie between two\npoints on the outline (but they _must_ be on the outline). The only way I knew\ndo this was with the [line\nequation](https://github.com/charlieegan3/policing-christmas-trees/blob/b90a6cfe064defebaec5aee89b48f11aed78a044/server/tinsels.rego#L34-L42)\nand [bounds\nchecking](https://github.com/charlieegan3/policing-christmas-trees/blob/b90a6cfe064defebaec5aee89b48f11aed78a044/server/tinsels.rego#L44-L52)\non the coordinate values\nso that's what I did - in Rego ofc!\n\nFirst I calculate the gradient and intercept for the outline segment:\n\n```go\ngradient := (outline_point_a[1] - outline_point_b[1]) /\n\t\t\t(outline_point_a[0] - outline_point_b[0])\n\ny_intercept := -1*((-1*outline_point_a[1]) + (gradient * outline_point_a[0]))\n```\n\nThen I can plug the point for the tinsel into this and find out if we're good.\n\n```go\nexpected_y := gradient * point[0] + y_intercept\nexpected_y == point[1]\n```\n\n---\n\nSo there you have it. Some things I did in Rego in the name of festive fun.\n\nIn the new year I'm going to be spending my time working on Jetstack's\n[_Preflight_](https://github.com/jetstack/preflight), an open source tool for\ninfrastructure policy checking built on Rego. Hopefully that will be fun too.\n","date":"Dec 5 2019","id":142,"title":"Fun things I did in Rego while validating Christmas Trees","type":"blog post","url":"/posts/2019-12-05-rego-fun/"},{"body":"\nSince starting at Jetstack I've spent much time building a personal Kubernetes\ncluster to run my various side projects. I no longer run anything on Heroku,\nLamdba, App Engine or Netlify. With Ingress, Cronjobs and Knative I'm able to\nreplicate all the features of these platforms I valued on Kubernetes.\n\nHaving a single platform has really reduced the time involved in deploying side\nprojects. I'm a big fan of side projects being more than a GitHub repo - part of\nthe fun is making a usable thing or something I can use. Scale to zero really\nhelps pack things in here - something I've been making much more use of recently\n([borked](https://github.com/charlieegan3/borked),\n[mycriticmatch](https://github.com/charlieegan3/mycriticmatch),\n[rssmerge](https://github.com/charlieegan3/rssmerge),\n[xmas-trees](https://github.com/charlieegan3/policing-christmas-trees) and\nvarious other personal tools run on Knative and scale to zero).\n\nThis post was mostly just to introduce this repo:\n[charlieegan3/infrastructure](https://github.com/charlieegan3/infrastructure)\n\nI had to spend quite a bit of time getting all the secrets out of the repo. I\ntook the opportunity to deploy [vault](https://www.vaultproject.io/)¬†and [GKE\nworkload\nidentity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity)\nrather than use something like [gitcrypt](https://github.com/AGWA/git-crypt) or\n[sealed secrets](https://github.com/bitnami-labs/sealed-secrets). In the end\nthis was more of a tongue twister than anything particularly good but I do\nappreciate how transparent the public repo can be.\n\nIn the repo there are two 'projects'; GCP \u0026 K8s. GCP is a relatively simple\nTerraform stack to deploy a GKE cluster, DNS configuration, object storage\nresources, KMS etc. K8s is a collection of folders, one for each namespace in\nthe cluster (which map approximately to side projects). Most of these are raw\nYAMLs but there are also some of my (signature?) Docker Helm template projects\ntoo.\n\nI was keen to make this public so it was easier to share my config more easily\nwith others - in particular with customers while working on site.\n","date":"Dec 8 2019","id":143,"title":"Introducing charlieegan3/infrastructure","type":"blog post","url":"/posts/2019-12-08-charlieegan3-infra/"},{"body":"\n**Update:** I've written a follow up post [here](/posts/2020-08-31-rego-semver-contribution/) explaining how I later added this functionality to Rego itself.\n\n![versions.png](versions.png)\n\nI recently found myself faced with the task of writing [OPA](https://www.openpolicyagent.org/) policies that involved comparing [Semantic Versions](https://semver.org/). It seemed like an interesting challenge, and something more useful than [validating Christmas trees](https://www.notion.so/posts/2019-12-05-rego-fun/)...\n\nThis was the end goal, a function that was able to compare two versions:\n\n```go\nis_greater_or_equal(\"1.0.0\", \"0.1.0\")\n// =\u003e true\nis_greater_or_equal(\"1.0.0\", \"2.0.0\")\n// =\u003e false\nis_greater_or_equal(\"1.0.0\", \"1\")\n// =\u003e true\n```\n\nWhile some of the functions are refined from their original implementations, the\nprocess went something like this.\n\n## How can we parse versions and represent them?\n\nFirst, I decided on an internal representation for a Semantic Version, I settled\non the following object - for better or worse.\n\n```go\n{\n  \"major\": int,\n  \"minor\": int,\n  \"patch\": int,\n}\n```\n\nNext, I needed a means of parsing the version strings and getting back versions\nin my representation. So I needed the following to work:\n\n```go\nparse_version_string(\"1.0.0\")\n// =\u003e { \"major\": 1, \"minor\": 0, \"patch\": 0 }\nparse_version_string(\"2.3.4\")\n// =\u003e { \"major\": 2, \"minor\": 3, \"patch\": 4 }\n\n// but also...\nparse_version_string(\"1.0\")\n// =\u003e { \"major\": 1, \"minor\": 0, \"patch\": 0 }\nparse_version_string(\"1\")\n// =\u003e { \"major\": 1, \"minor\": 0, \"patch\": 0 }\n\n// and also...\nparse_version_string(\"v1.0.0\")\n// =\u003e { \"major\": 1, \"minor\": 0, \"patch\": 0 }\n```\n\nTo implement `parse_version_string`, it seemed to make sense to start by\nsplitting on '.' - then all I needed was a means of creating a new version from\nthe split data.\n\n```go\nparse_version_string(version_string) = version {\n\tcomponents := split(version_string, \".\")\n\tversion := new_version_from_components(components, count(components))\n}\n```\n\nUsing multiple function heads (or whatever these are called in Rego) seemed like\nan ok way to handle the different lengths. I called *major*, *minor* and *patch*\n‚Äòcomponents‚Äô *sigh*, nice and generic...\n\n```go\nnew_version_from_components(components, 1) = version {\n\tversion := new_version(components[0], 0, 0)\n}\nnew_version_from_components(components, 2) = version {\n\tversion := new_version(components[0], components[1], 0)\n}\nnew_version_from_components(components, 3) = version {\n\tversion := new_version(components[0], components[1], components[2])\n}\nnew_version(major, minor, patch) = version {\n\tversion := {\n\t\t\t\"major\": to_number(trim_prefix(sprintf(\"%v\", [major]), \"v\")),\n\t\t\t\"minor\": to_number(minor),\n\t\t\t\"patch\": to_number(patch)\n\t\t}\n}\n```\n\nNote that I also wanted `new_version` to work with strings or numbers (this made my tests easier to write and handling the 'v' prefix possible).\n\nSo now I have a means of creating versions in my internal representation.\n\n## What does 'greater' mean and how do we define it?\n\nWhat does ‚Äògreater‚Äô mean in SemVer? I didn‚Äôt read the spec, so this might be\nmissing something, but my definition was any of the following:\n\n- major version is greater\n- major is equal, and minor version is greater\n- major is equal, minor is equal, patch is greater\n\nNow I needed to find a means of writing this in Rego. Fundamentally, I need to\nmake comparisons between the parts of the version. I created two functions like\nthis which operate on two versions:\n\n```go\nis_key_greater(key, a, b) = result {\n\tresult := a[key] \u003e b[key]\n}\nis_key_equal(key, a, b) = result {\n\tresult := a[key] == b[key]\n}\n```\n\nThese will return the integer comparison of the components at the given ‚Äòkey‚Äô\n(major, minor, or patch).\n\nWith these building blocks I can implement my definition for ‚Äòis greater‚Äô:\n\n```go\nis_greater(a, b) = result {\n\tresult := {\n\t\t{ is_key_greater(\"major\", a, b) },\n\t\t{ is_key_equal(\"major\", a, b), is_key_greater(\"minor\", a, b) },\n\t\t{ is_key_equal(\"major\", a, b), is_key_equal(\"minor\", a, b), is_key_greater(\"patch\", a, b) },\n\t} \u0026 { { true } } == { { true } }\n}\n```\n\nThis reads as: ‚Äúbind the result to true if the set of conditions contains one\nthat is true; where a condition is true if all the sub conditions are also\ntrue‚Äù. Sub conditions in this case being the comparisons between the version\ncomponents.\n\nIn order to achieve world domination with `is_greater_or_equal` we also need to\nhave a definition for version equality - this one is more straight forward:\n\n```go\nis_equal(a, b) = result {\n\tkeys := [\"major\", \"minor\", \"patch\"]\n\tresult := { r | key := keys[_]; r := is_key_equal(key, a, b) } == { true }\n}\n```\n\nI read this as, ‚Äúfor all version components (with keys master, minor, patch),\ncheck the components in both versions are strictly equal‚Äù.\n\nWith these two, the implementation of `is_greater_or_equal` is trivial:\n\n```go\nis_greater_or_equal(a, b) = result {\n\tresult := {\n\t\tis_greater(a, b),\n\t\tis_equal(a, b),\n\t} \u0026 { true } == { true }\n}\n```\n\nI read this as: ‚Äúany element in the set { is_greater(a, b), is_equal(a, b) } is\ntrue‚Äù. We test this using set intersection with `{ true }`.\n\n## This is a mess, you should go back to school!\n\nI‚Äôve been writing Rego policies for a while now, I find them fun - a little like a puzzle or something. However, I have no background in logic programming and sometimes wonder if there is a better way to do things. I still regularly find myself mapping from imperative thinking to my declarative policies.\n\nYou can review all the code in [this gist](https://gist.github.com/charlieegan3/76dbec05c65164ac98dfec74b1381c5a). Feel free to comment there if you have questions or suggestions. I‚Äôm also on [Twitter](https://twitter.com/charlieegan3).\n","date":"May 8 2020","id":144,"title":"SemVer comparisons with OPA\n","type":"blog post","url":"/posts/2020-05-08-semver-comparisons-with-opa/"},{"body":"\nEarlier this week I found myself migrating my contact list to Notion. I decided\nto do this because I was lacking the following features in Outlook to\neffectively manage my contacts:\n\n- Syncing delays and inconsistencies with Exchange for contacts\n- Not being able to have nicknames work in the way that I want (e.g. only on my phone / WhatsApp)\n- A means of archiving contacts, without deleting them\n- ...and the straw that broke the camels back: being able to store birthdays and have a birthday calendar (this is something that is meant to be supported but it just doesn't work at all)\n\nI've had a lot of fun in putting Notion to work for me over the last 6 months\nand thought it was worth a shot for this too. A few years ago I was set on\nself-hosting things, now I think I'm more interested in trying to make sure that\nthere are paths between my cloud accounts for my data and that the data works\nfor me as I need it to. I think this is a good example of that 'philosophy'.\n\n## What counted as minimum and viable?\n\nThe requirements were pretty simple. I need to have a contact list in Notion\nthat tracked birthdays, supported nicknames and archiving. This needed to be\nsimple to sync to my phone - even if it were to be a manual task (I don't update\nmy contacts much and can still update in Notion from anywhere).\n\nI decided to use iCloud to get the contacts to my phone. On\n[iCloud.com](http://icloud.com/) it's possible to upload a vcard with many\ncontacts. It's also possible to delete all your contacts easily when uploading a\nnew list (not possible on Outlook). If I ever find myself in the situation where\nI have an Android phone again I'll find some way to go via Google or similar.\n\nThe other nice thing about iCloud and vcards is that they support base64 encoded\nJPEGs for profile pictures. This means I can manage pictures in Notion too and\nformat when syncing.\n\n## From Notion, to disk, to iCloud - with the help of some Ruby\n\nWhile I was happy for the process to be manually run, I wanted to automate what\nwas easy. This is what I ended up with.\n\nI have a database in Notion that I can export (with subpages to get pictures)\n\n![export.png](export.png)\n\nThe table has the following properties:\n\n![properties.png](properties.png)\n\nThe computed fields are included in the export. `Display Name` is set to the\nnickname if present and otherwise both First and Last names. This is the value\nthat I want to use in the contacts app on my phone.\n\n```ruby\nif(\n  prop(\"Nickname\") != \"\",\n  prop(\"Nickname\"),\n  concat(prop(\"First Name\"), \" \", prop(\"Last Name\"))\n)\n```\n\n`Days Until Birthday` is set by the following Notion formula and is used to sort\non a different birthday view of the data.\n\n```ruby\ndateBetween(\n  dateAdd(\n    prop(\"Birthday\"),\n    dateBetween(\n      now(),\n      prop(\"Birthday\"),\n      \"years\"\n    ) + 1,\n    \"years\"\n  ),\n  now(),\n  \"days\"\n)\n```\n\nWith the data in this format, it was relatively simple to write a script that converted the exported Notion data (CSV and image files) into a single vcard file.\n\n```ruby\n#!/usr/bin/env ruby\n\nrequire \"csv\"\nrequire \"base64\"\nrequire \"rmagick\"\nrequire \"vcardigan\"\n\ninput = Dir.glob(\"Index*.csv\")\nfail \"uncertain input, expected single CSV file\" if input.size != 1\ntable = CSV.parse(File.read(input[0]), headers: true)\n\nvcards = []\ntable.each do |c|\n  # if the contact has been archived, skip during export\n  next if c[\"Archived\"] == \"Yes\"\n\n  name = c[\"Display Name\"]\n\n  vcard = VCardigan.create\n  vcard.name(*name.split(\" \").reverse)\n  vcard.fullname name\n\n  # I don't really use 'work' and 'home' emails much, so I just\n  # have a plain email for each and store them as a single field\n  c[\"Emails\"].to_s.split(\"\\n\").map(\u0026:chomp).each do |email|\n    vcard.email email\n  end\n\n  unless c[\"Home Phone\"].nil?\n    vcard['item0'].add('tel', c[\"Home Phone\"])\n    vcard['item0'].add('x-ablabel', 'Home')\n  end\n\n  unless c[\"Mobile Phone\"].nil?\n    vcard['item1'].add('tel', c[\"Mobile Phone\"])\n    vcard['item1'].add('x-ablabel', 'Mobile')\n  end\n\n  unless c[\"Note\"].nil?\n    vcard.note = c[\"Note\"].gsub(\"\\n\", '\\n')\n  end\n\n  unless c[\"Company\"].nil?\n    vcard.org = c[\"Company\"].to_s\n  end\n\n  # pictures need to be quite small, if they're too large icloud\n  # will fail to import that contact.\n  unless c[\"Profile Picture\"].nil?\n    path = c[\"Profile Picture\"].gsub(\"%20\", \" \")\n    image = Magick::Image.read(path).first\n    image.change_geometry!(\"200x200\") { |cols, rows, img|\n        newimg = img.resize(cols, rows)\n        newimg.write(path + \".small\")\n    }\n    b64 = Base64.strict_encode64(File.read(path + \".small\"))\n    vcard.photo b64, :encoding =\u003e \"b\", :type =\u003e \"JPEG\"\n  end\n\n  vcards \u003c\u003c vcard.to_s\nend\n\n# contacts.vcard is the file to upload to icloud\nFile.write(\"contacts.vcard\", vcards.join)\nputs \"done\"\n```\n\nOnce that's run, I open [iCloud.com](http://icloud.com/), delete all the contacts\n(ctrl+A, delete) and upload the file. It's a very quick process and they're on\nmy phone instantly.\n\n## What was this all about really and why did you write a post about it?\n\nIt's not the most exciting topic but it's something that's made me happy this\nweek. I feel like scratched an itch and it paid off with minimal effort. I think\nI spent longer writing this post than the script.\n","date":"Jun 6 2020","id":145,"title":"Building a contact list in Notion that works for me\n","type":"blog post","url":"/posts/2020-06-06-notion-contacts-list/"},{"body":"\n- This week after work I‚Äôve been doing a bit of side project house keeping. I‚Äôve mostly been automating backups of my Notion and Dropbox data using GitHub actions. I‚Äôve also automated my [contact export](https://charlieegan3.com/posts/2020-06-06-notion-contacts-list/) from Notion. While we wait for Notion to release a public API, the private API seems to be relatively easy to use for such tasks. GitHub actions isn‚Äôt perfect but it‚Äôs free and it sends me emails when things break.\n- My ‚Äòmain‚Äô side project at the moment is some kind of ‚Äòqueryable overlay‚Äô populated on data in my Dropbox. The idea is to be able to search for photos based on their location and other EXIF data - as well as perhaps eventually faces and other smarts. I‚Äôm interested to know what my highest altitude photo is, for example.\n- At [work](https://preflight.jetstack.io/) I‚Äôve been trying to understand how to represent severity of warnings raised about clusters and things running in them. There are many different dimensions: security, availability considerations; how confident we are that we‚Äôre right; and how much impact the worst case scenario has.\n- I also had some really valuable feedback from a senior colleague on a design proposal I'd written. I've never really had feedback on that before and found it to be really helpful. I'm planning to write the notes as a blog post.\n- AFK, I‚Äôve been for two swims at the [Parliament Hill Lido](http://parliamenthilllido.org/). Pleasantly surprised at both how close and how clean it is. I am thinking of buying a [wetsuit](https://uk.roka.com/collections/mens-wetsuits/products/mens-maverick-comp-ii-wetsuit?variant=13603914743919) however as I can‚Äôt really stay in there for more than 20 mins safely. My feeling is that it‚Äôll only get colder...\n- Last weekend someone tried to snatch my phone so I bought a cheaper backup iPhone 5SE. I was really impressed by the nearby device [quick start feature](https://support.apple.com/en-gb/HT210216). I also bought what is effectively a [¬£14 keyring](https://www.quadlockcase.co.uk/collections/accessories/products/phone-ring-stand?variant=31349703573619) for my Quad Lock case.\n- Back at work, the video calling arms race continues and I got sucked into it this week after [someone](https://twitter.com/_jsfuentes?s=21) sent me a link to a ¬£6 capture card on eBay...\n\n![Screenshot_from_2020-08-31_18-11-35.png](Screenshot_from_2020-08-31_18-11-35.png)\n\n- I‚Äôve booked my trains home for Christmas. Booked to visit my granny for the first time this year. And tried to book a trip to Edinburgh but LNER said not yet.\n- We‚Äôve been playing Sea of Thieves. We all find the controls infinitely frustrating and still can‚Äôt seem to beat any other ships with real players in them. That said, we still seem to enjoy it, firing people out of canons and drinking grog is fun too.\n    ![Screenshot_from_2020-08-31_18-12-11.png](Screenshot_from_2020-08-31_18-12-11.png)\n- On Sunday, I was meant to have a run+bike brick session but ended up [running with Luke](https://www.strava.com/activities/3987602822) so decided to [double](https://www.strava.com/activities/3985526181) [up](https://www.strava.com/activities/3985795212) on the bike while I waited ü•µ\n","date":"Aug 30 2020","id":146,"title":"Week Notes: 1\n","type":"blog post","url":"/posts/2020-08-30-week-notes-1/"},{"body":"\n![Screenshot_from_2020-08-31_22-09-36.png](Screenshot_from_2020-08-31_22-09-36.png)\n\nThis is a post about a recent contribution I made to the OPA project. OPA (Open Policy Agent) aims to provide a standard means of authorization in the form of a policy server with a domain specific language called Rego. The project has [good documentation](https://www.openpolicyagent.org/docs/latest/) where you can read more about it.\n\n## Why do this in the first place?\n\nBack in May I [wrote a post](/posts/2020-05-08-semver-comparisons-with-opa/) about an experiment I'd done to see if it was possible to compare [Semantic Versions](https://semver.org/) in pure Rego. The experiment was a 'success' in that this simple task was possible - however, what I'd written wasn't particularly pretty.\n\nI did this mostly out of interest, however I got the idea that from something I was working on with [Preflight](http://preflight.jetstack.io) at work at the time.\n\n## Why not publish a Rego package instead?\n\nRego \u0026 OPA don't have the concept of a third party library or package. There is no leftpad, database wrapper, or API client that you can install and have some code available in your Rego rules. I wonder if this will come at some point, perhaps there's a good reason other than the project still being relatively young.\n\nInstead, it would seem that the OPA community are taking a different route and including more features in the Rego language itself - which is where my changes come in.\n\n## What was added?\n\nExactly what changed can be seen on [my PR here](https://github.com/open-policy-agent/opa/pull/2538).\n\nIn summary, I added two functions as built-ins to Rego. Built-ins are functions that you can call from anywhere in your Rego policies without needing to import or install. You can use any built-in that is in that version of OPA.\n\n`semver.is_valid` allows those writing Rego policies to check that a value is a valid SemVer string. `[semver.compare](http://semver.compare)` allows the comparison of two valid SemVers.\n\nThis adds a subset of the functionality that the [coreos semver golang package](https://github.com/coreos/go-semver) implements - and it vendors this under the hood. Rego built-ins are just go functions underneath.\n\n## What's interesting about this PR?\n\nTo most people, not a lot. The thing about this process that was most interesting to me was the decision to vendor the required functions from the coreos package to keep the dependencies on in the project low.\n\nPatrick East, one of the OPA project's maintainers, left a [comment here](https://github.com/open-policy-agent/opa/pull/2538#pullrequestreview-448422711) explaining the reasoning.\n\nI've never worked on a project that needed to be included in other projects as a library, this is hasn't ever really been something I've thought a great deal about as I'm usually working at a 'leaf' in the 'library dependency graph'.\n\nSo in the PR you can see we have just [copied in the package](https://github.com/open-policy-agent/opa/tree/22d4efee4d795c6a4c9345465a9771bbfbad22c9/internal/semver), and removed all but the functions we need.\n\nThis was interesting to me anyway.\n\n## Is this being used?\n\nYes! I had hoped to use this to compare versions of third party installed software (prometheus, ingress-nginx etc.) but in the end, the first place this has been used is to check the version of Preflight's own agent. I think that we can use it for more in the future though and that it's a valid addition to the language.\n\n## Final Thoughts\n\nI started this on a 'dev-day' at work but finished it on my own time. At Jetstack we have 10 dev-days a year to use on things like this, learning and so on. I find them to be really valuable and this one was satisfying too since it represented my first 'real' contribution to a project that I've got a lot of value from.\n","date":"Aug 31 2020","id":147,"title":"Teaching Rego to compare Semantic Versions\n","type":"blog post","url":"/posts/2020-08-31-rego-semver-contribution/"},{"body":"\n- Over the course of the bank holiday I moved my [personal website](https://github.com/charlieegan3/personal-website)s from my Raspberry Pi Kubernetes cluster to Netlify. I‚Äôm enjoying using GitHub actions and am pleased to have these running somewhere I don‚Äôt need to worry about them. All three projects (charlieegan3.com, photos, and music) are based around CronJobs running to keep data fresh. I‚Äôve lost confidence in these running reliably on my home cluster and wasn‚Äôt really interested in building a solution when GitHub Actions is sitting there and I have a long list of more interesting \u0026 less 'worky' projects to be getting on with.\n- I wrote a [short post](/posts/2020-08-31-rego-semver-contribution/) about an open source contribution to the [OPA](https://www.openpolicyagent.org/) project I made a while back which allows the comparing of [Semantic Versions](https://semver.org/) in policies.\n- This week our parent company [Venafi](https://www.venafi.com/) announced that we‚Äôre not going to be required to return to an office before June 2021. Both Jetstack and Venafi have always been pretty remote friendly, but it was interesting and I think important to make the date clear. I‚Äôve settled into my WFH life, but would likely do one day in the office given the chance.\n- I continue to struggle with the new words on Codenames at [netgames.io](http://netgames.io). The original wordlist seems to be reserved for [codenames.game](https://codenames.game/), which doesn't appear to be as reliable.\n- I bought, waited for and wore [my new wetsuit](https://uk.roka.com/collections/mens-wetsuits/products/mens-maverick-comp-ii-wetsuit?variant=13603914809455) for a swim session at the lido. Would recommend for fellow skinny people, very cosy.\n- I met Lewis for a [ride on Zwift](https://www.strava.com/activities/4009210358/overview). I made the mistake of thinking that the route would be 'presumably undulating'... I almost missed a meeting.\n    ![Screenshot_from_2020-09-05_10-59-35.png](Screenshot_from_2020-09-05_10-59-35.png)\n","date":"Sep 6 2020","id":148,"title":"Week Notes: 2\n","type":"blog post","url":"/posts/2020-09-06-week-notes-2/"},{"body":"\nTwo years ago, GitHub announced *Actions,* their repo-integrated, workflow automation product. It took me ages to get access, then when I finally did, it seemed to get a bit of a bad review for being rough around the edges. Roll on 6 months and I‚Äôm having a great time with it. This posts is a list of the fun things I‚Äôm doing with GitHub Actions - spoiler alert, not many really seem to fit the intended use case...\n\n## charlieegan3/charlieegan3\n\nStarting out with the most self-centered \u0026 self-promoting entry in the list is my repo that updates [my own GitHub profile](https://github.com/charlieegan3).  At the time of writing, it looks like this:\n\n![profile.png](profile.png)\n\nThis section of the page shows the readme of the [charlieegan3/charlieegan3 repo](https://github.com/charlieegan3/charlieegan3). In that repo I have a [script](https://github.com/charlieegan3/charlieegan3/blob/fa92f0cbfacf6820873b699d7a5bdf9e355b05bf/hack/update_readme.rb) that gets this information from my website and replicates it here. It also has a little emoji for the city at the time of day it‚Äôs meant to be for me here in London. This little project was my GitHub Actions gateway drug!\n\nSadly it‚Äôs not especially easy to replicate, mostly that script just processes [the data](https://charlieegan3.github.io/json-charlieegan3/build/status.json) that‚Äôs updated by another, older project called [json-charlieegan3](https://github.com/charlieegan3/json-charlieegan3). Which leads me on to...\n\n## json-charlieegan3\n\nThis project is older than GitHub Actions, but now runs there too. It‚Äôs long been responsible for updating this little ‚Äòlive‚Äô section on my site. It‚Äôs run in all kinds of places before, from Heroku to GKE to my Raspberry Pi cluster to... ü•Å GitHub Actions!\n\n![live.png](live.png)\n\nThis one makes use of git/GitHub as the storage too and shares the generated JSON file via GitHub pages with [charlieegan3.com](http://charlieegan3.com). Committing and pushing from Actions is pretty nice as you don‚Äôt need to configure the access to the repo, just update the files and commit the result back. This works in the same way as charlieegan3/charlieegan3 for the profile README update.\n\n## personal-website\n\nMy personal website is now, once again, back on GitHub. I use actions to build the site with [Hugo](https://gohugo.io/) and to commit the result to a `netlify` branch. This is then picked up and deployed to Netlify. I don‚Äôt use pages for my personal site to keep [charlieegan3.github.io](http://charlieegan3.github.io) simple and my other Pages sites working.\n\nThis is pretty boring, everyone and their mum has a Hugo site these days... what makes this interesting?\n\nSo glad you asked. I‚Äôve also got a script that can import page content and posts from [Notion](https://notion.so). I [make use of the export functionality](https://github.com/charlieegan3/personal-website/blob/master/bin/export_notion.rb) to get a zip of the page called ‚Äò*Website*‚Äô and all it‚Äôs children pages. Using this data, I then have a script to [import](https://github.com/charlieegan3/personal-website/blob/master/bin/import_notion.rb) this into the existing Hugo site.\n\nThis is all a bit of a hack, but it seems to work really nicely. This post is the second to be written using Notion as the ‚ÄòCMS‚Äô (Content Management System). What‚Äôs nice about using Notion for me is that it takes the friction out of writing on my iPad. I can import images and format the post easily.\n\nTo actually update the repo, I make use of this little known(?) GitHub Actions feature called `workflow_dispatch`. It looks a bit like this:\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      commitSubject:\n        description: Subject\n        required: true\n      commitDetail:\n        description: Detail\n        required: false\n```\n\nThese two fields allow me to set the commit message Subject and detail. GitHub uses this to generate the form like this:\n\n![trigger.png](trigger.png)\n\nThen when I click `Run workflow` I get my site updated from Notion with a nice commit message explaining what‚Äôs been added.\n\nAt work we once had lunch with a professional blogger. One of the things he explained was important was reducing the friction to update and create content. This seemed so simple, and I‚Äôve taken it to heart. Before last weekend, I didn‚Äôt have the path from WYSIWYG ‚Üí published site properly automated, now I do and this is my second post in 2 weeks. Not committing to that schedule, but it‚Äôs just an interesting change for me.\n\n## notion-export \u0026 dropbox-backup\n\nYou won‚Äôt find these repos on my profile as I‚Äôm keen to keep the GitHub Actions run state and logs private. I‚Äôve not yet worked out who can see what in the logs on public repos.\n\nThese repos don‚Äôt use Actions for anything related to git or GitHub, they just run jobs on a schedule.\n\n**notion-export** downloads a full copy of my Notion workspace and uploads it to Dropbox.\n\nExporting the data is much the same as it is to [get the data for the website](https://github.com/charlieegan3/personal-website/blob/f5c916ffc598693de1dd789a03f16869f330706f/bin/export_notion.rb). The upload is super simple too with the `[dropbox_api`gem](https://github.com/Jesus/dropbox_api). I find Dropbox‚Äôs API access story to be so much better than Google Drive and all the others. Dropbox isn‚Äôt without faults, but it does seem to be the most user friendly cloud storage as a one-man-band user trying to organise and automate the dull stuff.\n\n```ruby\nrequire \"dropbox_api\"\n\nts = Time.now.utc.strftime(\"%Y-%m-%d\")\n\nclient = DropboxApi::Client.new(ENV.fetch(\"DROPBOX_TOKEN\"))\nFile.open(\"export.zip\") do |f|\n  client.upload_by_chunks(\n    \"/Archive/Account Exports/notion-#{ts}.zip\",\n    f,\n    {\n      mode: \"overwrite\"\n    }\n  )\nend\n```\n\n**dropbox-backup** might be a rather unexpected project. I don‚Äôt know anyone else that backs up their personal cloud storage. My school computing teacher said that computers allow you to make mistakes very quickly, I‚Äôm sure that‚Äôs not original, but it stuck with me. I make reasonably heavy use of automation and bulk update tools like [Rclone](http://rclone.org/) to manage my Dropbox and since it contains things like the only digital copy of photos going back as far as 1950, paying BackBlaze ¬£2 a month for some space in b2 felt worth it.\n\nThis repo contains only the GitHub Actions manifest really:\n\n```yaml\non:\n  schedule:\n  - cron: '0 1 1,15 * *'\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n      with:\n        fetch-depth: 0\n    - name: Install rclone\n      run: |\n        curl https://rclone.org/install.sh | sudo bash\n    - name: Run sync\n      shell: bash\n      env:\n        RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG }}\n      run: |\n        set -ex\n        echo $RCLONE_CONFIG | base64 -d \u003e rclone.conf\n        rclone \\\n          --config rclone.conf \\\n          sync \\\n          \"dropbox:/\" \\\n          \"bb:bucket-name/\" \\\n          --exclude \"Vault/**\"\n```\n\nHere we see a really clear example of Actions as ‚Äòcron-as-a-service‚Äô, and I suppose a secret store too for Rclone config.\n\n## photos \u0026 music\n\nMy two ‚Äòflagship‚Äô (lol) projects are my [Instagram-driven \u0026 enhanced photo library](https://github.com/charlieegan3/photos) and my [musical memory crutch](https://github.com/charlieegan3/music). Both these projects refresh their data using GitHub actions.\n\n**photos** downloads new post data, downloads missing locations, and then stores the media from the post in object storage. It then commits the updated data files which kicks off another action to build the site and push it to the `netlify` branch. Seems like a pretty good fit, but I need to make the requests to Instagram via my residential proxy these days. The GitHub IPs appear to be unable to make the same requests...\n\n**music** does quite [a lot more](https://github.com/charlieegan3/music/tree/master/.github/workflows)... in summary, the following tasks are all running in Actions:\n\n- Data refresh (download play data and store in BigQuery)\n    - Spotify\n    - Shazam\n    - Soundcloud\n    - YouTube\n- Summarization (save the results of a query to object storage for the website to consume)\n    - Home page overview (plays by month, top tracks and artists per year)\n    - Recent plays\n    - Top monthly plays\n    - Tracks per artist to generate search index\n- ‚ÄòEnriching‚Äô data. Operations to complete missing data and consolidate a number of consistencies. Saving the result back to another BigQuery table.\n- Generating the artist pages and updating the Hugo site at [music.charlieegan3.com](http://music.charlieegan3.com)\n- Backing up the play data to GCS (in case I screw it up, which has happened before)\n- Monitoring probe to test for missing data and send alerts\n\nI was hoping that this final example would show that GitHub Actions can be the glue to tie together git, object storage, external services and static sites into something that looks like an ‚Äòapplication‚Äô.\n\n---\n\nSo that‚Äôs how I‚Äôm using GitHub actions, as of today.\n\nI plan to use it more since it‚Äôs free and seems to be pretty reliable.\n\n**Why not use k8s?** Can‚Äôt really be bothered maintaining things that I want to ‚Äòjust-work‚Äô on my pi cluster that sometimes gets dripped on and breaks. My new rule is if it‚Äôs used by anyone other than me, then I run it on someone else‚Äôs cloud - not the little one in my house, GitHub Actions has been a big part of that shift.\n\n**Why not run these on [insert cloud here]?** Actions is free, and integrated with git.\n\n**Why not use [insert other free cron tool]?** I probably haven‚Äôt heard of it and it doesn‚Äôt integrate as nicely with git.\n","date":"Sep 7 2020","id":149,"title":"Fun things I‚Äôm doing with GitHub Actions\n","type":"blog post","url":"/posts/2020-09-07-github-actions-fun/"},{"body":"\n- At work this week I‚Äôve spent much of my time writing and responding to feedback on technical design proposals for features I‚Äôm working on. This is not something that I‚Äôve done in a big way before so I‚Äôm enjoying learning something new and getting feedback on them. One thing I‚Äôm still not sure how to balance is when to actually make a start... I guess I‚Äôll get there are I do more of them as part of my work.\n- I went for another swim (with wetsuit!). I stayed for the full length of the session (about 45 minutes of swimming after 15 mins of getting the suit on!). It seems like it‚Äôs too cold for most to stay the full hour so for the last 10 minutes I had the whole lane to myself which, on a lovely summer‚Äôs evening, felt like quite the luxury.\n- I took two days off from running and biking at the start of the week. I just needed a break really after a pretty big week. This turned into a generally more chill week... I did get some new shoes very much like the old ones though.\n    ![IMG_4255.jpg](IMG_4255.jpg)\n- Gaming this week has been another slightly haphazard Sea of Thieves lark about as well as an ‚Äòintro to warzone‚Äô session which was my first ever cross platform gaming experience - is the future here at last? It seemed to work ok for me but one of our party was unable to join. We‚Äôre all enjoying being able to play together with Games Pass but the relentless multi gigabyte updates and coordinating these in the group is starting to take it‚Äôs toll. It‚Äôs hard enough to find a time we can all manage, let alone make sure we‚Äôve all left consoles ages to finish downloads. This week we got stung by a Sea of Thieves update that landed just as we were about to play - very frustrating experience all round really.\n- I‚Äôve been keeping a little todo list of things to fix and add to my website. This week I‚Äôve mostly been working on improvements to the search feature. Now as you search matches within pages, posts, projects and profile are shown highlighted. I‚Äôve been using [lunr.js](https://lunrjs.com/). It‚Äôs default stemmer, trimmer and stopwords list weren‚Äôt working, so I opted to turn them off. I suspect that this works for me with a relatively small set of documents where I have some knowledge of their contents and can manually check the results.\n- About 2 years ago I bought a Patagonia jacket before taking a trip to Copenhagen. I really loved the jacked and wore it so much that it wore through in the back panel. I was delighted to get the jacket repaired free of charge.\n    ![before.jpg](before.jpg)\n    ![after.jpg](after.jpg)\n- I went to visit a friend I'd not seen in a long time and relearned how to play the card game *Shithead*. Minimal quick reactions required, would recommend.\n","date":"Sep 13 2020","id":150,"title":"Week Notes: 3\n","type":"blog post","url":"/posts/2020-09-13-week-notes-3/"},{"body":"\n- I've been making small steps towards being able to do dev work on a cloud instance. This week's small steps have been:\n    - Adding a [GCE builder](https://github.com/charlieegan3/dev-machine/blob/9093fe25322ca3554a84f5b0ec59a1512fee66df/packer.json#L21-L29) for my dev-machine project. This will allow me to create dev instances in my work GCP account. (I'm not yet using a cloud instance for work, but I plan to be able to at some point as I need to send my laptop off for repair...).\n    - Creating a script to upload all my local, secret (and so uncommitted) config files to my password manager ([bitwarden](http://bitwarden.com)). Terribly messy Ruby script [here](https://github.com/charlieegan3/bitwarden-config-sync). This makes it easier to work on multiple machines.\n    - I used a tool called *[Algo](https://github.com/trailofbits/algo/blob/master/docs/deploy-to-ubuntu.md#road-warrior-setup)* to setup a VPN instance running on my free f1-micro in GCP. This is going to allow my to connect to services (such as ‚Äòlocal‚Äô web dev servers) without exposing them to the world. Algo works ok, the system seems to be: deploy the instance, need to change it? Delete it and deploy it again. You can update users but other updates aren‚Äôt supported. The small amount of config to do this lives [here](https://github.com/charlieegan3/infrastructure/tree/master/vpn).\n    - I also got one of these! This is a Lenovo ThinkCentre m90n. This is going to be my ‚Äòthin client‚Äô while my laptop is away at some point. In the meantime, I‚Äôm getting back up to speed with the latest and greatest in the world of Wayland! I‚Äôve been pleasantly surprised at how doable much of what I thought wasn‚Äôt now is (it‚Äôs been some years since I last tried Wayland). There are lots of quality tools to replace X counterparts, [sway](https://github.com/swaywm/sway), [grim](https://github.com/emersion/grim), [gammastep](https://gitlab.com/chinstrap/gammastep)... I‚Äôve had the most fun getting [wf-recorder](https://github.com/ammen99/wf-recorder) to create a virtual webcam to share my screen... (Wayland screen sharing still a bit WIP in Zoom and I need to have a backup option)\n        ![8F1CF00E-1B68-4E3A-BC60-6E4A3333B88F.jpeg](8F1CF00E-1B68-4E3A-BC60-6E4A3333B88F.jpeg)\n- I found this [cool (but sadly insecure) site](http://sortyourmusic.playlistmachinery.com) that sorts Spotify playlists by BPM and other factors. I've been enjoying the slightly random collection of high-bpm tunes. Sadly it was not all my Drum \u0026 Bass greatest hits as expected.\n- At work, I've been setting up a Postgres database for our application. I found Go lacking a good means of creating different 'suites' with setup and tear down routines. This [testify package](https://github.com/stretchr/testify#suite-package) seems to work quite nicely. With Go, when it's not in the standard library, it can be a bit of a rabbit hole working out what others have done to solve what feels like a common problem. We ended up using [sql-builder](https://github.com/huandu/go-sqlbuilder) to generate queries as it felt the most idiomatic.\n- I opened a [PR on a Firefox addon](https://github.com/tiansh/always-in-container/pull/3) I use where I committed my first ever Chinese characters! This addon is helpful as it stops the browser opening a URL before you select a container. This means that rather than opening it in the wrong container, then the right one, you can intercept it and open the URL in the container you want. In the end the maintainer reimplemented it differently but I still got the feature I wanted so I‚Äôm happy!\n- I also added this [unexciting 404 page](https://charlieegan3.com/404) to my website.\n- I hardly played any games this week beyond 30 mins of Warzone. Granny also called to cancel my trip to visit, which I‚Äôm actually quite relieved about given the change in COVID circumstances.\n","date":"Sep 20 2020","id":151,"title":"Week Notes: 4\n","type":"blog post","url":"/posts/2020-09-20-week-notes-4/"}]